[
{
	"uri": "https://linuxias.github.io/machinelearning/basic/",
	"title": "Basic Theory",
	"tags": [],
	"description": "",
	"content": "Basic Theory "
},
{
	"uri": "https://linuxias.github.io/machinelearning/agent/",
	"title": "Software Agent",
	"tags": [],
	"description": "",
	"content": "Software Agent "
},
{
	"uri": "https://linuxias.github.io/linux/",
	"title": "1. Linux",
	"tags": [],
	"description": "",
	"content": "1. Linux The chapter that organizes information about Linux.\n"
},
{
	"uri": "https://linuxias.github.io/linux/profile/",
	"title": "Profile",
	"tags": [],
	"description": "",
	"content": "Profile "
},
{
	"uri": "https://linuxias.github.io/container/docker/1_docker_cmd_basic/",
	"title": "Docker Basic",
	"tags": [],
	"description": "",
	"content": "Docker Hub에서 Image Download Docker pull command를 사용하여 DockerHub 에 등록된 이미지 다운로드 합니다. 다운로드할 이미지는 ubuntu 18.04 이미지입니다.\n$sudo docker pull ubuntu:18.04 18.04: Pulling from library/ubuntu 5c939e3a4d10:Pull complete c63719cdbe7a: Pull complete 19a861ea6baf: Pull complete 651c9d2d6c4f: Pull complete Digest: sha256:8d31dad0c58f552e890d68bbfb735588b6b820a46e459672d96e585871acc110 설치된 Docker Image 확인하기 docker images 명령어를 이용하여 설치된 도커 이미지들의 리스트를 확인할 수 있습니다. 제 PC 설치된 이미지 리스트는 아래와 같습니다. 총 5가지 이미지가 설치되어 있네요.\n$sudo docker images REPOSITORY TAG IMAGE ID CREATED SIZE ubuntu 18.04 ccc6e87d482b 3 days ago 64.2MB mysql 5.7 b598110d0fff 4 days ago 435MB wordpress latest 1b1624b63467 3 weeks ago 539MB node 6.11.5 852391892b9f 2 years ago 662MB REPOSITORY와 TAG명 그리고 IMAGE ID가 보입니다. IMAGE ID는 16진수 해쉬값으로 구성되어 있습니다. 그리고 CREATE로 이미지가 생성된 일자, 이미지의 사이즈까지 확인할 수 있습니다.\nDocker Container 생성하기 1. docker create docker create 명령어를 실행하여 컨테이너를 생성할 수 있습니다.\n$sudo docker create -i -t ubuntu:18.04 9f1be16cd91b3673ac95d77eaf0906405c335f4aaed9e77520a245fa0b33299f 이 명령어로 컨테이너를 생성할 시에 랜덤한 16진수 해쉬값이 생성되며, 이 해쉬값은 생성한 컨테이너의 ID가 됩니다.\n2. docker run docker run 명렁어를 사용하여 컨테이너를 생성하고 실행할 수 있습니다. \u0026lsquo;-i -t\u0026rsquo; 옵션은 각각 입출력과, tty 활성화를 요청하는 옵션입니다. 해당 옵션을 제외하고 실행 시 사용자와의 인터페이스가 실행되지 않고, 컨테이너가 실행된 직 후 바로 종료됩니다.\ndocker pull 로 다운받은 ubuntu 18.04 이미지를 실행하여 보겠습니다.\n$sudo docker run -i -t ubuntu:18.04 root@f54e84f31187:/# ps PID TTY TIME CMD 1 pts/0 00:00:00 bash 10 pts/0 00:00:00 ps root@f54e84f31187:/# exit exit 3. docker create 와 docker run 의 차이점 docker create와 docker run의 차이는 컨테이너를 생성하는 것 까지와 생성 후 실행까지 시키느냐에 따른 차이입니다. 아래 이미지에서 docker pull은 컨테이너로 실행할 이미지가 없는 경우 필요한 단계이며, docker create는 create 단계에서 멈추며, docker run은 start 이후 attach가 동작하게 됩니다.\ngraph LR; pull[docker pull] --\u0026gt; create[docker create] create --\u0026gt; start[docker start] start --\u0026gt; attach[docker attach] Container 목록 및 정보 확인하기 $sudo docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES $sudo docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 9f1be16cd91b ubuntu:18.04 \u0026#34;/bin/bash\u0026#34; 12 minutes ago Created vigorous_lumiere docker ps 명령어는 컨테이너의 리스트를 보여줍니다. docker images와는 리스트를 보여주는 것은 같으나, 컨테이너 리스트인지 이미지 리스트인지의 차이입니다. -a옵션은 정지된 컨테이너까지 포함하여 출력하여 줍니다.\n특정 컨테이너의 상세한 정보를 보기 위해서 inspect 명령어를 사용할 수 있습니다. inspect 명령어는 container 내부 명령어인데요, container 외에도 image, volume 등에도 사용이 가능합니다. 사용 방법은 아래와 같습니다.\n$sudo docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 9f1be16cd91b ubuntu:18.04 \u0026#34;/bin/bash\u0026#34; 16 minutes ago Created vigorous_lumiere // docker ps 명령어로 존재하는 컨테이너를 확인하고 inspect 명령어로 상세 정보를 확인하면 아래처럼 출력됩니다. $sudo docker container inspect vigorous_lumiere | more [ { \u0026#34;Id\u0026#34;: \u0026#34;9f1be16cd91b3673ac95d77eaf0906405c335f4aaed9e77520a245fa0b33299f\u0026#34;, \u0026#34;Created\u0026#34;: \u0026#34;2020-01-19T13:33:00.518174743Z\u0026#34;, \u0026#34;Path\u0026#34;: \u0026#34;/bin/bash\u0026#34;, \u0026#34;Args\u0026#34;: [], \u0026#34;State\u0026#34;: { \u0026#34;Status\u0026#34;: \u0026#34;created\u0026#34;, \u0026#34;Running\u0026#34;: false, \u0026#34;Paused\u0026#34;: false, ... Container 삭제하기 docker rm 명령어와 docker container prune 명령어를 이용하여 컨테이너를 삭제할 수 있다.\n$sudo docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 9f1be16cd91b ubuntu:18.04 \u0026#34;/bin/bash\u0026#34; 20 minutes ago Created vigorous_lumiere $sudo docker rm vigorous_lumiere vigorous_lumiere $sudo docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 존재하던 컨테이너가 docker rm 명령어 이후 삭제되었음을 확인하였습니다. 다양한 컨테이너를 다루다 너무 많은 컨테이너가 생성되었을 경우에는 docker rm 명령어로 하나씩 삭제하기가 번거롭습니다. 그때 docker container prune 명령어로 모두 삭제가 가능합니다.\nContainer IP 확인하기 ubuntu:18.04 이미지를 다운받아 컨테이너로 생성하여 실행하게 되면 네트워크 정보들 간단히 IP 확인하기 위한 ifconfig 툴도 설치가 되어있지 않습니다. ifconfig를 이용하여 ip를 확인하는데, 만약 ifconfig가 설치되어 있지 않다면 net-tools를 설치해줍니다\n$sudo docker run -i -t ubuntu:18.-4 root@f54e8a13312:/# apt-get update \u0026amp;\u0026amp; apt-get install net-tools ... root@f54e8a13312:/# ifconfig "
},
{
	"uri": "https://linuxias.github.io/container/",
	"title": "2. Container",
	"tags": [],
	"description": "",
	"content": "2. Container Namespace, Cgroup\n"
},
{
	"uri": "https://linuxias.github.io/container/docker/2_docker_storage/",
	"title": "Storage",
	"tags": [],
	"description": "",
	"content": "컨테이너를 사용하여 다양한 작업을 할 수 있습니다. 여러 작업 중 컨테이너 내부에서 생성한 정보, 파일들이 컨테이너가 종료되고 난 이후에 손실되어 찾을 수 없습니다. 기본적으로 컨테이너 내부에 생성되는 모든 파일들은 쓰기가능한 컨테이너 레이에어 저장되어 집니다. 이렇게 저장된다는 것은 아래와 같은 의미를 포함합니다.\n 컨테이너가 종료된 후에는 데이터가 지속 가능하지 않습니다. 다른 프로세스가 컨테이너 내부의 데이터를 참조하기 어렵습니다. 컨테이너의 쓰기가능한 레이어는 컨테이너가 동작 중인 상태에서 호스트와 매우 타이트하게 연결되어 있습니다. 이 말은, 여러분이 데이터를 다른 곳으로 옮기는게 쉽지 않음을 의미합니다. 컨테이너의 쓰기 가능한 레이어에 쓰기 위해서는 파일 시스템을 관리하기 위한 스토리지 드라이버가 필요합니다. 스토리지 드라이버는 리눅스 커널을 사용하여 유니언 파일 시스템을 제공합니다. 유니언 파일 시스템은 별도로 정리하겠습니다. 이러한 추가 추상화는 호스트 파일 시스템에 직접 쓰는 데이터 볼륨을 사용하는 것에 비해 성능을 감소시킵니다.  여러분들이 겪는 불편함을 해소하기 위해 도커는 스토리지를 지원하고 있습니다. 여러분들은 2가지 중 하나를 선택할 수 있습니다. 호스트에 직접 파일을 저장하여 컨테이너가 종료되더라도 파일을 유지할 수 있는 방법과 tmpfs 마운트를 이용하는 방법입니다.\n스토리지 종류 도커 스토리지의 종류는 3가지가 있습니다. Volume, bind mount, tmpfs mount입니다. 각 방식을 가장 잘 나타내는 표는 docker docs 페이지에 나와있는 아래 그림입니다.\n   Storage 종류 설명     Volume Volume은 도커에 의해 관리되는 호스트 파일시스템의 일부분에 저장하는 방법입니다. 호스트 내의 도커 프로세스가 아닌 다른 프로세스들은 Volume 관련 파일시스템을 수정할 수 없습니다.   Bind mounts 호스트 시스템 내의 어디든 저장할 수 있습니다. 도커 내의 모든 프로세스가 접근 수정할 수 있다.   tmpfs mounts 호스트 시스탬의 메모리에 저장하는 방식으로 시스템의 파일시스템에 정보를 저장하는 방식과는 차이가 있다.    언제 사용하는게 좋은가? Volume volume은 도커 컨테이너 내의 데이터를 유지시키는데 가장 선호되는 방법입니다. 몇 가지 사용되는 경우는 아래와 같습니다.\n 여러 동작중인 컨테이너 사이의 데이터를 공유하는 경우에 사용됩니다. 특정 컨테이너가 정지 또는 제거되더라도 volume은 삭제되지 않습니다. 여러 컨테이너들이 동일 volume에 대하여 마운트 할 수 있습니다. 도커 호스트가 주어진 디렉터리 또는 파일 구조를 보장하지 않는 경우에 사용됩니다. volume은 컨테이너 런타임으로부터 도커 호스트 구성을 분리(decouple)하는데 도움이 됩니다. 백업, 복구, 호스트에서 다른 도커 호스트로 데이터를 이관하는 경우 volume은 좋은 선택지가 됩니다.  bind mounts 일반적으로 가능하다면 volume을 사용하는 것을 추천합니다. 하지만 특정 경우에 한 해 bind mount를 사용하는 경우가 있습니다. 몇몇 유스케이스는 아래와 같습니다.\n 호스트 시스템에서 컨테이너로 구성 파일을 공유하는 경우. Docker는 호스트 시스템에서 각 컨테이너에 /etc/resolv.conf를 마운트하여 기본적으로 DNS 확인을 컨테이너에 제공하는 방식이다. 소스 코드 또는 도커 호스트 상의 개발환경과 컨테이너 사이의 요소를 빌드하기 위하여 사용합니다. 예를 들어, Maven 타겟 / 디렉토리를 컨테이너에 마운트 할 수 있으며 Docker 호스트에서 Maven 프로젝트를 빌드 할 때마다 컨테이너가 재구성 된 아티팩트에 액세스 할 수 있습니다. Docker 호스트의 파일 또는 디렉토리 구조가 컨테이너가 필요로 하는 바인딩 마운트와 일치하도록 보장되는 경우.  tmpfs bounts tmpfs 마운트는 데이터가 호스트 시스템이나 컨테이너 내에서 유지되지 않도록 하려는 경우에 가장 적합합니다. 이는 보안성 향상 로 또는 응용 프로그램이 많은 양의 비영구 상태 데이터를 써야 할 때 컨테이너의 성능을 위함입니다. 파일시스템에 저장된 데이터에 접근하는 것보다 메모리에 저장된 데이터를 접근하는 것이 성능 상 큰 이점이 있긴 때문입니다.\n참고 Manage data in Docker\n"
},
{
	"uri": "https://linuxias.github.io/sw_architecture/",
	"title": "3. S/W Architecture",
	"tags": [],
	"description": "",
	"content": "3. S/W Architecture The chapter that organizes information about S/W Architecture.\n"
},
{
	"uri": "https://linuxias.github.io/machinelearning/",
	"title": "4. Machine Learning",
	"tags": [],
	"description": "",
	"content": "Machine Leanrning "
},
{
	"uri": "https://linuxias.github.io/books/",
	"title": "Books",
	"tags": [],
	"description": "",
	"content": "Books This chapter is for reviewing books\n"
},
{
	"uri": "https://linuxias.github.io/container/cgroup/",
	"title": "Cgroup",
	"tags": [],
	"description": "",
	"content": "Cgroup "
},
{
	"uri": "https://linuxias.github.io/linux/compilelink/",
	"title": "Compile&amp;Link",
	"tags": [],
	"description": "",
	"content": "Compile \u0026amp; Link "
},
{
	"uri": "https://linuxias.github.io/container/namespace/",
	"title": "Namespace",
	"tags": [],
	"description": "",
	"content": "Namespace "
},
{
	"uri": "https://linuxias.github.io/container/cgroup/1.cgroup/",
	"title": "1. Cgroup",
	"tags": [],
	"description": "",
	"content": "cgroup은 단일 또는 태스크 단위의 프로세스 그룹에 대한 자원 할당을 제어하는 커널 모듈입니다. Cgroup은 Control group으로 처음 개발되었을 때는 group이 아니라 container란 용어를 사용했었습니다. 하지만 container란 용어는 너무 많은 의미를 내포하고 있다고 판단하여 지금의 group이만 이름으로 변경되었습니다.\n현재 cgroup은 v1와 v2 두가지 버전이 공존하고 있습니다. v1의 서비스시템 중 일부는 v2에 포함되어 있는 상태입니다. 언젠가 v2만 남겠죠..?\ncgroup 서브시스템 cgroup은 여러 개의 서브시스템으로 이루어져 있습니다. (아래 표에 모든 서브시스템을 작성한 것은 아니니, 유의해주세요.)\n   subsystem description     cpu cgroups는 시스템이 busy 상태일 때 CPU 공유를 최소화 즉 사용량을 제한 할 수 있습니다. 이 서브시스템은 CPU에 cgroup 작업 액세스를 제공하기 위한 스케줄러(Documentation/scheduler/sched-design-CFS.txt)를 제공합니다.   cpuacct 프로세스 그룹 별 CPU 자원 사용에 대한 분석 통계를 생성 및 제공합니다. (Documentation/cgroup-v1/cpuacct.txt)   cpuset 개별 CPU 및 메모리 노드를 cgroup에 바인딩 하기 위해 사용하는 서브시스템입니다(Documentation/cgroup-v1/cpusets.txt.)   memory cgroup 작업에 사용되는 메모리(프로세스, 커널, swap)를 제한하고 리포팅을 제공하는 서브시스템입니다. (Documentation/cgroup-v1/memory.txt)   blkio 특정 block device에 대한 접근을 제한하거나 제어하기 위한 서브시스템입니다. block device에 대한 IO 접근 제한을 설정할 수 있습니다. (Documentation/cgroup-v1/blkio-controller.txt)   devices cgroup의 작업 단위로 device에 대한 접근을 허용하거나 제한합니다. whitelist와 blacklist로 명시되어 있습니다. (Documentation/cgroup-v1/devices.txt.)   freezer cgroup의 작업을 일시적으로 정지(suspend)하거나 다시 시작(restore)할 수 있습니다. (Documentation/cgroup-v1/freezer-subsystem.txt.)   net_cls 특정 cgroup 작업에서 발생하는 패킷을 식별하기 위한 태그(classid)를 지정할 수 있습니다. 이 태그는 방화벽 규칙으로 사용되어 질 수 있습니다. (Documentation/cgroup-v1/net_cls.txt.)   net_prio cgroup 작업에서 생성되는 네트워크 트래픽의 우선순위를 선정할 수 있습니다. (Documentation/cgroup-v1/net_prio.txt.)   hugetlb HugeTLB에 대한 제한을 설정할 수 있습니다.   pid cgroup 작업에서 생성되는 프로세스의 수를 제한할 수 있습니다. (Documentation/cgroup-v1/pids.txt.)    cgroup은 가상 파일시스템의 디렉토리로 표시되는 계층 구조로 구성되어 집니다. 하지만 프로세스 계층구조와는 달리 다중 계층구조를 가집니다. 즉 하나의 계층 구조가 아닙니다.\ncgroup 사용방법 cgroup은 두 가지 방법으로 사용할 수 있습니다.\n cgroup 가상파일시스템을 마운트하여 파일 및 디렉토리를 조작하는 방법 사용자 도구 사용 (Debian, ubuntu는 cgroup-bin / RHEL, CentOS는 libcgroup)  cgroup의 실체는 파일시스템입니다. cgroup은 오직 프로세스(태스크) 들을 그룹화 하는 역할만 하며 내부적으로 자원을 제한하거나 할당하는 역할은 서브시스템에서 수행됩니다.\n참고자료  Containerization with LXC | Konstantin Ivanov [Linux] cgroup - (task) control group (1) (http://egloos.zum.com/studyfoss/v/5505982) [Linux] cgroup - (task) control group (1)(http://studyfoss.egloos.com/5506102) Docker(container)의 작동 원리: namespaces and cgroups(https://tech.ssut.me/what-even-is-a-container/) Control Groups | 문C 블로그(http://jake.dothome.co.kr/control-groups/) CGROUPS(7) linux manual page  "
},
{
	"uri": "https://linuxias.github.io/container/namespace/1.what_is_namespace/",
	"title": "1. Namespace",
	"tags": [],
	"description": "",
	"content": "Namespace 기술은 cgourp(Control Group)과 함께 컨테이너(Container) 솔루션을 구성하는 기술 중 하나입니다. 이번 글에서는 namespcae에 대해 정리한 후linux에서 제공하는 namespcae의 종류에 대해 정리하고자 합니다.\nnamespace는 전역 시스템 리소스를 추상화하여 전역 리소스의 자체 격리 인스턴스가있는 namespace 내의 프로세스에 표시 되도록합니다. 전역 리소스에 대한 변경은 namespace의 멤버, 즉 동일한 namespace를 가진 다른 프로세스에서 볼 수 있지만 다른 namespace를 가진 프로세스에서는 보이지 않습니다. namespcae를 사용하는 것은 컨테이너를 구현하는 것입니다.\n   Namespace Constant Isolates     IPC CLONE_NEWIPC System V IPC, POSIX message queues   Network CLONE_NEWNET Network devices, stacks, ports, etc.   Mount CLONE_NEWNS Mount points   PID CLONE_NEWPID Process IDs   User CLONE_NEWUSER User and group IDs   UTS CLONE_NEWUTS Hostname and NIS domain name    namespace API namespace API에는 아래의 시스템 콜과 함께 /proc 파일이 포함됩니다. 먼저 system call에 대해 설명드립니다.\n시스템 콜    API Description     setns(2) setns 시스템 콜은 호출하는 프로세스가 존재하는 namespace에 조인합니다. 조인하고자 하는 namespace는 /proc/[pid]/ns 디렉토리 아래 존재하는 하나의 namespace 파일디스크립터(fd) 를 이용합니다.   clone(2) clone(2) 시스템 콜은 새로운 프로세스를 생성합니다. 시스템 콜 호출 시 flags argument로 CLONE_NEW* flag를 하나 이상 전달합니다. CLONE_NEW* flag는 위에서 설명한 바 있습니다. 그럼 각 flag에 해당하는 새로운 namespace가 생성되며 그 namepsaces의 멤버로 자식 프로세스가 생성됩니다.   unshare(2) unshare() 시스템 콜을 호출한 프로세스를 새로운 namespace로 이동시킵니다. 만약 flags argutment가 CLONE_NEW* flag를 입력한다면, 새로운 namespace가 생성되고, 해당 namespace의 멤버로 이동합니다.    여기서 유의할 점은 clone(2) 와 unshare(2) 시스템 콜을 사용하여 새로운 namespace들을 생성하기 위해선 CAP_SYS_ADMIN capability가 필요합니다. user namespace 생성은 예외적으로 privilege가 필요하지 않습니다.\n/proc/[pid]/ns 디렉토리 그럼 /proc 파일시스템에서 namespace에 대해 간략히 정리하겠습니다. 모든 프로세스들은 /proc/[pid]/ns 디렉토리가 존재합니다. 아래 ns 디렉토리에 여러 namespace가 존재하는 것을 확인할 수 있습니다. 이 namespace는 setns(2) 시스템 콜을 이용해 namespace를 변경하고자 할 때 변경을 원하는 namespace의 fd로 사용됩니다.\n$ ls -l /proc/$$/ns total 0 lrwxrwxrwx. 1 mtk mtk 0 Jan 14 01:20 ipc -\u0026gt; ipc:[4026531839] lrwxrwxrwx. 1 mtk mtk 0 Jan 14 01:20 mnt -\u0026gt; mnt:[4026531840] lrwxrwxrwx. 1 mtk mtk 0 Jan 14 01:20 net -\u0026gt; net:[4026531956] lrwxrwxrwx. 1 mtk mtk 0 Jan 14 01:20 pid -\u0026gt; pid:[4026531836] lrwxrwxrwx. 1 mtk mtk 0 Jan 14 01:20 user -\u0026gt; user:[4026531837] lrwxrwxrwx. 1 mtk mtk 0 Jan 14 01:20 uts -\u0026gt; uts:[4026531838] 각 namespace 별 proc 파일은 다음과 같습니다.\n   namespace /proc file     IPC namespace /proc/[pid]/ns/ipc   Mount namespace /proc/[pid]/ns/mount   Network namespace /proc/[pid]/ns/net   PID namespace /proc/[pid]/ns/pid   User namespace /proc/[pid]/ns/user   uts namespace /proc/[pid]/ns/uts    Namespace 간략 정리 각 Namespace의 상세내용은 다른 글로 정리하고자 하며, 여기서는 간략하게 정리하고자 합니다.\nIPC namespace (CLONE_NEWIPC) IPC namespace는 특정 IPC 자원들(System V IPC,와 POSIX message queue)을 격리시킵니다. 특정 IPC 자원들의 공통적인 특징은 IPC 객체가 파일시스템 경로명 외의 메커니즘으로 식별된다는 것입니다. System V IPC에는 메시지 큐, 세마포어, 공유메모리를 가리킵니다.\nNetwork namespace (CLONE_NEWNET) Network namespace는 네트워크와 관련된 시스템 자원의 격리, 고립을 제공합니다. 해당되는 시스템 자원으로는 네트워크 디바이스들, IPv4, IPv6 프로토콜 스택, IP routing tables, 방화벽, /proc/net 디렉토리, /sys/class/net 디렉토리, 포트 번호 등등이 있습니다. 물리적 네트워크 장치는 정확히 하나의 네트워크 네임스페이스를 가질 수 있습니다. 가상 네트워크 장치는 네트워크 namespace 간에 터널을 생성하는데 사용할 수 있는 추상화된 파이프를 제공합니다.\nNetwork namespace가 해제되면, 물리 네트워크 장치는 초기 Network namespace로 변경됩니다. Network namespace를 사용하기 위해선 CONFIG_NET_NS 커널 옵션이 설정되어 있어야 합니다.\nMount namespaces (CLONE_NEWNS) Mount namespace는 파일시스템 마운트 지점의 집합을 고립, 격리합니다. 즉, 서로 다른 Mount namespace의 프로세스가 파일 시스템 구조에 대해 다른 뷰를 가질 수 있습니다. mount(2), umount(2) 를 이용해 Mount namespace 내에 마운트 집합들을 수정할 수 있습니다.\n/proc/[pid]/mounts 파일은 현재 프로세스의 Mount namespace에 마운트된 모든 파일시스템을 나열합니다. 이 파일의 포맷은 **fstab(5)**를 참고해주세요.\nseunghason@linuxias$cat /proc/self/mounts sysfs /sys sysfs rw,nosuid,nodev,noexec,relatime 0 0 proc /proc proc rw,nosuid,nodev,noexec,relatime 0 0 udev /dev devtmpfs rw,nosuid,relatime,size=16413672k,nr_inodes=4103418,mode=755 0 0 ... binfmt_misc /proc/sys/fs/binfmt_misc binfmt_misc rw,relatime 0 0 tmpfs /run/user/1000 tmpfs rw,nosuid,nodev,relatime,size=3288548k,mode=700,uid=1000,gid=1000 0 0 gvfsd-fuse /run/user/1000/gvfs fuse.gvfsd-fuse rw,nosuid,nodev,relatime,user_id=1000,group_id=1000 0 0 /proc/[pid]/mountstat 파일은 프로세스의 Mount namespace의 마운트 지점에 대한 정적, 설정 정보들을 보여줍니다. 보여지는 정보에 대한 것은 아래를 참고해주세요.\ndevice /dev/sda7 mounted on /home with fstype ext3 [statistics] ( 1 ) ( 2 ) (3 ) (4) The fields in each line are: (1) The name of the mounted device (or \u0026quot;nodevice\u0026quot; if there is no corresponding device). (2) The mount point within the filesystem tree. (3) The filesystem type. (4) Optional statistics and configuration information. Currently (as at Linux 2.6.26), only NFS filesystems export information via this field. PID namespace PID namespace는 프로세스 ID 공간을 격리 시킵니다. 이 말인 즉, 다른 PID namespace의 프로세스들은 같은 PID를 가질 수도 있음을 의미합니다. PID namespace들은 프로세스 집합의 종료, 재시작과 같은 기능을 제공하기 위한 컨테이너를 허용합니다. 또한 컨테이너를 새로운 호스트로 마이그레이션하는 등의 기능을 컨테이너가 제공할 수 있도록 해줍니다.\nPID namepsace의 특이한 점은 새로운 PID namespace의 PID는 1 부터 시작한다는 것입니다. standalone 시스템과 동일하게 각 namespace의 시작 프로세스는 pid를 1번을 가지게됩니다.\nPID namespace를 사용하기 위해선 CONFIG_PID_NS 커널 옵션을 설정해야 합니다.\nUser namespace User namespace는 시큐리티와 관련된 식별자 및 속성을 분리하며, 특히 User ID와 Group ID, 루트 디렉토리, Key, Capability를 분리합니다. 프로세스의 User, Group ID는 user namespace 내,외부적으로 다를수 있습니다. 특히 프로세스는 User namespace 외부에 권한이 없는 정상적인 User ID를 가질 수 있으며, 동시에 namepsace 내부에 User ID 0을 가질 수 있습니다. 즉, 프로세스에는 user namespace 내의 작업에 대한 전체 권한이 있지만 namespace 외부 작업에 대한 권한이 없습니다. 자세한 내용은 다른 글로 살펴보겠습니다.\nUTS namespace UTS namespace는 두개의 시스템 식별자를 고립, 격리시킵니다. 두 개의 시스템 식별자는 hostname과 NIS domain name입니다. 이 식별자들은 sethostname(2), **setdomainname(2)**으로 설정가능합니다. UTS namespace의 사용은 CONFIG_UTS_NS 커널 옵션을 설정해야 합니다.\n이상으로 Namespace에 대해 간단히 정리해보았습니다. 다음 글에서 각 namespace의 사용 및 예제에 대해 다뤄보겠습니다.\n감사합니다.\n감사합니다.\n"
},
{
	"uri": "https://linuxias.github.io/container/cgroup/2.cgroup_blkio/",
	"title": "2. blkio",
	"tags": [],
	"description": "",
	"content": "Subsystem - blkio blkio 서브시스템은 특정 block device에 대한 접근을 제한하거나 제어하기 위한 서브시스템 입니다. 자세한 내용은 ** Documentation/cgroup-v1/blkio-controller.txt ** 를 참고해주세요.\n예제 살펴보기 I/O 작업이 많은 몇몇의 어플리케이션이 하나의 서버에서 동작한다고 가정합니다. 각 어플리케이션에 대해 다른 우선순위와 I/O 대역폭을 설정해주는 예제를 살펴보겠습니다.\n cgroup 가상파일시스템을 마운트합니다.  $ sudo mkdir -p /cgroup/blkio $ sudo mount -t cgroup -o blkio blkio /cgroup/blkio 마운트 여부 확인하기. 아래 명령어를 수행해서 결과가 표시된다면 정상적으로 마운트되었습니다.  $ mount | grep cgroup blkio on /cgroup/blkio type cgroup (rw,relatime,blkio) or $ cat /proc/mount | grep cgroup blkio /cgroup/blkio cgroup rw,relatime,blkio 0 0 mount 상태 확인해보기.  linuxias@linuxias-VirtualBox:/cgroup/blkio$ ls -al total 4 dr-xr-xr-x 2 root root 0 8월 11 13:35 . drwxr-xr-x 3 root root 4096 8월 6 17:07 .. -r--r--r-- 1 root root 0 8월 11 13:55 blkio.io_merged -r--r--r-- 1 root root 0 8월 11 13:55 blkio.io_merged_recursive -r--r--r-- 1 root root 0 8월 11 13:55 blkio.io_queued -r--r--r-- 1 root root 0 8월 11 13:55 blkio.io_queued_recursive -r--r--r-- 1 root root 0 8월 11 13:55 blkio.io_service_bytes -r--r--r-- 1 root root 0 8월 11 13:55 blkio.io_service_bytes_recursive -r--r--r-- 1 root root 0 8월 11 13:55 blkio.io_serviced -r--r--r-- 1 root root 0 8월 11 13:55 blkio.io_serviced_recursive -r--r--r-- 1 root root 0 8월 11 13:55 blkio.io_service_time -r--r--r-- 1 root root 0 8월 11 13:55 blkio.io_service_time_recursive -r--r--r-- 1 root root 0 8월 11 13:55 blkio.io_wait_time -r--r--r-- 1 root root 0 8월 11 13:55 blkio.io_wait_time_recursive -rw-r--r-- 1 root root 0 8월 11 13:55 blkio.leaf_weight -rw-r--r-- 1 root root 0 8월 11 13:55 blkio.leaf_weight_device --w------- 1 root root 0 8월 11 13:55 blkio.reset_stats -r--r--r-- 1 root root 0 8월 11 13:55 blkio.sectors -r--r--r-- 1 root root 0 8월 11 13:55 blkio.sectors_recursive -r--r--r-- 1 root root 0 8월 11 13:55 blkio.throttle.io_service_bytes -r--r--r-- 1 root root 0 8월 11 13:55 blkio.throttle.io_serviced -rw-r--r-- 1 root root 0 8월 11 13:55 blkio.throttle.read_bps_device -rw-r--r-- 1 root root 0 8월 11 13:55 blkio.throttle.read_iops_device -rw-r--r-- 1 root root 0 8월 11 13:55 blkio.throttle.write_bps_device -rw-r--r-- 1 root root 0 8월 11 13:55 blkio.throttle.write_iops_device -r--r--r-- 1 root root 0 8월 11 13:55 blkio.time -r--r--r-- 1 root root 0 8월 11 13:55 blkio.time_recursive -rw-r--r-- 1 root root 0 8월 11 13:55 blkio.weight -rw-r--r-- 1 root root 0 8월 11 13:55 blkio.weight_device -rw-r--r-- 1 root root 0 8월 11 13:55 cgroup.clone_children -rw-r--r-- 1 root root 0 8월 11 13:35 cgroup.procs -r--r--r-- 1 root root 0 8월 11 13:55 cgroup.sane_behavior -rw-r--r-- 1 root root 0 8월 11 13:55 notify_on_release -rw-r--r-- 1 root root 0 8월 11 13:55 release_agent -rw-r--r-- 1 root root 0 8월 11 13:55 tasks 마운트한 디렉토리에서 내부 리스트를 확인해보니 위와 같이 많은 파일들이 생성되었습니다. 그 중 우리가 write할 수 있는 파일은 몇 개 되지 않네요. 나머지는 모두 read만 할 수 있습니다. 그 말은 우리가 cgroup blkio 서브시스템을 사용하여 제어할 수 있는 요소들은 write 권한이 있는 파일들을 유심히 보면 될 것 같습니다.\n특정 프로세스들을 high_io와 low_io 그룹에 할당하기. 특정 프로세스의 PID가 각각 10001, 10002 라고 해봅시다.  $ echo 10001 \u0026gt;\u0026gt; /cgroup/blkio/high_io/tasks $ echo 10002 \u0026gt;\u0026gt; /cgroup/blkio/high_io/tasks 위와 같이 설정하거나 프로세스의 이름이 test 라고 할 때\n$pidof test | while read PID; do echo $PID \u0026gt;\u0026gt; /cgroup/blkio/high_io/tasks; done 으로도 설정할 수 있습니다.\nhigh_io 와 low_io에 대해 가중치 비율 설정하기  $ echo 200 \u0026gt; /cgroup/blkio/high_io/blkio.weight $ echo 100 \u0026gt; /cgroup/blkio/low_io/blkio.weight 2:1 비율로 가중치를 설정하였습니다.\nblkio 파일 설명 아래 몇 가지 파일에 대한 내용을 표로 작성해두었습니다. 그 외 다른 파일에 대한 설명은 Documentation을 보시면 좋습니다.\n   파일 설명     blkio.weight cgroup에 제공되는 장치에 대한 접근 가중치를 적용할 수 있습니다. 상대적인 비율로 100에서 1000까지 설정이 가능하며 설정하지 않을 시 기본 가중치입니다.   blkio.weight_device blkio.weight와 동일하지만 가중치를 적용할 블록 장치를 지정합니다.   blkio.time device 당 cgroup에 할당된 disk time으로 msec 단위입니다. 특정 장치로의 I/O access 시간을 설정할 수 있습니다. 설정을 위해선 디바이스의 major, minor 번호와 msec가 필요합니다.   blkio.io_service_bytes cgroup에 의해 지정된 디바이스에서 송수신한 바이트 수 입니다.   blkio.serviced cgroup에 의해 디바이스에 실행된 I/O 작업의 수 입니다.   blkio.service_time cgroup에 의해 수행된 I/O 작업 요청의 시작부터 완료까지의 총 시간으로 nsec 단위입니다.   blkio.io_wait_time I/O 작업이 지정된 장치에 대해 스케줄러 큐에서 대기한 전체 시간입니다.   blkio.io_merged I/O 작업에 대한 요청으로 병합된 읽기, 쓰기, 동기화 또는 비동기의 수 입니다.    이 외에도 매우 많은 파일들이 존재하는데요. 필요한 내용은 Documentation을 꼭 참고해주세요.\n참고자료  Containerization with LXC | Konstantin Ivanov [Linux] cgroup - (task) control group (1) (http://egloos.zum.com/studyfoss/v/5505982) [Linux] cgroup - (task) control group (1)(http://studyfoss.egloos.com/5506102) Docker(container)의 작동 원리: namespaces and cgroups(https://tech.ssut.me/what-even-is-a-container/) Control Groups | 문C 블로그(http://jake.dothome.co.kr/control-groups/) CGROUPS(7) linux manual page /Documentation/cgroup-v1/blkio-controller.txt  "
},
{
	"uri": "https://linuxias.github.io/container/namespace/2_pid_namespace/",
	"title": "2. PID Namespace",
	"tags": [],
	"description": "",
	"content": "PID namespace는 프로세스 ID 공간을 격리 시킵니다. 이 말인 즉, 다른 PID namespace의 프로세스들은 같은 PID를 가질 수도 있음을 의미합니다. PID namespace들은 프로세스 집합의 종료, 재시작과 같은 기능을 제공하기 위한 컨테이너를 허용합니다. 또한 컨테이너를 새로운 호스트로 마이그레이션하는 등의 기능을 컨테이너가 제공할 수 있도록 해줍니다.\nPID namepsace의 특이한 점은 새로운 PID namespace의 PID는 1 부터 시작한다는 것입니다. standalone 시스템과 동일하게 각 namespace의 시작 프로세스는 pid를 1번을 가지게됩니다. PID namespace를 사용하기 위해선 CONFIG_PID_NS 커널 옵션을 설정해야 합니다.\nThe namespace init process CLONE_NEWPID flag를 파라미터로 한 unshare(2) 시스템 콜을 호출한 이후, 또는 clone(2) 시스템 콜의 flag로 CLONE_NEWPID를 전달하여 생성한 프로세스는 새로운 Namespace의 첫 번째 프로세스가 됩니다. 이 말인 즉, 이 프로세스의 PID가 1번이라는 것입니다.\n조금 혼란스러울 수 있습니다. 리눅스에서 PID는 고유하며, 프로세스의 식별자로 사용이 되는데, 새로운 Namespace의 첫 번째 프로세스의 PID가 1번이라면, 중복될테니까요. 그 이후 이 프로세스에 자식 프로세스들도 2,3,4\u0026hellip; 와 같은 PID를 가질 수 있다는 말이됩니다. 식별자로써의 가치가 사라지게 되는 것일까요? 조금씩 정리해보도록 하겠습니다.\n새로운 PID namespace의 첫 번째 프로세스의 PID가 1번이라고 말씀드렸습니다. 그 의미는 해당 namespace를 위한 init process가 된다는 의미입니다. 아래 그림처럼 새로운 namespace는 PID 1번부터 시작하게 됩니다. 뭔가 속임수 같나요?\n그림 출처 : https://www.toptal.com/linux/separation-anxiety-isolating-your-system-with-linux-namespaces\n만약 8,1 두 개의 PID를 가진 프로세스에서 getpid(2) 시스템 콜을 호출하게 되면, 어떤 결과가 리턴 될까요? 결과는 1입니다. PID를 이용해 동작하는 시스템 콜들은 항상 호출자의 PID namespace 내에 표시되는 PID를 사용하게 됩니다. 그렇기 때문에 child PID namespace에서 표시되는 1이 반환됩니다.\nnamespace 동작 중에 init process가 종료되면 어떻게 될까요? 만약 PID namespace 내의 init process가 종료된다면, 커널은 SIGKILL 시그널을 통해 해당 namespace 내에 모든 프로세스를 종료시키게 됩니다. 이 의미는 PID namespace가 정상적으로 동작하기 위해선 PID 1의 init process가 필수적이란 의미입니다.\ninit process에 시그널을 보낼 수 있는 경우는 시그널 핸들러에 등록한 시그널들만 PID namespace의 다른 프로세스들에 의해 전달될 수 있습니다. 이러한 제한은 권한이 있는 프로세스들에게도 해당되며 실수로 init process가 PID namespace 내의 다른 멤버 프로세스에 의해 종료되는 것을 막아주게됩니다. 마찬가지로 상위 PID namespace의 프로세스는 자식 PID namespace의 init process가 등록한 시그널 **kill(2)**을 호출하여 전달할 수 있습니다. 여기서 SIGKILL과 SIGSTOP은 예외적으로 처리되는데요, 상위 PID namespace에서 시그널을 전달하면 init process에서는 처리할 수 없기에, 해당 시그널이 처리되어 프로세스 종료 및 중지가 발생하게 됩니다.\nNesting PID namespace PID Namespace는 중첩해서 사용이 가능합니다. 그 말은, 각 PID namespace는 상위(부모) namespace를 가지고 있습니다.(root PID namespace는 제외입니다 :D) PID namepsace의 부모 namespace는 clone(2) 또는 **unshare(2)**를 사용하여 namespace를 생성한 프로세스의 PID namepsace가 됩니다. 이러한 구조는 PID namespace가 트리 자료구조 형태로 이루어져 있습니다. 모든 namepsace는 자신의 상위 namespace들(root namespace 포함)을 언제든 찾을 수 있습니다.\n특정 Namespace에 속한 프로세스는 해당 namespace에 속한 프로세스들과, 상위 모든(root namespace로 가는 경로의) namespace 프로세스들에게 보여집니다. 보여진다는 의미는 해당 프로세스를 타겟으로 작업을 진행할 수 있다는 의미입니다. 하지만 반대로 자식 PID namespace에서는 부모나 제거된 상위 namespace의 프로세스들을 볼 수 없습니다. 정리하면, 프로세스는 오직 자신의 PID namsepace의 프로세스들이나 자식 namespace들의 프로세스들만 볼 수 있습니다.\n특정 PID namespace내의 프로세스들은 가끔 namespace 외부에 부모 프로세스를 가지는 경우가 있습니다. 첫 번째는 위에서 살펴보았듯이, Namespace가 생성된 후 첫 프로세스는 자신을 생성한 프로세스가 부모프로세스가 됩니다. 이 경우엔 부모와 자식 프로세스가 각각 다른 PID namespace에 존재하게 됩니다. 두 번째로 setns(2) 시스템콜을 이용하여 특정 PID namespace로 조인하게 되는 경우입니다. 조인할 수 있는 PID namespace는 자식 PID namespace으로만 가능합니다. 완전 다른 방향의 namespace로는 불가능합니다. 잘 생각하셔야 할게 지금 설명드리는 부분은 namespace간 부모, 자식 관계가 아닌 프로세스의 부모, 자식 관계입니다.\n/proc 파일시스템과 PID namespace /proc 파일시스템은 /proc 파일시스템이 다른 Namespace에서 보여지더라도 마운트를 수행한 프로세스의 PID namespace에 보이는 프로세스만 보여줍니다. 새 PID namespace를 만든 후에는 ps (1)와 같은 툴이 정상적으로 작동하도록 /proc 파일시스템에 새로운 procfs 인스턴스를 마운트하고 루트 디렉토리를 변경하는 것이 좋습니다. clone (2) 또는 unshare(2)의 flags에 CLONE_NEWNS를 포함하여 새로운 마운트 네임 스페이스를 동시에 생성하면 루트 디렉토리를 변경할 필요가 없습니다. 새로운 procfs 인스턴스를 /proc에 직접 마운트 할 수 있습니다.\nmount -t proc proc /proc PID namespace는 container에서 유용하게 사용되는 기술 중 하나입니다. 추가적인 내용들은 정리되는대로 갱신하겠습니다. 글 읽어주셔서 감사합니다.\n감사합니다.\n참조 : https://www.toptal.com/linux/separation-anxiety-isolating-your-system-with-linux-namespaces\n"
},
{
	"uri": "https://linuxias.github.io/container/cgroup/3.cgroup_memory/",
	"title": "3. memory",
	"tags": [],
	"description": "",
	"content": "Subsystem - memory memory 서브시스템은 cgroup에서 사용하는 메모리 자원에 대해 프로세스가 사용하는 메모리 양과 사용 가능한 메모리 자원을 컨트롤 할 수 있습니다. 또한 사용되는 메모리 자원에 대한 레포트도 자동으로 생성해주는 서브시스템입니다.\nmemory 서브시스템은 시스템으로부터 태스크의 그룹의 메모리 접근등의 동작을 격리화시킵니다. 이 서브시스템이 사용되는 경우는 다음과 같습니다.\n 메모리 소모가 많은 어플리케이션을 격리하고 더 작은 어플리케이션으로 제한할 수 있습니다. mem=XXXX 설정을 통해 부팅하는 경우의 좋은 대안이 될 수 있습니다. 가상화 솔루션에서 원하는 메모리 양을 제어할 수 있습니다. 위 내용 외에도 여러 요소가 있을 수 있습니다.  예제 살펴보기  cgroup 가상파일시스템을 마운트합니다.  $ sudo mkdir -p /cgroup/memory $ sudo mount -t cgroup -o memory memory /cgroup/memory 마운트 여부 확인하기. 아래 명령어를 수행해서 결과가 표시된다면 정상적으로 마운트되었습니다.  $ mount | grep cgroup memory on /cgroup/memory type cgroup (rw,relatime,blkio) or $ cat /proc/mount | grep cgroup memory /cgroup/memory cgroup rw,relatime,blkio 0 0 mount 상태 확인해보기.  linuxias@linuxias-VirtualBox:/cgroup/memory$ ls -al total 4 dr-xr-xr-x 2 root root 0 8월 11 13:35 . drwxr-xr-x 4 root root 4096 8월 11 15:49 .. -rw-r--r-- 1 root root 0 8월 11 15:49 cgroup.clone_children --w--w--w- 1 root root 0 8월 11 15:49 cgroup.event_control -rw-r--r-- 1 root root 0 8월 11 13:35 cgroup.procs -r--r--r-- 1 root root 0 8월 11 15:49 cgroup.sane_behavior -rw-r--r-- 1 root root 0 8월 11 15:49 memory.failcnt --w------- 1 root root 0 8월 11 15:49 memory.force_empty -rw-r--r-- 1 root root 0 8월 11 15:49 memory.kmem.failcnt -rw-r--r-- 1 root root 0 8월 11 15:49 memory.kmem.limit_in_bytes -rw-r--r-- 1 root root 0 8월 11 15:49 memory.kmem.max_usage_in_bytes -r--r--r-- 1 root root 0 8월 11 15:49 memory.kmem.slabinfo -rw-r--r-- 1 root root 0 8월 11 15:49 memory.kmem.tcp.failcnt -rw-r--r-- 1 root root 0 8월 11 15:49 memory.kmem.tcp.limit_in_bytes -rw-r--r-- 1 root root 0 8월 11 15:49 memory.kmem.tcp.max_usage_in_bytes -r--r--r-- 1 root root 0 8월 11 15:49 memory.kmem.tcp.usage_in_bytes -r--r--r-- 1 root root 0 8월 11 15:49 memory.kmem.usage_in_bytes -rw-r--r-- 1 root root 0 8월 11 13:35 memory.limit_in_bytes -rw-r--r-- 1 root root 0 8월 11 15:49 memory.max_usage_in_bytes -rw-r--r-- 1 root root 0 8월 11 15:49 memory.move_charge_at_immigrate -r--r--r-- 1 root root 0 8월 11 15:49 memory.numa_stat -rw-r--r-- 1 root root 0 8월 11 15:49 memory.oom_control ---------- 1 root root 0 8월 11 15:49 memory.pressure_level -rw-r--r-- 1 root root 0 8월 11 15:49 memory.soft_limit_in_bytes -r--r--r-- 1 root root 0 8월 11 15:49 memory.stat -rw-r--r-- 1 root root 0 8월 11 15:49 memory.swappiness -r--r--r-- 1 root root 0 8월 11 15:49 memory.usage_in_bytes -rw-r--r-- 1 root root 0 8월 11 13:35 memory.use_hierarchy -rw-r--r-- 1 root root 0 8월 11 15:49 notify_on_release -rw-r--r-- 1 root root 0 8월 11 15:49 release_agent -rw-r--r-- 1 root root 0 8월 11 15:49 tasks 마운트한 디렉토리에서 내부 리스트를 확인해보니 위와 같이 많은 파일들이 생성되었습니다. 그 중 우리가 write할 수 있는 파일은 몇 개 되지 않네요. 나머지는 모두 read만 할 수 있습니다. 그 말은 우리가 cgroup blkio 서브시스템을 사용하여 제어할 수 있는 요소들은 write 권한이 있는 파일들을 유심히 보면 될 것 같습니다.\n특정 프로세스의 메모리 설정하기.  $ sudo mkdir /cgroup/memory/test_process $ sudo echo 1G \u0026gt; /cgroup/memory/test_process/memory.limit_in_bytes $ cat /cgroup/memory/test_process/memory.limit_in_bytes 1073741824 cgroup에 적용하기. test process의 pid가 10001 이라고 할 때 아래와 같이 설정해 줍니다.  $ echo 10001 \u0026gt;\u0026gt; /cgroup/memory/test_process/tasks 위와 같이 설정하거나 프로세스의 이름이 test 라고 할 때\n$pidof test | while read PID; do echo $PID \u0026gt;\u0026gt; /cgroup/memory/test_process/tasks; done memory 서브시스템 statistics 확인해보기. memory.stat 파일에는 다양한 통계 정보가 존재합니다. 아래와 같이 확인 하실 수 있습니다.\n$ cat memory.stat cache 912625664 rss 281018368 rss_huge 0 shmem 1433600 mapped_file 178327552 dirty 0 writeback 0 pgpgin 915786 pgpgout 624369 pgfault 1014204 pgmajfault 2469 inactive_anon 536576 active_anon 280662016 inactive_file 379850752 active_file 521773056 unevictable 10792960 hierarchical_memory_limit 9223372036854771712 total_cache 912625664 total_rss 281018368 total_rss_huge 0 total_shmem 1433600 total_mapped_file 178327552 total_dirty 0 total_writeback 0 total_pgpgin 915786 total_pgpgout 624369 total_pgfault 1014204 total_pgmajfault 2469 total_inactive_anon 536576 total_active_anon 280662016 total_inactive_file 379850752 total_active_file 521773056 total_unevictable 10792960 각 정보에 대한 설명은 Documentation file 5.2 stat file 를 참고해주세요.\nmemory 파일 설명 아래 몇 가지 파일에 대한 내용을 표로 작성해두었습니다. 그 외 다른 파일에 대한 설명은 Documentation을 보시면 좋습니다.\n   파일 설명     tasks 태스크를 추가하고, 스레드의 리스트를 볼 수 있습니다.   cgroup.procs 프로세스들의 리스트를 볼 수 있습니다.   cgroup.event_control event_fd() 를 위한 인터페이스 입니다.   memory.usage_in_bytes 현재 메모리 사용량을 표시합니다.   memory.memsw.usage_in_bytes 현재 memory+swap 사용량을 보여줍니다.   memory.limit_in_bytes 메모리 사용량의 제한을 설정 또는 보여줍니다.   memory.memsw.limit_in_bytes memory + swap 메모리 사용량의 제한을 설정 또는 보여줍니다.   memory.failcnt 메모리 사용량 hits 제한을 보여줍니다.   memory.memsw.failcnt memory + swap 메모리 사용량 hits 제한을 보여줍니다.   memory.max_usage_in_bytes 최대 메모리 사용량을 보여줍니다.   memory.memsw.max_usage_in_bytes memory + swap 메모리의 최대 사용량을 보여줍니다.   memory.soft_limit_in_bytes 메모리 사용의 소프트 제한을 설정 또는 보여줍니다.   memory.stat 다양한 statistics를 보여줍니다.   memory.use_hierarchy 자식 프로세스의 메모리 여부를 설정 또는 보여줍니다.   memory.force_empty 0으로 설정하면 작업에서 사용하는 메모리를 해제합니다.   memory.pressure_level 메모리 입력 알람을 설정합니다.   memory.oom_control oom 제어를 설정또는 보여줍니다.   memory.numa_stat Numa 노드 당 메모리 사용의 수를 보여줍니다   memory.kmem.limit_in_bytes 커널 메모리에 대한 hard limit을 설정 또는 보여줍니다.   memory.kmem.usage_in_bytes 현재 커널 메모리 할당을 보여줍니다.   memory.kmem.failcnt 커널 메모리 사용 hits 제한의 수를 보여줍니다.   memory.kmem.max_usage_in_bytes 커널 메모리 사용의 max 값을 보여줍니다.    이 외에도 매우 많은 파일들이 존재하는데요. 필요한 내용은 Documentation을 꼭 참고해주세요.\n참고자료  Containerization with LXC | Konstantin Ivanov [Linux] cgroup - (task) control group (1) (http://egloos.zum.com/studyfoss/v/5505982) [Linux] cgroup - (task) control group (1)(http://studyfoss.egloos.com/5506102) Docker(container)의 작동 원리: namespaces and cgroups(https://tech.ssut.me/what-even-is-a-container/) Control Groups | 문C 블로그(http://jake.dothome.co.kr/control-groups/) CGROUPS(7) linux manual page https://www.kernel.org/doc/Documentation/cgroup-v1/memory.txt  "
},
{
	"uri": "https://linuxias.github.io/container/namespace/3.user_namespace/",
	"title": "3. User Namespace",
	"tags": [],
	"description": "",
	"content": " 먼저 이 글에서 사용한 코드는 linux kernel 4.16 임을 알려드립니다.\n User namespace는 시큐리티와 관련된 식별자 및 속성을 분리하며, 특히 User ID와 Group ID, 루트 디렉토리, Key, Capability를 분리합니다. 프로세스의 User, Group ID는 user namespace 내,외부적으로 다를수 있습니다. 특히 프로세스는 User namespace 외부에 권한이 없는 정상적인 User ID를 가질 수 있으며, 동시에 namepsace 내부에 User ID 0을 가질 수 있습니다. 즉, 프로세스에는 user namespace 내의 작업에 대한 전체 권한이 있지만 namespace 외부 작업에 대한 권한이 없습니다.\nNested namespaces, Namespace membership User namespace는 PID namespace 처럼 중첩되어 질 수 있습니다. 이 말은 root namespace를 제외하고 각 User namespace는 부모 user namespace를 가질 수 있다는 것입니다. 다른 관점에서 본다면 User namespace는 0개 또는 그 이상의 자식 User namespace를 가질 수 있습니다. 부모 User namespace는 CLONE_NEWUSER flag를 사용한 unshare(2) 또는 clone(2) 시스템콜을 통해 user namespace를 생성하는 프로세스의 namespace입니다. 음, 새로 생성된 namespace는 그 namespace를 생성하는 프로세스의 namespace를 부모 namespace로 가진다는 의미입니다.\n커널은 이렇게 중첩할 수 있는 user namespace의 레벨을 32개로 제한하고 있습니다.\n아래 struct user_namespace를 살펴보시죠. 아래 line.61에 int level; 구조체 멤버변수가 보이시나요? user namespace 는 이처럼 레벨을 관리하고 있습니다.\nFile path : include/linux/user_namespace.h 55 struct user_namespace { 56 struct uid_gid_map uid_map; 57 struct uid_gid_map gid_map; 58 struct uid_gid_map projid_map; 59 atomic_t count; 60 struct user_namespace *parent; 61 int level; 62 kuid_t owner; 63 kgid_t group; 64 struct ns_common ns; 65 unsigned long flags; 66 67 /* Register of per-UID persistent keyrings for this namespace */ 68 #ifdef CONFIG_PERSISTENT_KEYRINGS 69 struct key *persistent_keyring_register; 70 struct rw_semaphore persistent_keyring_register_sem; 71 #endif 72 struct work_struct work; 73 #ifdef CONFIG_SYSCTL 74 struct ctl_table_set set; 75 struct ctl_table_header *sysctls; 76 #endif 77 struct ucounts *ucounts; 78 int ucount_max[UCOUNT_COUNTS]; 79 } __randomize_layout; 만약 limit을 초과하게 되면 EUSERS 에러가 발생하게 됩니다.\n모든 프로세스들은 User namespace 중 하나에 속합니다. 여러분들이 프로세스 생성에 많이 사용하는 fork(2), **clone(2)**을 사용할 때 flag로 CLONE_NEWUSER를 전달하지 않는다면 해당 시스템 콜을 호출한 프로세스의 User namespace에 속하게 됩니다. 싱글스레드 프로세스는 setns(2) 시스템 콜을 사용하여 다른 user namespace로 포함될 수 있습니다. 조건은 setns(2) 시스템 콜을 호출하는 프로세스가 CAP_SYS_ADMIN Capability를 가지고 있어야 합니다.\n여기서 주의할 점은 멀티스레드 프로세스에서는 setns(2) 시스템 콜을 호출한 스레드만 namespace가 변경되어 버립니다. 그럼, 이상한 문제점들이 발생하게 될겁니다.\n하나의 스레드만 적용되는 이유는 아래 코드에서 확인할 수 있습니다. setns(2) 시스템 콜을 호출하게 되면 아래 함수가 수행됩니다. 여기서 line.268을 확인해보시면 현재 task_struct를 가져오게되고, line.283에서 task_struct를 가져와 namespace를 생성하게 되는데, 스레드는 각각의 task_struct를 가지고 있기에, 해당 thread에 대해서만 변경이 됩니다.\nFile path : kernel/nsproxy.c 266 SYSCALL_DEFINE2(setns, int, fd, int, nstype) 267 { 268 struct task_struct *tsk = current; 269 struct nsproxy *new_nsproxy; 270 struct file *file; 271 struct ns_common *ns; 272 int err; 273 274 file = proc_ns_fget(fd); 275 if (IS_ERR(file)) 276 return PTR_ERR(file); 277 278 err = -EINVAL; 279 ns = get_proc_ns(file_inode(file)); 280 if (nstype \u0026amp;\u0026amp; (ns-\u0026gt;ops-\u0026gt;type != nstype)) 281 goto out; 282 283 new_nsproxy = create_new_namespaces(0, tsk, current_user_ns(), tsk-\u0026gt;fs); 284 if (IS_ERR(new_nsproxy)) { 285 err = PTR_ERR(new_nsproxy); 286 goto out; 287 } 288 289 err = ns-\u0026gt;ops-\u0026gt;install(new_nsproxy, ns); 290 if (err) { 291 free_nsproxy(new_nsproxy); 292 goto out; 293 } 294 switch_task_namespaces(tsk, new_nsproxy); 295 296 perf_event_namespaces(tsk); 297 out: 298 fput(file); 299 return err; 300 } Capability CLONE_NEWUSER flag를 이용하여 clone(2) 시스템 콜로 생성된 자식 프로세스는 새로운 User namespace에서 완전한 Capability 집합을 가지고 실행됩니다. 마찬가지로 unshare(2), setns(2) 시스템 콜도 마찬가지로 Namepsace 내부에서 Capability의 전체 집합을 가지게 됩니다.\n반면에, 새로운 네임 스페이스가 생성 되더라도 그 프로세스는 부모 네임 스페이스 (클론 (2)의 경우) 또는 이전 (unshare(2) 및 **setns(2)**의 경우) User namepsace의 CCapability를 갖지 않습니다 또는 루트 사용자 (즉, Root User namepsace에서 사용자 ID가 0 인 프로세스)에 의해 조인됩니다.\n다른 경우로서, **execve(2)**를 호출하면 프로세스의 기능이 일반적인 방법으로 재계산됩니다 이 방식은 이 글의 범위를 벗어나기 때문에 다루지 않겠습니다. 관심있으신 분은 capabilites(7) 을 참조해주세요결과적으로 Namespace 내에 프로세스의 사용자 ID가 0이 아니거나 실행 가능 파일에 비어 있지 않은 상속 기능 마스크가 있으면 프로세스가 모든 기능을 잃게됩니다. 음, 자세한 내용은 아래에서 다시 다루겠습니다.\nclone(2) 또는 unshared(2) 를 이용해 새로운 IPC, mount, network, PID, UTS namespace를 생성할 때 커널은 새로운 Namespace에 대해 생성한 프로세스의 User namespace를 기록합니다. 새로운 Namespace의 프로세스가 나중에 Namespace 내에 격리된 전역 리소스에서 작동하는 권한 작업을 수행하면 커널이 새 Namespace와 연결된 User namespace의 프로세스 Capability에 따라 검사가 수행됩니다. 즉 Namepsace의 Capability는 User namespace와 상호작용하며 체크하게 된다는 것입니다.\nRestrictions on mount namespaces mount namespace 관련하여 정리한 내용입니다.\nmount namespace는 owner user namespace를 가지고 있습니다. owner user namespace가 상위 mount namespace의 owner user namespace와 다른 mount namespace는 권한이 낮은 mount namespace로 간주됩니다. 낮은 권한의 mount namespace가 생성될 때 공유 마운트는 슬레이브 마운트로 축소됩니다. 이렇게하면 권한이 낮은 mount namespace에서 수행 된 매핑이보다 많은 권한을 가진 mount namespace로 전파되지 않습니다.\n더 많은 권한을 가진 마운트에서 하나의 단위로 나오는 마운트는 함께 잠기고 특권이 적은 mount namespace에서 분리되지 않을 수 있습니다.\n파일 및 디렉토리에 대해서는 다른 namespace 마운트 지점이 아닌 하나의 namespace의 마운트 지점인 파일 또는 디렉터리는 마운트 지점이 아닌 mount namespace에서 이름을 변경하거나 연결 해제하거나 제거(rmdir(2))할 수 있다. 다른 mount namespace에서 마운트 포인트의 파일, 디렉토리를 삭제, rename, unlink를 시도하게 되면 EBUSY 에러가 나타납니다. 이런 결과는 특권이 많은 사용자로부터 DoS 공격을 막기 위한 방안입니다.\nInteraction of user namespaces and other types of namespaces User 네임스페이스는 다른 네임스페이스들과 연관관계를 맺고있습니다.\n리눅스 커널 3.8부터 권한이 없는(unprivileged) 프로세스들도 User 네임스페이스를 생성할 수 있습니다. 프로세스가 CAP_SYS_ADMIN Capability 를 가지고 있다면 Mount, PID, IPC, Network, UTS 네임스페이스도 생성할 수 있습니다.\nNon-user 네임스페이스(User 네임스페이스를 제외한 다른 네임스페이스)가 생성될 때 새로운 프로세스는 자신을 생성한 프로세스가 속한 User 네임스페이스에 속하게 됩니다. Non-user 네임스페이스은 User 네임스페이스의 기능이 필요합니다. 만약 ** clone(2) ** 또는 ** unshare(2) ** 호출 시 ** CLONE_NEWUSER ** 가 다른 ** CLONE_NEW* ** 플래그와 함께 명시된다면 User 네임페이스를 먼저 생성하게 됩니다. 그 이후 권한을 확인하여 나머지 네임스페이스의 생성을 하게 됩니다. 따라서 권한이 없는 호출 프로세스는 이와 같은 플래그의 조합으로 시스템 콜을 호출할 수 있습니다.\n새로운 IPC, mount, network, PID, UTS 네임스페이스가 ** clone(2) ** 또는 **unshare(2) ** 시스템 콜을 통해 생성되면 커널은 새로운 네임스페이스에 대해 생성되는 프로세스의 사용자 네임스페이스를 기록합니다. 새로운 네임스페이스의 프로세스가 네임스페이스 격리된 전역 리소스에서 동작하는 권한이 필요한 작업을 연속적으로 수행하면 커널이 새로운 네임스페이스와 연결된 User 네임스페이스의 새로운 프로세스 기능에 따라 권한을 검사하게 됩니다.\nUser 네임스페이스는 다른 네임스페이스들과 연관관계를 맺고 있는 부분을 확인 할 수 있으며, 특권(Capability)와 권한(Privilege)에 대한 관계도 관련이 있음을 알 수 있습니다.\nUser and Group ID mapplings : uid_map and gid_map User 네임스페이스가 생성되면 상위 User 네임스페이스에 User, Group ID가 매핑되지 않고 시작합니다. ** /proc/[pid]/uid_map ** 과 ** /proc/[pid]/gid_map ** 파일은 해당 프로세스의 User 네임스페이스 내부의 사용자와 그룹 ID 매핑 정보를 보여줍니다. 이 정보는 Namespace(1) 글에서 자료구조 내부적으로 어떻게 관리하는지 본 적이 있습니다.\n55 struct user_namespace { 56 struct uid_gid_map uid_map; 57 struct uid_gid_map gid_map; 58 struct uid_gid_map projid_map; 59 atomic_t count; 60 struct user_namespace *parent; 61 int level; 62 kuid_t owner; 63 kgid_t group; 64 struct ns_common ns; 65 unsigned long flags; 66 67 /* Register of per-UID persistent keyrings for this namespace */ 68 #ifdef CONFIG_PERSISTENT_KEYRINGS 69 struct key *persistent_keyring_register; 70 struct rw_semaphore persistent_keyring_register_sem; 71 #endif 72 struct work_struct work; 73 #ifdef CONFIG_SYSCTL 74 struct ctl_table_set set; 75 struct ctl_table_header *sysctls; 76 #endif 77 struct ucounts *ucounts; 78 int ucount_max[UCOUNT_COUNTS]; 79 } __randomize_layout; 위 구조체에서 uid_map, gid_map이 구조체의 멤버로 존재하며, user_namespace 구조체 내부에서 관리하고 있음을 알 수 있습니다.\n"
},
{
	"uri": "https://linuxias.github.io/container/cgroup/4.cpu.cpuset.cpuacct/",
	"title": "4. cpu, cpuset, cpuaccet",
	"tags": [],
	"description": "",
	"content": "Subsystem - cpu, cpuset 이번 글에서는 Cgroup의 서브시스템 중 cpu와 cpuset 에 대해 정리해보겠습니다.\ncpu subsystem cpu 서브시스템은 cgroup 계층 및 해당 작업에 대한 CPU 시간을 스케쥴링 할 수 있습니다. 시스템이 busy 상태일 때 CPU 공유를 최소화 즉 사용량을 제한 할 수 있습니다. 이 서브시스템은 CPU에 cgroup 작업 액세스를 제공하기 위한 스케줄러(Documentation/scheduler/sched-design-CFS.txt)를 제공합니다.\ncpuset subsystem cpuset 서브시스템은 개별 CPU 및 메모리 노드를 cgroup에 바인딩 하기 위한 서브시스템입니다. 리눅스의 testset 명령과 유사하게 CPU 코어를 할당 할 수 있는 서브시스템 입니다.\ncpu, cpuset 이 두 서브시스템을 이용하면 CPU 사용량이 많은 어플리케이션에 대해 프로세서 코어들을 할당 및 제어할 수 있으므로 더욱 좋은 활용성을 가질 수 있습니다.\nimage 참고 : 문C블로그 (http://jake.dothome.co.kr/control-groups/)\nCPU 코어 별로 원하는 Task 들을 할당할 수 있어 코어 간 스위칭 되는 로드를 감소할 수 있으며 입맛에 맞게 그룹을 조정할 수 있습니다. 즉 컨테이너 환경에서도 각 컨테이너 별로 다양한 인스터스를 생성할 수 있습니다.\nCPU 예제 살펴보기 몇몇의 프로세스에 대해 설정하는 방법에 대해 알아보겠습니다. 여기서는 cpu와 cpuacct 함께 설정합니다.\n cgroup 가상파일시스템을 마운트합니다.  $ sudo mkdir -p /cgroup/cpu $ sudo mount -t cgroup -o cpu,cpuacct cpu,cpuacct /cgroup/cpu 마운트 여부 확인하기. 아래 명령어를 수행해서 결과가 표시된다면 정상적으로 마운트되었습니다.  $ mount | grep cgroup cpu,cpuacct on /cgroup/cpu type cgroup (rw,relatime,cpu,cpuacct) or $ cat /proc/mount | grep cgroup cpu,cpuacct /cgroup/cpu cgroup rw,relatime,cpu,cpuacct 0 0 mount 상태 확인해보기.  $ ls -al total 4 dr-xr-xr-x 2 root root 0 8월 11 20:22 . drwxr-xr-x 6 root root 4096 8월 11 20:21 .. -rw-r--r-- 1 root root 0 8월 11 20:22 cgroup.clone_children -rw-r--r-- 1 root root 0 8월 11 20:22 cgroup.procs -r--r--r-- 1 root root 0 8월 11 20:22 cgroup.sane_behavior -rw-r--r-- 1 root root 0 8월 11 20:22 cpuset.cpu_exclusive -rw-r--r-- 1 root root 0 8월 11 20:22 cpuset.cpus -r--r--r-- 1 root root 0 8월 11 20:22 cpuset.effective_cpus -r--r--r-- 1 root root 0 8월 11 20:22 cpuset.effective_mems -rw-r--r-- 1 root root 0 8월 11 20:22 cpuset.mem_exclusive -rw-r--r-- 1 root root 0 8월 11 20:22 cpuset.mem_hardwall -rw-r--r-- 1 root root 0 8월 11 20:22 cpuset.memory_migrate -r--r--r-- 1 root root 0 8월 11 20:22 cpuset.memory_pressure -rw-r--r-- 1 root root 0 8월 11 20:22 cpuset.memory_pressure_enabled -rw-r--r-- 1 root root 0 8월 11 20:22 cpuset.memory_spread_page -rw-r--r-- 1 root root 0 8월 11 20:22 cpuset.memory_spread_slab -rw-r--r-- 1 root root 0 8월 11 20:22 cpuset.mems -rw-r--r-- 1 root root 0 8월 11 20:22 cpuset.sched_load_balance -rw-r--r-- 1 root root 0 8월 11 20:22 cpuset.sched_relax_domain_level -rw-r--r-- 1 root root 0 8월 11 20:22 notify_on_release -rw-r--r-- 1 root root 0 8월 11 20:22 release_agent -rw-r--r-- 1 root root 0 8월 11 20:22 tasks 마운트한 디렉토리에서 내부 리스트를 확인해보니 위와 같이 많은 파일들이 생성되었습니다. 그 중 우리가 write할 수 있는 파일은 몇 개 되지 않네요. 나머지는 모두 read만 할 수 있습니다. 그 말은 우리가 cgroup blkio 서브시스템을 사용하여 제어할 수 있는 요소들은 write 권한이 있는 파일들을 유심히 보면 될 것 같습니다.\n특정 프로세스의 CPU 제한 설정하기. 새로운 계층구조를 생성하고 60%를 할당합니다.  $ sudo mkdir /cgroup/cpu/limit_60_percent $ sudo echo 600 \u0026gt; /cgroup/cpu/limit_60_percent/cpu.shares cgroup에 적용하기. test process의 pid가 10001 이라고 할 때 아래와 같이 설정해 줍니다.  $ echo 10001 \u0026gt;\u0026gt; /cgroup/cpu/limit_60_percent/tasks 위와 같이 설정하거나 프로세스의 이름이 test 라고 할 때\n$pidof test | while read PID; do echo $PID \u0026gt;\u0026gt; /cgroup/cpu/limit_60_percent/tasks; done CPUSET 예제 살펴보기 몇몇의 프로세스에 대해 설정하는 방법에 대해 알아보겠습니다.\n cgroup 가상파일시스템을 마운트합니다.  $ sudo mkdir -p /cgroup/cpuset $ sudo mount -t cgroup -o cpuset cpuset /cgroup/cpuset 마운트 여부 확인하기. 아래 명령어를 수행해서 결과가 표시된다면 정상적으로 마운트되었습니다.  $ mount | grep cgroup cpuset on /cgroup/cpuset type cgroup (rw,relatime,cpuset) or $ cat /proc/mount | grep cgroup cpuset /cgroup/cpuset cgroup rw,relatime,cpuset 0 0 mount 상태 확인해보기.  $ ls -al total 4 dr-xr-xr-x 2 root root 0 8월 11 20:22 . drwxr-xr-x 6 root root 4096 8월 11 20:21 .. -rw-r--r-- 1 root root 0 8월 11 20:22 cgroup.clone_children -rw-r--r-- 1 root root 0 8월 11 20:22 cgroup.procs -r--r--r-- 1 root root 0 8월 11 20:22 cgroup.sane_behavior -rw-r--r-- 1 root root 0 8월 11 20:22 cpuset.cpu_exclusive -rw-r--r-- 1 root root 0 8월 11 20:22 cpuset.cpus -r--r--r-- 1 root root 0 8월 11 20:22 cpuset.effective_cpus -r--r--r-- 1 root root 0 8월 11 20:22 cpuset.effective_mems -rw-r--r-- 1 root root 0 8월 11 20:22 cpuset.mem_exclusive -rw-r--r-- 1 root root 0 8월 11 20:22 cpuset.mem_hardwall -rw-r--r-- 1 root root 0 8월 11 20:22 cpuset.memory_migrate -r--r--r-- 1 root root 0 8월 11 20:22 cpuset.memory_pressure -rw-r--r-- 1 root root 0 8월 11 20:22 cpuset.memory_pressure_enabled -rw-r--r-- 1 root root 0 8월 11 20:22 cpuset.memory_spread_page -rw-r--r-- 1 root root 0 8월 11 20:22 cpuset.memory_spread_slab -rw-r--r-- 1 root root 0 8월 11 20:22 cpuset.mems -rw-r--r-- 1 root root 0 8월 11 20:22 cpuset.sched_load_balance -rw-r--r-- 1 root root 0 8월 11 20:22 cpuset.sched_relax_domain_level -rw-r--r-- 1 root root 0 8월 11 20:22 notify_on_release -rw-r--r-- 1 root root 0 8월 11 20:22 release_agent -rw-r--r-- 1 root root 0 8월 11 20:22 tasks 마운트한 디렉토리에서 내부 리스트를 확인해보니 위와 같이 많은 파일들이 생성되었습니다. 그 중 우리가 write할 수 있는 파일은 몇 개 되지 않네요. 나머지는 모두 read만 할 수 있습니다. 그 말은 우리가 cgroup blkio 서브시스템을 사용하여 제어할 수 있는 요소들은 write 권한이 있는 파일들을 유심히 보면 될 것 같습니다.\n특정 프로세스의 cpucore 할당 설정하기.  $ sudo mkdir /cgroup/cpuset/test $ sudo echo 0-1 \u0026gt; /cgroup/cpuset/test/cpuset/cpus cgroup에 적용하기. test process의 pid가 10001 이라고 할 때 아래와 같이 설정해 줍니다.  $ echo 10001 \u0026gt;\u0026gt; /cgroup/cpuset/test/cpuset.cpus 위와 같이 설정하거나 프로세스의 이름이 test 라고 할 때\n$pidof test | while read PID; do echo $PID \u0026gt;\u0026gt; /cgroup/cpuset/test/cpuset.cpus; done 참고자료  Containerization with LXC | Konstantin Ivanov [Linux] cgroup - (task) control group (1) (http://egloos.zum.com/studyfoss/v/5505982) [Linux] cgroup - (task) control group (1)(http://studyfoss.egloos.com/5506102) Docker(container)의 작동 원리: namespaces and cgroups(https://tech.ssut.me/what-even-is-a-container/) Control Groups | 문C 블로그(http://jake.dothome.co.kr/control-groups/) CGROUPS(7) linux manual page https://www.kernel.org/doc/Documentation/cgroup-v1/cpusets.txt https://www.kernel.org/doc/Documentation/cgroup-v1/cpuacct.txt  "
},
{
	"uri": "https://linuxias.github.io/container/namespace/4.qemu/",
	"title": "4. QEMU",
	"tags": [],
	"description": "",
	"content": "Introduction QEMU는 고속의 동적 바이너리 변환 기법을 사용하는 프로세서 에뮬레이터이자 가상화 하이퍼바이저 입니다. 여기서 바이너리 변환이란 하나의 Instruction Set을 다른 Instruction Set으로 번환해주는 처리과정을 말하며, 정적 또는 동적인 방법이 존재합니다.\nQEMU에 대해 정리하기 앞서 QEMU와 KVM을 굉장히 많은 분들이 헷갈려 하시는데, 아래 블로그에 매우 자세하게 정리가 되어 있으니 참고하시면 좋을 것 같습니다.\n[Qemu와 KVM의 개념 - 에뮬레이션과 가상화][http://blog.naver.com/PostView.nhn?blogId=alice_k106\u0026amp;logNo=221179347223\u0026amp;parentCategoryNo=7\u0026amp;categoryNo=\u0026amp;viewDate=\u0026amp;isShowPopularPosts=true\u0026amp;from=search]\n이 글에서는 KVM이 아닌 QEMU에 관한 내용만 정리하려 합니다.\n처음 QEMU 개발 시에는 x86 기반이 아닌 아키텍처에서 x86 기반의 리눅스 응용 프로그램을 실행하는 목적으로 개발되었다고 합니다. 하지만 지금은 x86, ARM 등 다양한 아키텍처를 지원하고 있습니다.\nQEMU가 에뮬레이터로 사용될 때에는 특정 하드웨어 기반에서 실행 가능하도록 작성된 OS나 프로그램을 아키텍처의 변경없이 다른 하드웨어에서 실행되도록 하는 역할을 합니다. 하이퍼바이저로 사용될 때에는 KQEMU를 통해 게스트 OS의 코드를 호스트 시스템의 CPU에서 직접 실행하기 때문에 높은 성능이 발휘됩니다.\n또한 QEMU는 Xen 또는 KVM 하이퍼바이너의 가상화 기능을 도와주는 역할을 수행하기도 하는데, 이 경우 하드웨어 디바이스들의 에뮬레이터 기능을 수행하기 위해 변형된 형태의 QEMU 버전이 사용됩니다.\nArchitecture 하이퍼바이저에는 두 가지 타입이 있습니다. 베어메탈 (TYPE1)과 호스티드 (TYPE2) 방식입니다. 베어메탈 방식은 하드웨어 위에서 HOST OS없이 바로 실행되는 하이퍼바이저를 말하며, 호스티드 방식은 HOST OS 위에서 하이퍼바이저를 실행하여 GUEST OS를 실행하는 방식입니다. QEMU는 TYPE2 방식에 속합니다. QEMU는 두 가지 모드를 제공하고 있는데요, 전체 시스템 모드와 사용자 모드 에뮬레이션을 제공하고 있습니다. 전체 시스템 모드는 CPU 및 주변 장치들을 포함하는 전체 시스템을 에뮬레이션 합니다. 사용자 모드 에뮬레이션은 사용자가 특정 아키텍처에서 동작시키기 위해 크로스 컴파일 한 프로세스를 다른 아키텍처에서 실행할 수 있도록 해줍니다. 특정 프로그램만 빠르게 동작시켜보기 위해서는 사용자 모드를 사용하는게 편합니다.\nHypervisor QEMU는 KVM과 밀접한 관계가 있습니다. KVM은 리눅스 커널에 포함된 모듈로서 하이퍼바이저입니다. kVM 하이퍼바이저에서의 QEMU는 KVM에 최적화되어 변형된 버전인 ** qemu-kvm ** 이 실행되어서 x86 아키텍처에 대한 에뮬레이션을 제공합니다. QEMU는 호스트와 동일한 아키텍처의 OS를 실행하고자 할 떄만 KVM 가속화 기능을 사용합니다.\n"
},
{
	"uri": "https://linuxias.github.io/container/namespace/5_mount_namespace/",
	"title": "5. Mount namespace",
	"tags": [],
	"description": "",
	"content": "mount namespace는 프로세스와 그 자식 프로세스에게 다른 파일시스템 마운트 포인트를 제공합니다. 각 namespace instance의 프로세스들에게 보여지는 마운트 포인트를 격리시키는 기능을 제공합니다. 이렇게 격리된 마운트 포인트는 각 프로세스에게 단일 디렉토리 구조로 보여지게됩니다.\n처음 설치된 시스템에서는 기본적으로 모든 프로세스가 하나의 mount namespaece(기본 namespace)에 속하기 때문에 파일시스템를 마운트하거나 해제하는 등의 모든 사항을 확인 및 인지할 수 있습니다.\nclone(2) 또는 unshare() 시스템 콜과 CLONE_NEWNS 플래그를 사용하여 새로운 프로세스를 생성하면 생성하는 프로세스의 mount namespace를 그대로 복사하여 생성하게 됩니다.\nmount의 종류 mount 할 때 private, shared, slave 옵션을 이용하여 mount를 할 수 있습니다. 각 방식의 차이는 아래와 같습니다.\n   mount 설명     private mount 각 마운트 포인트가 다른 마운트 포인트에 반영되지 않는 방법   shared mount 각 마운트 포인트가 다른 마운트 포인트에 반영되어 보여지는 방법(양방향)   slave mount A 파일시스템 하위에서 새로운 마운트는 B 파일시스템에 반영되나, 반대는 반영되지 않는 방법(단방향)    manual page를 검색하면 아래와 같이 나뉘어져 있습니다.\n source(A) shared private slave unbind ────────────────────────────────────────────────────────────────── dest(B) shared | shared shared slave+shared invalid nonshared | shared private slave unbindable  Restriction on mount namespace   mount namespace는 owner user namespace를 가지고 있습니다. 부모 mount namespace의 owner user namespace와 다른 mount namespace는 그 권한이 낮다고 판단할 수 있습니다.\n  권한이 낮은 mount namespace를 생성할 때, shared mount namespace는 slave mounts로 권한이 낮아집니다.\n  실습  임시 디렉토리를 생성한다.  linuxias$ mkdir /tmp/mount_ns unshare(1) 를 이용하여 새로운 bash 프로세스를 생성할 때 새로운 마운트 네임스페이스를 생성한다.  linuxias$ sudo unshare -m /bin/bash bash 프로세스가 별도의 네임스페이스에 속함을 확인한다. readlink 명령어를 이용해 네임스페이스의 inode 번호를 확인한다.  root# readlink /proc/$$/ns/mount mnt:[4026532199] 임시 마운트 지점을 생성하여 1번에서 생성한 디렉토리에 마운트 시킨다.  root# mount -n -t tmpfs tmpfs /tmp/mount_ns 새로 생성한 마운트 지점을 확인한다.  root# mount | grep mount_ns or root# cat /proc/mounts | grep mount_ns 새로운 터미널을 열고, 네임스페이스 inode를 확인한다.  linuxias$ readlink /proc/$$/ns/mount mnt:[4026531840] 새로운 터미널에서 마운트 지점을 확인한다.  linuxias$ mount | grep mount_ns or linuxias$ cat /proc/mounts | grep mount_ns 위 실습을 통해 알 수 있는 것은 새로 생성한 mount namespace 내에서 새로운 마운트 지점을 생성해도 기본 namespace에서는 알 수 없다는 것이다. 즉 독립화 되어 동작하게 됩니다.\n참고자료 및 문헌  Konstantin Ivanov | Containerization with LXC linux manual page - mount_namespace  "
},
{
	"uri": "https://linuxias.github.io/container/namespace/6.uts_namespace/",
	"title": "6. UTS namespace",
	"tags": [],
	"description": "",
	"content": "UTS(Unix TimeSharing) namespace는 리눅스 컨테이너가 ** hostname -f ** 명령어에 의해 반환된 결과 값인 자신의 식별자를 유지관리하기 위해 hostname과 domainname을 namespace 별로 격리해줍니다. 격리하는 대상은 구별을 위한 식별자가 됩니다. 단순하게 생각하면 hostname은 호스트 각각의 이름이고 domainname은 그룹의 이름입니다. 기본적으로 domainname을 설정하지 않았다면 none으로 표시됩니다.\nUTS namespace로 분리할 수 있는 식별자는 sethostname(2)과 setdomainname(2)을 사용하여 설정되며 unname(2), gethostname(2) 및 getdomainname(2)을 사용하여 검색할 수 있습니다.\nunshare 또는 clone 시스템 콜 호출 시에 CLONE_NEWUTS 플래그를 사용하면 새로운 UTS namespace를 생성하고 해당 namespace를 가진 프로세스를 생성할 수 있습니다.\n실습  ** unshare ** 를 이용하여 새로운 UTS namespace를 만들어서 bash 프로세스를 실행합니다. 그 후 namespace를 변경합니다.   linuxias@linuxias-VirtualBox:~$ hostname linuxias-VirtualBox linuxias@linuxias-VirtualBox:~$ sudo unshare -u /bin/bash [sudo] password for linuxias: root@linuxias-VirtualBox:~# hostname linuxias-VirtualBox root@linuxias-VirtualBox:~# hostname uts-namespace root@linuxias-VirtualBox:~# hostname uts-namespace root@linuxias-VirtualBox:~# cat /proc/sys/kernel/hostname uts-namespace root@linuxias-VirtualBox:~# 새로운 터미널 세션을 생성하여 hostname을 확인해봅니다. hostname이 설정 전의 hostname임을 확인할 수 있습니다.  linuxias@linuxias-VirtualBox:~$ hostname linuxias-VirtualBox 참고자료 및 문헌  Konstantin Ivanov | Containerization with LXC linux manual page - namespace, hostname  "
},
{
	"uri": "https://linuxias.github.io/container/namespace/7.cgroup_namespace/",
	"title": "7. Cgroup Namespace",
	"tags": [],
	"description": "",
	"content": "cgroup namespace는 프로세스의 cgroups의 뷰를 가상화 합니다.\n각 cgroup 네임스페이스에는 고유한 cgroup 루트 디렉토리 세트가 있습니다. 그 루트 디렉토리는 /proc/[pid]/cgroup 파일의 해당 레코드에 표시되는 상대 위치의 베이스 위치입니다. clone(2) 또는 unshare(2) 시스템 콜에 CLONE_NEWCGROUP 플래그를 전달하여 새로운 cgroup namespace를 생성할 수 있습니다. 이렇게 생성된 namespace는 현재 cgroups 디렉토리가 새 namespace의 cgroup 루트 디렉토리가되는 새 cgroup namespace로 들어갑니다. 이 정책은 cgroup 버전 1,2 모두에 적용됩니다.\n/proc/[pid]/cgroup 을 확인하면 표시되는 각 레코드의 세 번째 필드에 표시된 경로 이름은 해당 cgroup 계층에 대한 읽기 프로세스의 루트 디렉토리에 상대적인 위치입니다.\ncat /proc/$$/cgroup 12:pids:/user.slice/user-1000.slice/session-2.scope 11:memory:/ 10:cpuset:/ 9:hugetlb:/ 8:blkio:/ 7:freezer:/ 6:cpu,cpuacct:/ 5:devices:/user.slice 4:perf_event:/ 3:net_cls,net_prio:/ 2:rdma:/ 1:name=systemd:/user.slice/user-1000.slice/session-2.scope 0::/user.slice/user-1000.slice/session-2.scope 만약 해당 프로세스의 cgroup 디렉토리가 읽은 프로세스의 cgroup namespace의 외부에 위치한다면 경로는 ../ 와 같이 cgroup 구조에서 상위 레벨로 표시될 것 입니다.\n"
},
{
	"uri": "https://linuxias.github.io/container/docker/",
	"title": "Docker",
	"tags": [],
	"description": "",
	"content": "Docker This chapter is for Docker I will writes archites about Docker install, remove, configruation, etc ..\n"
},
{
	"uri": "https://linuxias.github.io/books/readyforprogramminginterview/",
	"title": "프로그래밍 면접 이렇게 준비한다",
	"tags": [],
	"description": "",
	"content": "목차\nCHAPTER 1 구직을 시작하기 전에\n너 자신을 알라\n시장을 알라\n팔릴 만한 능력을 계발하라\n일 제대로 해내기\n온라인 프로파일을 정돈하라\n요약\nCHAPTER 2 입사 지원 절차\n회사 선택 및 접촉\n면접 절차\n리크루터의 역할\n근무 조건 협상\n요약\nCHAPTER 3 전화 예비 면접\n전화 예비 면접의 이해\n전화 예비 면접 방법\n전화 예비 면접 문제\n요약\nCHAPTER 4 프로그래밍 문제 접근법\n절차\n문제 해결\n풀이 분석\n요약\nCHAPTER 5 연결 리스트\n왜 연결 리스트인가?\n연결 리스트의 종류\n기초적인 연결 리스트 연산\n연결 리스트 문제\n요약\nCHAPTER 6 트리와 그래프\n트리\n그래프\n트리 및 그래프 문제\n요약\nCHAPTER 7 배열과 문자열\n배열\n문자열\n배열과 문자열 문제\n요약\nCHAPTER 8 재귀 호출\n재귀 호출의 이해\n재귀 호출 문제\n요약\nCHAPTER 9 정렬\n정렬 알고리즘\n정렬 문제\n요약\nCHAPTER 10 동시성\n스레드 기본 개념\n동시성 문제\n철학자들의 저녁 식사\n요약\nCHAPTER 11 객체지향 프로그래밍\n기본 원리\n객체지향 프로그래밍 문제\n요약\nCHAPTER 12 디자인 패턴\n디자인 패턴이란 무엇인가?\n일반적인 디자인 패턴\n디자인 패턴 문제\n요약\nCHAPTER 13 데이터베이스\n데이터베이스의 기초\n데이터베이스 문제\n요약\nCHAPTER 14 그래픽스와 비트 조작\n그래픽스\n비트 조작\n그래픽스 문제\n비트 조작 문제\n요약\nCHAPTER 15 데이터 과학, 난수, 그리고 통계학\n확률과 통계\n인공지능과 기계학습\n난수 생성기\n데이터 과학, 난수, 통계 문제\n요약\nCHAPTER 16 카운팅, 측정 및 순서 관련 퍼즐\n퍼즐 공략법\n퍼즐 문제\n요약\nCHAPTER 17 그림 및 공간 퍼즐\n일단 그려보자\n그림 및 공간 퍼즐 문제\n요약\nCHAPTER 18 지식 기반 문제\n준비\n문제\n요약\nCHAPTER 19 기술과 무관한 질문\n왜 기술과 무관한 질문이 필요할까?\n질문\n요약\nAPPENDIX A 이력서\n기술 이력서\n이력서 예\n취업을 준비하는 대학생들 중 많은 학생분들이 어떻게 취업을 준비해야 하는지, 면접은 어떻게 진행이 되는지 모르는 경우가 많습니다. 특히 전공 지식을 요구하는 소프트웨어 지식 고나련 인터뷰는 어떻게 진행되는지, 어떤 질문이 나올지 막연하고, 두려움도 있습니다.\n특히 실무 경험이 없는 학생분들의 경우, 책과 학교 실습으로 배운 내용이 완전히 이해하지 못하는 경우도 많고 이해하고 있다고 한들 누군가가 질문을 했을 때 논리 정연하게 자신의 지식을 말하는 기술이 부족합니다. 위에 작성한 내용에 포함되지 않는 학생 분들도 분명 있겠지만, 소수일 거라 생각됩니다.\n학생 외에도 현업에 종사하는 개발자 분들과의 대화에서도 종종 느낍니다. 가끔 자신의 실력에 자신감있는 개발자분들을 뵐 때가 있습니다. 그때 이직에 대한 대화주제에서 누군가 면접 인터뷰 경험을 얘기하고 자신이 받은 질문을 공유하는 경우가 있습니다. 그 질문에 대해서 \u0026lsquo;쉽네요?\u0026rsquo; 라고 대답하시는 분들이 계십니다. 정말 그 분들은 쉬울수도 있지만 10명 중 9명 정도는 답변 부탁드린다고 했을 때 알고있는 지식을 논리 정연하게 설명하지는 못하는 경우가 많았습니다. 저는 자신이 아는 것과 누군가에게 설명하는 것은 명확하게 다르다고 생각합니다. 이 책을 보시려고 고민하는 분들이라면 취업 준비 중인 대학생 또는 이직을 준비하는 현업 개발자 분일거라 생각합니다. 만약 그렇다면 저자가 부탁하는 과정을 꼭 지키면 큰 도움이 될 것 같습니다.\n  문제를 읽은 다음 바로 책을 덮어놓고 직접 문제를 풀어본다.\n  문제를 풀다가 막히면 풀이를 읽어본다.\n  풀이를 읽다가 필요한 힌트가 나왔다 싶으면 다시 책을 덮고 문제를 풀어본다.\n  위 과정을 반복한다.\n  저는 위 저자의 부탁과 추가로 \u0026lsquo;생각을 정리하고, 직접 소리내어 답변해본다\u0026rsquo; 도 함께 해보셨으면 좋겠습니다. 저도 면접 경험이 4~5회 정도 있습니다. 준비를 하는 과정 중 가장 아쉬운 것이 왜 아는 걸 제대로 말하지 못하였나.. 라는 후회감이였습니다. 꼭 누군가에게 정확하게 설명할 수 있는 연습을 하셨으면 좋겠습니다.\n추가로 이 책은 가이드는 주나 여기 나오는 문제가 면접에 꼭 질문받을 거란 생각을 하시는 분은 없을거라 생각합니다. 특히 책에서는 다양한 분야를 다루기에 그 질문의 깊이가 깊지는 않습니다. 이 책은 분위기가 초급 난이도의 질문 방향, 요구하는 답변등을 정리하는 책으로는 좋습니다. 특히 취업 준비하는 대학생 분들은 꼭 읽으셨으면 좋겠습니다.\n이 책은 프로그래밍 면접에도 도움이 되지만 컴퓨터 공학 전공 기초에 대해서 한번 훑고 기억을 되살리는 목적으로도 좋습니다. 가벼운 마음으로 기본 소양을 되새긴다는 마음으로 읽으셔도 좋을 것 같습니다.\n오랜만에 도서 리뷰를 작성하니 주저리주저리 정리가 안되는 것 같네요. 읽어주셔서 감사합니다.\n"
},
{
	"uri": "https://linuxias.github.io/linux/compilelink/gcc_warning_option/",
	"title": "[gcc] Warning 옵션",
	"tags": [],
	"description": "",
	"content": "gcc 컴파일 옵션으로 많이 사용하는 -Wall , -Wextra 이외에 다양한 옵션들은 정리하고자 합니다.\n   옵션 설명 특이사항     -fstack-usage 컴파일러가 프로그램에 대한 스택 사용 정보를 함수 단위로 출력하도록 합니다. 함수의 이름, 바이트 수 등이 표기됩니다. x   -Wframe-larger-than={len} 함수 프레임의 크기가 len을 넘어가면 Warning이 출력합니다. x   -Wstrict-overflow=n -fstrict-overflow가 활성화 되어있는 경우에만 활성화됩니다. 컴파일러가 signed 오버플로우가 일어나지 않을거라 가정하고 최적화를 진행하는 경우에 Warning이 발생합니다. Wall에 포함 (-Wstrict-overflow=1)   -Wlogical-op 표현식에서 논리연산자의 사용에 대해 문제가 발생할 수 있는 경우 Warning을 발생합니다. x   -Wjump-misses-init goto문을 통해 변수 초기화 이전으로 분기하거나, 변수가 초기화된 이후로 분기하는 것을 Warning이 발생합니다. C, Objective-C only   -Wmissing-include-dirs 사용자 제공 include 디렉토리가 존재하지 않으면 Warning이 발생합니다. C/C++, Obj C/C++ only   -Wunused 여러 unused 옵션(unused-but-set-parament, unused-but-set-variable, unused-function, unused-label, unused-local-typedefs, unused-parameter, -no-unused-result, unused-variable, unused-value)을 한 번에 포함하는 옵션입니다. 사용되지 않는 함수 매개 변수에 대한 경고를 얻으려면 -Wextra -Wunused (-Wall implies -Wunused)를 지정하거나 -Wunused-parameter를 별도로 지정해야합니다 Wall에 -Wunused-function -Wunused-label -Wunused-value -Wunused-variable 옵션이 포함되어 있습니다.   -Wpacked-bitfield-compat 4.1, 4.2 및 4.3 계열의 GCC는 \u0026ldquo;char\u0026quot;유형의 비트 필드에서 \u0026ldquo;packed\u0026quot;속성을 무시합니다. GCC 4.4에서 그러한 필드의 오프셋이 변경되면 GCC에서 알려줍니다. Default enable   -Winvalid-pch precompile된 헤더가 검색 경로에서 찾았으나 사용하지 못하는 경우 Warning이 발생합니다. -   -Wstack-protector Stack smashing으로부터 보호되지 않는 경우 Warning이 발생합니다. Stack smashing protector(SSP) 기능은 -fstack-protector 옵션을 사용해야 합니다. SSP 는 함수 진입 시 스택에 return address와 frame pointer 정보를 저장할 때 이 정보를 보호하기 위해 (canary라고 부르는) 특정한 값을 기록해두고 함수에서 반환할 때 기록된 값이 변경되지 않았는지 검사하여 정보의 일관성을 관리합니다. 만약 악의적인 사용자가 buffer overflow 등의 공격을 통해 스택 내의 정보를 덮어쓰려면 canary 값을 먼저 덮어써야 하기 때문에 canary 값 만 보면 공격이 일어났는지를 알 수 있습니다. -fstack-proctector가 활성화 된 경우 활성화 됩니다.   -Wunused-variable 지역변수 또는 상수가아닌 정적변수가 사용되지 않을 때 Warning이 발생합니다. -Wunused , -Wall에 포함   -Wunused-value 명시적으로 사용되지 않은 결과를 계산하는 경우 Warning이 발생합니다. -Wunused , -Wall에 포함   -Wcast-qual 포인터를 형변환 할 때 기존 type qualifier가 사라지는 경우 경고해줍니다(const char_을 char_로 형변환) -   -Wconversion 묵시적으로 타입을 변환하는 상황에서 값이 바뀔 가능성이 있는 경우 경고해줍니다(int temp = 0.3 등) -   -Wsign-conversion 부호있는 정수 표현식을 부호없는 정수 변수에 할당하는 것과 같이 정수 값의 부호를 변경할 수있는 변환에 대해 Warning이 발생합니다. -Wconversion 옵션에 의해 활성화됩니다.   -Wbad-function-cast 함수 콜이 매칭할 수 없는 타입에 캐스팅된 경우 Warning이 발생합니다. -   -Wwrite-strings constant 스트링을 non-const char* 포인터에 복사하는 경우 Warning이 발생합니다. 컴파일 타임에 const string을 변경하려는 문제를 찾을 수 있습니다. -   -Wconversion-null NULL 과 non-pointer 타입간의 변환에 대해 Warning이 발생합니다. Default enable   -Wextra -Wall에 의해 활성화되지 않는 추가적인 Warning flags(-Wclobbered -Wempty-body -Wignored-qualifiers -Wmissing-field-initializers -Wmissing-parameter-type (C only) -Wold-style-declaration (C only) -Woverride-init -Wsign-compare -Wtype-limits -Wuninitialized -Wunused-parameter (only with -Wunused or -Wall) -Wunused-but-set-parameter (only with -Wunused or -Wall))를 활성화합니다. -   -Wpacked 구조체에 packed 속성이 주어졌으나, 해당 레이아웃이나 크기에 영향이 없는 경우 Warning이 발생합니다. 이런 구조체는 아주 작은 이득을 위해 잘못 정렬될 수도 있습니다. -   -Wredundant-decls 유효범위 내에 동일한 오브젝트(변수 등)이 여러 번 선언된 경우 Warning이 발생합니다. -   -Waggregate-return 구조체 또는 공용체를 반환하는 함수를 선언하거나 호출 시 Warning이 발생합니다. 거의 사용하지 않는 Warning.. ANSI C 표준에 맞추기 위해 사용하나.. 흠,   -Wpointer-arith void의 크기나 함수의 크기를 갖고 연산(+/- 등)을 하는 경우 Warning이 발생합니다. -   -Wswitch-default switch 문에서 default case가 존재하지 않는 경우 Warning이 발생합니다. -   -Wundef 정의되지 않는 식별자가 #if, #endif 구문에서 사용된 경우 Warning이 발생합니다 -   -Wstrict-prototype 함수가 인자 형을 명시하지 않고 선언, 정의된 경우 Warning이 발생합니다. 즉, void test()와 같이 ()로 인자를 비워둔 경우 발생합니다. -   -Wfloat-equal 부동소수점 값이 ==, != 등의 등호로 비교된 경우 Warning이 발생합니다. -   -Wformat-y2k 2자리 연도를 출력하는 strftime()에 대해 Warning이 발생합니다. -   -Wshift-count-overflow Shift 연산이 타입의 크기와 같거나 큰 경우 Warning이 발생합니다. Default enable   -Wswitch-bool switch문이 boolean 타입의 인덱스를 가지는 경우 Warning이 발생합니다.    -Wno-conversion-null NULL과 non-pointer 타입 간의 변환에 대해 Warning이 발생하지 않도록 합니다. -Wconversion-null이 Default입니다.   -Wnested-externs extern 선언이 함수 안에 존재하는 경우 Warning이 발생합니다. -   -Wvarargs va_start와 같은 가변인자를 처리하는데 사용된 매크로의 의심스러운 사용에 대해 Warning이 발생합니다. Default enable   -Wunsuffixed-float-constants 접미사가 없는 부동상수에 대해 Warning이 발생합니다. -   -Wswitch-enum switch문에서 index로 enum을 사용한 경우에 enum의 멤버와 case의 수가 맞지 않을 때 Warning이 발생합니다. -   -Wshadow 지역변수가 다른 지역변수, 매개변수 등(shadow) 덮는 경우 Warning이 발생합니다.    -Wunreachable-code 어떠한 경우에도 실행할 수 없는 코드 라인이 존재하는 경우 Warning이 발생합니다. gcc 4.4 이상에서 제거됨   -Winline inline으로 선언된 함수가 inline이 불가능 경우 Warning이 발생합니다.    -funroll-loops for()와 같은 루프문을 풀어서 최적화 해주는 옵션으로, 코드를 크게 만들어주며 수행속도가 빨라질수도 아닐수도 있습니다.     감사합니다.\n"
},
{
	"uri": "https://linuxias.github.io/linux/compilelink/gcc_specific_attribute/",
	"title": "[gcc] 최적화 속성 사용하기",
	"tags": [],
	"description": "",
	"content": "gcc를 컴파일러로 사용할 시 -O2, -O3와 같은 최적화 옵션을 자주 사용하게 됩니다.\n특별한 경우에 특정 함수나 코드 구간에 대해서 특정 최적화 단계를 적용하고 싶다면 아래와 같이 진행합니다.\n함수에 최적화 적용하기 void __attribute__((optimize(\u0026#34;O0\u0026#34;))) func(void) { } 코드 범위에 최적화 적용하기 #pragma GCC push_options#pragma GCC optimize (\u0026#34;O0\u0026#34;) //Write your code  #pragma GCC pop_options감사합니다.\n"
},
{
	"uri": "https://linuxias.github.io/linux/compilelink/staticliblink/",
	"title": "정적 라이브러리 링크",
	"tags": [],
	"description": "",
	"content": "라이브러리 중 정적 라이브러리(static library)를 사용하는 경우가 종종 있다. 그리고 해당 라이브러리가 또 다른 (static library)를 Link 하는 경우가 있다. 이럴 때 문제가 발생하게 된다. 아래와 같이 어플리케이션이 필요로 하는 정적 라이브러리를 링크하게 되는데, 해당 라이브러리가 다른 정적 라이브러리를 링크하게 되는 경우이다.\nApplication \u0026ndash;\u0026gt; Static library 1 \u0026ndash;\u0026gt; Static library 2\nApplication은 Static library 1의 존재만 알 뿐 2의 존재는 알지도, 알 필요도 없다.\npkg-config를 이용한 경우 pc 파일에 정의 아래 libpalosalodp 의 pc.in 파일이다.\n# Package Information for pkg-config prefix=@prefix@ exec_prefix=${prefix} libdir=${exec_prefix}/lib includedir=${prefix}/include/ Name: libtest Description: test library Version: 0.0.1 Requires.private: libtest_internal \u0026gt;= 0.0.1 Libs: -L${libdir} -ltest Cflags: -I${includedir} \\  -I${includedir}/Include Requires.private 에서 libodp_internal_${sdktype} 에 대한 필요 정보를 명시하고, 다른 Application의 Makefile에서 pkg-config \u0026ndash;static 옵션을 붙여서 추가 시 해당 lib을 추가해준다.\nRequires.private 은 이 패키지에 필요한 패키지 목록을 나타내는 것으로 동적 링크된 실행 파일(static이 지정되지 않은 경우)에 대해 플래그 목록을 계산할 때 고려되지 않는다.\nRequires 사용하기 # Package Information for pkg-config prefix=@prefix@ exec_prefix=${prefix} libdir=${exec_prefix}/lib includedir=${prefix}/include/ Name: libtest Description: test library Version: 0.0.1 Requires: libtest_internal \u0026gt;= 0.0.1 Libs: -L${libdir} -ltest Cflags: -I${includedir} \\  -I${includedir}/Include Requires 옵션을 이용하여 포함시킨다.\nRequires와 Requires.private의 차이는 아래와 같다.\n  Requires: 패키지가 의존하여 사용하는 다른 패키지의 이름의 목록으로 공백으로 구분한다. 비교 연산자(=, \u0026lt;, \u0026gt;, \u0026lt;=, \u0026gt;=)를 사용하여 버전을 지정할 수도 있다.\n  Requires.private: 패키지가 의존하여 사용하는 다른 패키지의 이름의 목록으로 공백으로 구분한다. 단 Requires와 다르게 패키지 안에서만 사용하며 이 패키지를 가져다가 사용하는 애플리케이션에는 사용할 필요 없을 패키지를 나열한다. 버전을 지정하는 형식은 Requires와 동일하다.\n  하지만 정적 라이브러리를 링크하고 있는 상황이기 때문에, Requires를 사용하던지 Application은 pkg-config 에서 \u0026ndash;static 을 사용해야만 Requires.private에 정의된 패키지를 사용할 수 있다.\n"
},
{
	"uri": "https://linuxias.github.io/",
	"title": "Developer&#39;s Delight",
	"tags": [],
	"description": "",
	"content": "Developer\u0026rsquo;s Delight Seungha Son  Expert knowledge and hands-on experience of system programming, application developing, debugging and profiling and optimization based linux. Programming skills in C (advanace level), and C++, Java, Python, Shell script (Intermediate level) Interested in open-source, system architecture  Career  2020.02 ~ Current, Software Engineer, Tizen Platform lab, Samsung Research, Samgsung Electronic.  System performance optimization and profiling   2018.03 ~ 2020.01, Software Engineer, Mac Platform, Network Business, Samgsung Electronic. 2016.03 ~ 2018.02, Software Engineer, Tizen Platform lab, Samsung Research, Samgsung Electronic. 2014.02 ~ 2016.02, Human Resource Management, Software Membership team, Samsung Research, Samgsung Electronic. 2012.07 ~ 2013.12, Member, Samsung Software Membership  Education  2017 ~ 2019, Yonsei Universiry master’s degree, Major : Computer Science 2007 ~ 2014, Korea Education \u0026amp; Technology University, Major : Mechatronics Engineering  "
},
{
	"uri": "https://linuxias.github.io/machinelearning/basic/model_representation_and_cost_function/",
	"title": "1. Model Representation / Cost function",
	"tags": [],
	"description": "",
	"content": "아래 내용은 Andrew Ng 교수님의 강의와 자료를 기반으로 학습한 내용을 정리하여 작성하였습니다.\n개인의 학습 내용이기에 잘못 해석 및 이해하고 있는 부분도 있을 수 있으니, 다양한 자료를 기반으로 참고하시는 걸 추천드립니다.\nMachine Learning 중 Supervised Learning에서 가장 기초적으로 다뤄지는 내용이 Linear regression(선형 회귀)입니다.\n제 개인적인 의견으로 Linear Regression은 모델표현이 쉽고 단순하며 Cost function과 Gradient descent 등을 이해하기 쉽기 때문이라고 생각됩니다.\n 위의 그래프를 보시면, Portland의 집 크기에 따른 가격 Data를 표현한 그래프입니다. 위 Data들을 보고 친구가 만약, 1250 feet^2의 집을 사고 싶다고 한다면 여러분은 얼마라고 예측해서 대답 할 수 있나요? 대부분의 사람들이 위의 Data들을 표현할 수 있는 Model을 찾게 될 것이고, 위 그림과 같이 하나의 직선을 그어서 예측할 수 있을 것입니다. 그 후 친구에게 1250 feet^2은 22만 달러 정도 할 거라고 말해 줄 수 있겠죠.\n 위 과정이 Data들을 표현할 수 있는 Model을 선정하는 것, 즉 Data들을 표현할 수 있는 Algorithm을 선정하는 것인데요,.\nSupervised Learning 중 Regression 이렇게 data set(right answer)가 주어져 있고, 이를 기반으로 값을 예측하는 과정을 말합니다.\n용어 정리 먼저, 본격적으로 시작하기 전에 사용하는 용어를 정리해 보도록 하죠, 위에서 예시로 든 Data들을 사용하여 살펴보면, m : 학습 데이터의 숫자\nx\u0026rsquo;s : 입력 변수 또는 feature\ny\u0026rsquo;s : 출력 변수 또는 feature\n라고 표현하고, (x, y)를 하나의 학습 데이터로 표현하며 (x(i), y(i)) 을 i 번째의 학습 입/출력 데이터를 표현합니다. 위의 그림과 같이 x(1)은 첫 번째 학습 데이터 중 입력 변수를 말함으로 x(1) 은 2104 이며, 그에 맞는 출력 변수, 결과 값은 y(1), 460 이 되죠.\n어떻게 표현할 것 인가? 그럼 가지고 있는 Data들을 이용해서 어떻게 표현을 할 것인가? Machine Learning을 할 것인가에 대해 위 그림으로 알아보면, Training Set(학습 데이터)들을 가지고 위에서 선정한 모델, 즉 직선의 방정식을 Learning Algorithm으로 선택할 수 있고, 학습된 부분을 h(hypothesis, 가설) 이라고 말할 수 있습니다. 그 이후 입력된 집의 크기에 대해 예측된 가격을 구할 수 있죠, 이 때 hypothesis는 error가 최소인 모델이여야 할 것입니다..\nCost function (비용함수) 그럼 위에서 직선의 방정식을 Learning Algorithm으로 선정하였고, 위와 같이 표현할 수 있다고 합시다. 이 때, hypothesis의 입력에 대한 출력의 error를 최소화 하기 위한 theta 0와 theta 1을 찾아야 합니다. 어떻게 찾아야 할까요?\n위 그림을 보면, hypothesis 모델, Linear regression을 위한 직선의 방정식은 여러 형태로 표현할 수 있습니다. theta 0와 theta 1에 따라 다양하게 표현될 수 있음을 알 수 있는데, 주어진 Training data에 대해서 위의 직선들이 모두 같은 결과를 나타내진 않을 것이며, 이 때 error을 최소화 하기 위해 theta0와 theta1을 결정하는 과정은 필수적입니다! 현재의 theta가 Training Set을 가장 잘 표현하고 있는가! 를 알 수 있는 방법으로 Cost function이 사용됩니다. 위 그림에서 Cost function은 **J(theta)**로 표현됩니다. 주어진 Training data들을 가장 적은 오차로 표현할 수 있는 방법을 사용하게 되며, Linear Regression은 Cost function으로 squared error function 을 사용합니다. squared error function 이란 위와 같이 최초 설정한 h(x)에 대해 Training data의 입력값을 넣었을 때 출력값과, 실제 해당 입력값에 대한 Training Data의 출력값의 차의 제곱을을 이용하는 방법입니다. 그렇다면, 전체 Training data에 대해 위와 같이 Cost function의 값을 구하고 그 값을 최소화 하면 우리가 원하는 theta0, theta1을 구할 수 있음을 알 수 있습니다.\n다양한 h(x)에 대해서 Cost function의 그래프가 어떻게 표현되는지 살펴보시죠.\n예를 들어 m = 3이며, (x,y) = {(1,1), (2,2), (3,3)} 이라 하고, J는 squared error function 을 사용한다고 하면,\n(1) h(x) = x , (theta0 = 0, theta1 = 1)\n주어진 Training data에 대해 J를 구하면, theta 1 = 1일 때, J는 0.\n(2) h(x) = 0.5x (theta0 = 0, theta1 = 0.5)\n주어진 Training data에 대해 J를 구하면, theta 1 = 0.5일 때, J는 0.5.\n위 처럼 theta = 0인 상태에서, theta1의 값을 계속 변경하게 되면 J(theta1)에 대해서 아래와 같은 그래프가 그려짐을 알 수 있습니다.\n위 그래프에서 minimize J(theta1)은 theta1이 1 일 때 임을 매우 쉽게 확인할 수 있습니다.\n그럼, 위의 예시로 나온 집 크기에 따른 가격 Training data를 이용해서 theta0와 theta1을 구하는 과정을 살펴보도록 하겠습니다.\n만약, h(x) = theta0 + theta1 * x 에 대해 theta0 = 50, theta1 = 0.06이라 가정하면 위와 같은 hypothesis를 구할 수 있습니다. 그에 대한 J(theta0, theta1)값은 오른쪽에 표시된 그래프인데, 유의할 점은 theta0는 고려하지 않은 theta1에 대한 J의 그래프 란 것입니다. 이렇게 보면 굉장히 단순하고 최소값을 찾기 쉽지만, theta0까지 함께 고려한다면, J의 그래프는 아래와 같이 곡면(?)이 될 수 있습니다. 아래와 같은 형태를 Contour plots 또는 Contour figure 라고도 합니다.\n그럼 이제 Cost function으로 구한 J값을 최소화 하기 위해 어떤 과정이 일어나는 알아보시죠.\n최초 무작위로 선정한 theta0, theta1에 대한 hypothesis와 J가 아래와 같을 때, J를 최소화 하기 위해선, J가 Contour plot의 중앙, 즉 등고선의 중심으로 이동해야 한다. 총 4개의 과정으로 J가 감소해가면서, hypothesis가 어떻게 변하는지 확인 할 수 있습니다.\n우리가 원하는 모습은 바로 위의 그래프처럼 입력된 Training Data에 대한 error가 최소화 될 수 있는 직선의 방정식, 즉 hypothesis를 구하는 것임을 알 수 있다! 위에서 Cost function을 이용해서 J를 구하고, theta0, theta1에 변화에 따라 J가 감소되는 것 까진 확인하였는데, 그럼 어떻게 J를 감소시켜야 할지, theta0와 theta1은 어떻게 결정해야 할지를 알아봐야 합니다. 그 내용은 다음 글에서 계속 설명 드리겠습니다.\n"
},
{
	"uri": "https://linuxias.github.io/machinelearning/agent/bdi_architect/",
	"title": "[Agent] BDI Architecture",
	"tags": [],
	"description": "",
	"content": "Belief - Desire - Intention Architect BDI Architect는 Software Agent 분야에서 자주 사용되었던 구조입니다. 이 구조의 이름인 BDI 는 3가지 단어 입니다. Belief, Desire, Intention 이 3가지 단어의 앞자리를 따서 만들어졌습니다. 그럼 3가지 단어가 이 구조의 큰 요소일 텐데 구조를 정리해 가며 설명드리겠습니다.\nBDI Architect는 목표를 이루기 위해 순간, 순간 행해야할 행위를 결정하는 의사결정 프로세스입니다. BDI 구조는 reactive behavior와 goal-directed behavior가 조화를 이루는 구조로써 Agent는 그 목표를 달성하기 위해 최선을 다하면서도 그 목표가 여전히 유효한지, 달성할 수 있는지에 대해 계속해서 확인하는 과정을 거치게 됩니다.\nBDI agent는 두 개의 중요한 프로세스를 사용합니다. Deliberations와 Means-ends reasoning인데요, Deliberation은 우리가 달성하기 원하는 목표가 무엇인지 결정하는 프로세스이며 Means-ends reasoning은 어떻게 우리가 그것을 달성할지에 대해 결정하는 것입니다. 이름은 어려운데 결국 어떤 목표를 어떻게 달성해야 할지에 대한 것을 프로세스를 나눠놓은 것이라 생각하시면 편합니다. 어떤 목표를 세우고 어떻게 달성하지에 대해 BDI에서는 3가지 개념을 이용해 정의해 놓았습니다. 가장 처음 말씀드린 BDI 입니다. 하나의 예시를 개념 별로 들어보겠습니다.\n  B (Beilive) : 만약 내가 공부를 열심히 한다면 이번 시험을 패스할 수 있습니다.\n  D (Desire) : 이번 시험을 통과하길 바랍니다.\n  I (Intend) : 나는 공부를 열심히 할겁니다!!\n  사람이 무엇인가 목표를 세우고, 목표를 이루기 위해 어떻게 해야할지 결정하는 과정과 유사한 것 같습니다. BDI가 이 구조에서 어떤 역할을 하는지 정리해야 할 것 같습니다. Belief는 Agent가 가진 믿음, 정보입니다. 여기서 이 정보가 100% True일 필요는 없으며 Agent가 어떠한 정보를 가지고 있다라는 것이 중요한 점이라고 생각됩니다. 조금 의아하시겠지만, 아래서 더 자세히 설명드리겠습니다. Desire은 목표입니다. 이번 시험에서 통과하는게 목표가 되겠네요. 그리고 Intend는 목표를 이루기 위해 행동하는 방안입니다.\n위에서 너무 나불나불 한 것 같아서 예시를 한번 들어보겠습니다. 예시는 호텔 매니저로 하겠습니다. 여러분들이 호텔 매니저라면 B, D, I 가 무엇이 되어야 할지 한번 생각해보시죠. 먼저 호텔 매니저는 룸 관리를 해야 할 것 입니다. 예약 상태나 현재 어느 객실에 고객이 숙박하고 있는지 등의 관리가 필요할 것입니다. 저는 아래와 같이 정리하였습니다.\n  B (Beilive) : 룸 상태와 스케쥴 정보(예약 상태, 현재 숙박 여부 등)를 알고 있어야합니다.\n  D (Desire) : 고객 문의에 대해 정확한 룸 상태와 예약 여부등을 알려주고 싶습니다.\n  I (Intend) : 룸 상태가 청결하며 현재 예약 여부 및 숙박 상태를 정리해 예약을 받아야 합니다.\n  뭔가 정리가 잘 안된 것 같지만.. 얼추 이해가 되시나요? 더 좋은 의견 있으시면 댓글로 부탁드려요 :D\n계속해서 설명드리겠습니다. Intention의 역할에 관한 것입니다. Intention은 목표를 이루기 위한 행위라고 하였습니다. 어떻게 달성할지에 대한 행위라고 했죠. 만약 이 행위가 실패한다면 또 다른 방법으로 목표를 달성하기 위해 행동해야 할 것 입니다. 이 행위는 목표를 달성하기 위해 쉽게 포기해서는 안될 것 입니다. 그렇다고 무조건적으로 유지되는 것도 아니죠. 만약 더 좋은 정보로 인해 목표 달성에 좋아진다면 행위가 변경될 수 있습니다. 아까 공부에서 열심히 공부한다도 좋겠지만, 시험 감독이 없다 라는 정보가 있다면(잘못된 정보라 하더라도) 컨닝을 한다 라는 행위가 발생할 수 있다는 것이죠. 최종 목표는 시험을 통과하는 것이닌깐요. 이러한 Intention은 추후 practical reasoning을 기반으로 Belief에 영향을 주게됩니다.\n그럼 얼마나 자주 현재 진행하고 있는 Intention이 재검토 될까요? 현재 결정된 Intention이 목표 달성에 불가능한 행위인데도 불구하고 계속 진행한다면 잘못된 것일테니 분명 재검토는 필요합니다. 공부를 계속해도 안된다면 컨닝으로 방향을 틀어서라도 목표를 달성해야죠!! (물론 잘못된 행동이지만요..) 재고의 빈도에 대해서는 2가지 Agent로 나뉠 수 있습니다. Bold agent와 Cautious agent인데요. 먼저 Bold agent는 대범한 에이전트입니다. 거의 재고하는 일이 없는 녀석이죠. 더이상 불가능하거나 달성할 이유가 없더라도 우직하게 수행해 나가는 에이전트입니다. 다른 하나는 Cautious agent입니다. 조심스러운 녀석으로 Intentions 수행위해 계속하여 시간을 투자해서 끊임없이 재고함으로써 결국 목표를 달성하지 못하는 에이전트입니다. Intention을 재고하는 가장 좋은 방법은 Bold도 Cautious도 아닌 둘의 적절한 균형이 필요할 것 같습니다. Agent는 항상 동일한 환경과 동일한 외부자극을 받는게 아니니 환경과 외부자극에 대해 현재 Intention을 검토할 필요가 있습니다. The rate of world change(r) 라는 값을 이용하게 됩니다. r이 낮다면 bold agents 쪽으로 더욱 강화하면 좋습니다. 외부 환경 변화가 적은편이니 현재 Intention을 유지해 나가면 좋죠, 반대로 r이 높다면 Cautious agent 쪽에 중점을 두면 좋습니다. 새로운 변화와 자극은 기회가 될 수 있고 어떠한 이점이 있는지 검토가 필요하닌깐요. r에 변화에 따라 어느 방향으로 나아갈 지 결정된다면 좋을 것 같네요.\n그럼 이제 BDI Architecture의 큰 그림을 살펴보겠습니다.\n  A belief revision function (brf)\n- 외부환경, 자극, 입력과 현재 Beliefs를 이용해 정보를 표현합니다.\n  A set of current beliefs\n- 현재 Beliefs의 집합\n  An option generation function\n- 환경 및 현재 Intention들 그리고 Belief 들을 기반으로 할 수 있는 Options들을 결정하게 됩니다.\n  A set of current desires (options)\n- 현재 Desire들의 집합입니다.   A filter function\n- Intentions을 표현하기 위한 녀석으로 Deliberation Process를 나타냅니다. 이때 현재 Belief, Desire, Intention 모두를 사용합니다.\n  A set of current intentions\n- 현재 Intection들의 집합\n  An action selection function\n- 현재 Intention들을 기반으로 수행할 Action을 선택합니다.\n  BDI Architecture에 대해 간단하게 정리해보았습니다. Belief, Desire, Intention을 기반으로 입력, 외부환경, 자극에 대해 최종적으로 목표를 이루기 위한 Output이 결정되는 모델이라는 점에 주목하면 좋을 것 같네요. 부족한 점이나 수정해야 될 부분이 있다면 언제든지 댓글 부탁드립니다. 감사합니다.\n"
},
{
	"uri": "https://linuxias.github.io/machinelearning/agent/agent_communication/",
	"title": "[Agent] Communication",
	"tags": [],
	"description": "",
	"content": "Communication between agents 에이전트들간의 커뮤니케이션을 알아보기 이전에 한 가지 용어를 정리하고자 합니다. \u0026lsquo;Speech Act\u0026rsquo; 란 용어입니다. Speech Act는 한국어로 언어행위론 이라고 정의할 수 있습니다. 언어행위론이란 언어를 통해 이루어지는 행위를 말합니다. 나는 너를 용서한다 라고는 말하면 말로써 행위가 표현되듯 언어가 어떤 영향을 주는지에 대해 초점이 맞춰져 있습니다. 명령, 요구 등으로 나뉠 수 있죠.\n그럼 이제 커뮤니케이션에 대해 정리해보겠습니다. 예를 들어서 살펴보죠. 개인비서 에이전트가 있다고 합니다. 이 에이전트에게 여러분은 김군과 저녁 약속을 잡아달라고 요청합니다. 그럼 이 에이전트를 어떠한 것들이 필요할까요? 간단히 생각해 보세요. 김군의 저녁 일정들이 필요할 것 같네요. 그리고 약속을 위한 약속장소의 정보도 필요할 것 같습니다. 약속을 잡기위한 목표를 이루기 위해 MAS가 필요할 것 같네요. 약속을 잡기 위한 전체 흐름을 살펴보죠.\n먼저 DA에게 김군 일정을 관리하는 에이전트가 누군지 문의합니다. 김군의 에이전트를 모르니 정보를 알아야겠죠. DA는 모든 정보를 관리하는 중계 에이전트라고 생각하시면 됩니다. DA에서 전달받은 김군 에이전트의 정보를 이용해 약속 가능한 일정을 문의하고 답변받은 리스트 중 선택한 요일을 알려줘 약속을 최종적으로 잡게되는 흐름입니다. 생각보다 단순하죠? 위 그림에서 저렇게 에이전트간 커뮤니케이션를 어떻게 이루어지는지에 대해 살펴보게 될 것 입니다. 여기서 목표는 김군과 약속을 잡는 것으로 김군 캘린더에 나의 시간을 할당받는 것입니다. 목표를 이루기 위한 Intention으로 김군의 캘린더 에이전트를 찾는 것도 있을테구요. 그 Intention은 Speech act로 이루어 집니다. DA에게 누가 김군 캘린더를 관리하니? 란 Speech가 행위로 연결되죠.\n에이전트간 커뮤니케이션을 위한 언어가 정의되어 있습니다. Agent Communication Language입니다. ACL은 다른 위치와 행동을 하는 에이전트들 간의 커뮤니케이션을 할 수 있도록 해주고 각 에이전트가 가진 정보와 지식을 교환할 수 있도록 지원해주는 역할을 합니다. 사람의 언어와 똑같은거죠. 처음엔 커뮤니케이션을 Remote Procedure Calls나 Remote Method Invocation과 같은 방식을 사용했습니다. 하지만 사람과 유사한 방식을 만들고자 노력하였고 유연하면서도 요청이나 역할을 다룰수 있는 방식으로 진화하며 만들어졌습니다.\nMAS에서 커뮤니케이션은 가장 처음 설명한 Speech act 이론에서 영감을 받아 만들어졌습니다. 사람들이 매우매우 Goal과 Intention을 달성하기 위해 어떻게 언어를 사용하였는지 확인하였고 사람과 가장 유사한 방식이길 바랬습니다. 일반적으로 Speech Act는 2가지 요소로 나눠볼 수 있습니다. Performative verb(실행자)와 Propositional content(상태)입니다. 실행자는 Request, Inform 등을 행하게되고, Content는 그 정보가 되는 것이죠. 단순 예시를 들어보겠습니다.\nSpeech Act에 따라 Performative는 문에 대한 요청이나 정보, 문의등이 되고 Content는 그에 따른 상태표현이 됩니다. 이렇게 Speech act는 물리적인 행위로서 간주될 수 있습니다. 행위는 선행조건(precondition) 또는 후행조건(postcondition)으로 특화될 수 있습니다. 만약 A가 B에서 t를 요청한 경우에 대해 선행 조건은 \u0026lsquo;A는 B가 t를 할 수 있다고 믿는다.\u0026rsquo; , \u0026lsquo;A는 B가 t를 할 수 있다는걸 B가 믿는다는걸 믿는다\u0026rsquo; , \u0026lsquo;A는 A가 t를 원한다고 믿는다\u0026rsquo; 는 조건이 될 수 있다. 후행조건은 \u0026lsquo;A가 t를 원한다고 A가 믿는걸 B는 믿는다\u0026rsquo; 라는 의미입니다. 음, 설명해 놓고나니 무슨 말인지 잘 모르겠네요. 다음에 좀 더 좋은 예시가 있다면 수정하겠습니다.\nSpeech Act를 기반으로 제작된 ACL은 다른 에이전트들과 효율적으로 커뮤니케이션하거나 지식 정보를 교환할 수 있도록 해줍니다. ACL의 3가지 측면(Syntax, Semantics, Pragmatics)에서 바라보면 Syntax는 어떻게 커뮤니케이션의 실볼들이 구조화되는지하는 것이며, Semantic은 심볼이 나타내는 것이 무엇인지, Pragmatics는 심볼을 어떻게 이해하는지에 관한 관점입니다. ACL로서 대표적인 언어 중 하나로 KQML이 있습니다. KQML은 (Knowledge Query and Manipulation Language)의 약자인데요, 에이전트 간의 지식과 정보의 교환을 위해 설계된 통신 언어입니다. 일반적으로 질문, 선언, 신뢰, 요구, 획득, 묘사, 제공 등과 같은 정보에 대한 상태를 교환하는 데 사용되는 일종의 메시지 포멧이라고 보시면 됩니다. KQML의 카테고리는 아래와 같습니다.\nKQML에는 송신자, 수신자, 그들의 주소 등 통신과 관련된 요소들을 나열한 통신 계층과 수행어로써 메시지의 성질을 정의하는 메세지 계층, 실제적인 메시지가 들어 있는 내용 계층이 있습니다. 또한 커뮤니케이션 프로토콜을 만들기 위해 도와주는 Facilitator도 포함하고 있습니다. Facilitator는 유용한 커뮤니케이션 서비스를 제공하기 위핸 에이전트들의 특별한 그룹입니다. 여기서 말하는 유용한 커뮤니케이션 서비스란 서비스 네임등록을 유지하거나 메시지를 서비스들에게 전달하거나 Content 기반의 메시지들을 Routing 하는 기능, 찾고자하는 자와 정보를 가진 에이전트를 매칭시켜주거나 하는 역할을 말합니다. Facilitator의 예시를 한번 보겠습니다.\nA, B란 에이전트가 있고 F란 이름의 Facilitator가 있다고 가정합니다.  A-B간 Point-to-Point Protocol  graph LR; A[A] --\u0026gt; |ask X| B[B] B --\u0026gt; |tell X|A[A] Point-to-point protocol에선 A가 B에게 X에 대한 정보를 얻기 위해 ask(X)를 다이렉트로 하고 B또한 A에게 tell(X)로 받아들입니다.\nUsing the subscribe performative  graph LR; A[A] --\u0026gt; |subscribe ask X| F[F] B --\u0026gt; |tell X|F F --\u0026gt; |tell X|A A -\u0026gt; F : subscribe(ask(X)) A가 F에게 ask(X)에 대한 구독을 요청합니다. B- \u0026gt; F : tell(X) B가 F에게 X에 대한 정보를 알립니다. F -\u0026gt; A : tell(X) F가 A에게 X의 정보를 알려줍니다. 위 방식은 F를 통해 구독을 요청하고 정보를 F를 통해 받는 방식입니다. A, B간에는 어떠한 경우도 다이렉트로 커뮤니케이션 하는 일이 없습니다.\nUsing the broker Performative  graph LR; A[A] --\u0026gt; |broker ask X| F[F] B --\u0026gt; |advertise ask X|F[F] F --\u0026gt; |tell X| A[A] F --\u0026gt; |ask X| B[B] A -\u0026gt; F : broker(ask(X)) A가 F에서 X를 해결해줄 에이전트를 찾음. B -\u0026gt; F : advertise(ask(X)) B는 F에게 X를 처리할 수 있다고 자신을 광고하고 알림 F -\u0026gt; A : tell(X) and F -\u0026gt; B : ask(X) 위 구조는 F가 브로커 역할을 하여 A와 B의 요청을 각각 처리해 줍니다.\nUsing the recruit performative  graph LR; A[A] --\u0026gt; |recruit tell X| F[F] B --\u0026gt; |tell X| A[A] B --\u0026gt; |advertise ask X|F[F] F --\u0026gt; |ask X| B[B] A -\u0026gt; F : recruit(tell(X)) tell(X)를 처리해줄 수 있는 적절한 에이전트를 찾아달라고 요청합니다. B -\u0026gt; F : advertise(ask(X)) B는 F에게 ask(X)를 자신이 처리할 수 있는 에이전트라고 광고합니다. F -\u0026gt; B : ask(X) A에게 요청한 ask(X)에 적절한 B 에이전트를 찾았고 B에게 ask(X)를 보낸다. B -\u0026gt; A : tell(X) F를 거치지 않고 B가 A에게 다이렉트로 응답합니다.\nUsing the recommend performative  graph LR; A[A] --\u0026gt; |recommend ask X| F[F] B --\u0026gt; |recommend advertise ask X|F[F] F --\u0026gt; |reply B| A[A] A --\u0026gt; |ask X| B[B] B --\u0026gt; |tell X| A[A] A -\u0026gt; F : recommend(ask(X)) A가 F에게 ask(X)를 처리해줄 수 있는 에이전트를 추천해 달라고 요청한다. B -\u0026gt; F : advertise(ask(X)) B는 F에게 ask(X)를 처리해 줄 수 있는 에이전트라고 광고한다. F -\u0026gt; A : reply(B) F는 A에게 B가 알맞은 에이전트라고 알려준다. A -\u0026gt; B : ask(X) B -\u0026gt; A : tell(X)\n위와 같이 A,B란 에이전트와 F (Facilitator)의 관계에서의 예시를 살펴보았습니다. 조금 이해가 되시나요?\n그럼 KQML이 아닌 FIPA ACL에 대해 정리해보겠습니다. FIPA는 The Foundation for Intelligent Physical Agents의 약자입니다. FIPA의 목적은 에이전트 기반의 어플리케이션, 서비스들의 성공을 위해 만들어진 곳입니다. 공공의 이익을 위한 집단입니다. FIPA가 집단이면 FIPA ACL은 FIPA에서 만든 ACL이겠죠? FIPA ACL은 KQML과 매우 유사합니다. Performative도 존재하구도 Content도 존재하구요. 메시지 구조는 봉투 형태를 띄고 있습니다. 메시지 내용은 봉수 내에 들어있고 봉투에는 전송 정보만 작성되어 있는 형태입니다. FIPA ACL에서 기본적인 Performative는 Inform 과 Request입니다. Inform과 Request의 의미는 두 부분으로 정의할 수 있습니다. Precondition(사전조건, speech act가 성공하기 위해 진실이여야만 하는 것들) 과 Rational effect(합리적 영향, 그 메신지의 발신자가 가져오길 바라는 것)입니다. KQML과 FIPA ACL을 잠시 비료해 보죠.\nKQML과 FIPA ACL의 유사한 점은 Outer Language(Performative)와 Inner Language(Content)가 분리되어 있으며 어떤 content language도 허용한다는 점입니다. 차이점은 KQML은 Performative를 FIPA ACL은 communicative act를 좀 더 중점에 두고 있다는 것과 다른 프레임워크 사용으로 KQML과 FIPA Performative 간 정확한 매핑이나 전송이 불가능하며 KQML은 facilitator를 지원하지만 FIPA ACL는 그렇지 않다는 점이죠. KQML은 무조건 중계자가 있어야 함으로 중앙 집중식 구조가 되며, Facilitator에 로드가 크다는 단점이 있을 수 있습니다.\n에이전트 간 커뮤니케이션하기 위한 방법으로 Speech Act를 기반으로 만들어진 ACL과 ACL 중 가장 많이 사용되는 KQML, FIPA ACL에 대해 정리해보았습니다.\n긴 글 읽어주셔서 감사합니다. 수정되어야 할 부분이나 질문있으시면 댓글로 언제든지 부탁드립니다.\n"
},
{
	"uri": "https://linuxias.github.io/machinelearning/agent/agent_mas/",
	"title": "[Agent] MAS (Multi-agent System)",
	"tags": [],
	"description": "",
	"content": "MAS (Multi-agent System) 현재 세계의 시스템에선 하나의 agent로는 모든 처리를 할 수 없습니다. 그래서 각각의 기능을 담당하는 여러 agent를 생성하여 협업 또는 조율하게 만드는 방법을 사용하기도 합니다. 이번 글에서는 MAS, Multi-agent System에 대해 정리해보겠습니다.\nMulti-agent System (지금부턴 MAS라 칭하겠습니다.)은 시스템 내에 여러 agent를 가지고 있는 시스템입니다. 각 agent 간 커뮤니케이션을 통해 상호작용을 하게 되고, 서로 다른 작업을 하기에 \u0026ldquo;spheres of influence\u0026rdquo; 라고 불리는 자신만의 환경 영역을 가지고 있습니다. 즉 변화하는 환경에 대해 영향을 받거나 주는 환경이 다를 수 있습니다. 분산 AI 측면에서 MAS를 살펴보면 하나의 Agent로는 해결하기 어려운 문제들을 여러 Agent가 서로 보유한 다른 정보들과 능력을 이용하여 해결하게 됩니다. 문제의 해를 찾기위해 함께 동작하게 되며 서로간은 루즈하게 결합되어 있습니다. 각각의 agent는 문제 해결을 위해 불완전한 기능들을 가지고 있습니다. 하나의 agent만으로는 문제 해결이 불가능하단 얘기이죠. 또한 MAS는 중계자, 전체 시스템을 컨트롤하는 녀석이 별도로 존재하지 않고 각 데이터도 agent들에게 분산되어 있기에 탈 중앙화 시스템입니다. 이러한 탈 중앙화로 보안성과 안정성이 확보되었습니다. 개별적으로 agent가 동작할 수 있기에 비동기적으로 동작되는 시스템입니다.\nMAS에서 agent가 모두 협렵하는 방식으로 구성되는 건 아닙니다. agent들이 서로 경쟁하도록 만들수도 있죠. 문제해결을 위한 방법 중 하나인데요. 하나의 문제를 협력하여 해결하는게 아니라 서로 경쟁시킴으로써 가장 좋은 해를 구할 수 있게 됩니다. 뭐가 더 좋은지는 상황에 따라 다르겠지요. 좀 더 자세히 Cooperative MAS와 Competitive of Self-interested MAS에 대해 정리해 보죠 Cooperative MAS  대표적인 MAS domain입니다. 분산 문제 해결로 각 agent는 자율성이 낮습니다. 협력과 팀웍을 위한 모델로써 각자 새우는 계획이 다릅니다. 다른 동작을 하기 때문이죠.  Competitie of Self-interested MAS  분산된 합리성입니다. 투표나 경쟁하는 시스템입니다. 협상 등에 자주 사용되는 모델입니다.  MAS의 전통적인 모델은 서버-클라이언트 모델이였습니다. 서버-클라이언트 간 커뮤니케이션 또한 매우 Low-level의 메시지들을 사용하였으며 동기적으로 동작하였습니다. 클라이언트가 요청하면 서버가 응답하는 구조였죠. 하지만 지금은 여러 Agents들이 Peer-to-Peer로 연결되어 있으며 서버라는 개념이 없습니다. 캡슐화된 메시징 기법으로 보안성도 강화화였으며 High-level의 메시지 프로토콜들을 사용하게 되었습니다. MAS를 연구하는 사람들은 Agent간 커뮤니케이션을 매우 중요하게 생각하였습니다. 그래서 멀티 시스템에 맞는 Communication Language, Interaction Protocol 들을 개발하게 되었지요. 개발에 영감을 준건 곤충의 군집생활입니다. 이런 MAS와 같은 시스템에 좋은 본보기가 될 수 있죠. 공통의 목적을 위해 어떻게 군집활동을 하고 어떻게 커뮤니케이션하며 어떠한 룰을 정하는가와 같은 점을 토대로 만들었습니다.\n그럼 유명한 MAS 몇개 모델만 간단히 정리해보겠습니다. Object Manager Group (OMG) 일반적인 패턴들과 정책들을 사용하여 협업하는 에이전트들과 에이전시들로 구성되어 있습니다. 에이전트는 능력, 상호협력의 타입으로 특정지어지며 에이전스들은 에이전트들의 동시 실행, 보안 등을 지원하는 녀석입니다. FIPA(Foundation for Intelligent Physical Agents)\u0026lsquo;s Model Agents, Agent Platform, Directory Facilitator, Agent Management System, Agent Communication Channel, Agent Communication Language 로 구성되어 있는 모델입니다.\nKAoS\u0026rsquo;S Model 에이전트를 위한 개방형 분산 아키텍쳐입니다. 다양한 에이전트 구현들을 정의하고 있으며 에이전트 간 커뮤니케이션을 위해 대화 정책을 사용합니다.\nOAA Model User Interface, NL to ICL, Application, Meta에이전트들과 이 에이전트들이 커뮤니케이션을 하는 Facilitator 에이전트로 구성되어 있습니다. 어플리케이션은 API를 통해 Application Agent를 사용하게 되고 User Interface 에이전트는 사용자의 요청등을 처리하게 됩니다. 각 에이전트간의 커뮤니케이션은 모두 Facilitator 에이전트가 중재하게 되는 구조입니다.\nGeneral Magic\u0026rsquo;s Model 전자상거래를 위한 금융 에이전트로 시작되었습니다. Zeus (MAS development toolkil) MAS 개발을 위한 개발도구 입니다.\nMAS의 각 Agent들은 서로 조직을 이루기 위해 다른 기능을 가지고 있는데요, 그 기능들 사이의 종속성들을 관리하는 과정을 Coordination이라고 합니다. Coordination을 통해 순서나 병렬적 수행들을 관리할 수 있죠. 이런 Coordination은 Activity, Conversation, Implementation 3가지 측면에서 살펴보겠습니다.   Activity\n실행해야 할 행위가 무엇인지 언제 실행되어야 하는지에 대한 것입니다. 분산 태스크에 대해 Statechart나 Flowchart, process algebra 등으로 표현할 수 있습니다.\n  Conversation (State)\n협력하는 개체들간의 대화 구조가 무엇인지에 대한 것입니다. FSM, Petri-Nets, State Transition Diagram 등이 있습니다.\n  Implementation\n어떻게 분산 시스템을 구현할지 협력하는 행위들의 요소들이 어디에 위치해야 할지에 대한 것입니다.\n  또한 MAS는 기본 협업 메커니즘(Coordination mechanism)에 기초한 지식을 교환하여 단일 목표에 도달하기 위해 에이전트 간 협업을 합니다. 대표적인 예시가 CNP(Contract Net protocol)입니다. CNP는 4개의 Phase로 구성되어 있습니다.\n  Phase 1 - Task Announcement\n계약 에이전트가 태스크를 수행할 능력이 있는 에이전트들이 나타나도록 브로드캐스팅합니다. 그 태스크를 수행하기 위한 잠재적 후보자들을 평가하기 위함입니다.\n  Phase 2 - Submission of Bids / Proposal\n요구를 만족하고 태스크를 수행할 능력이 있는 에이전트들은 계약 에이전트에게 입찰을 하게 됩니다. 본인이 해당 태스크를 처리할 수 있도록 말이죠.\n  Phase 3 - Selection\n계약 에이전트는 받은 입찰들을 기반으로 최종 후보자를 선택하는 과정을 진행합니다.\n  Phase 4 - Contract awarding\n계약 에이전트, 최종 선택된 에이전트간 계약이 성립되고 상호 커뮤니케이션을 위한 채널이 생성됩니다.\n  위와 같은 단계를 거쳐 Collaboration이 일어나게 됩니다. 일반 사회에서 일어나는 경매 시스템과 비슷한 것을 느끼시나요? 그에 맞춰서 이해하셔도 좋을 것 같습니다.\n이 글의 앞부분에서 MAS는 Cooperative와 Competitive 모델이 있다고 했습니다. Competitive보단 Cooperative가 많이 사용되는데 Competitive 모델을 디자인하는데 몇 가지 이슈를 확인해보겠습니다. 첫 번째 이슈는 Distributed Rationality입니다. Distributed Rationality는 Self-interested Agent 들이 샌드 박스에서 공정하게 경쟁하도록 유도하거나 강제하는 기술입니다. 모든 에이전트들의 의견을 취합하는 투표방식이나 모든 에이전트들이 얼마나 공정하게 기회를 부여받을지에 대한 경쟁등이 있습니다. 공정성, 안정성, 거짓말, 전역적으로 사용성을 높이기 위한 방안 등에 대한 이슈입니다. 두 번째 이슈는 Pareto optimality(파레토 효율성)입니다. Pareto Optimality는 [1]어떤 경제주체가 새로운 거래를 통해 예전보다 유리해지기 위해서는 반드시 다른 경제주체가 예전보다 불리해져야만 하는 자원배분상태를 의미합니다. 최적화를 위해서 단편적으로 보아서는 안되며 여러 측면에서 고려가 필요한 문제입니다. 마지막으로 Stability(안정성)입니다. 만약 어느 에이전트가 특정 전략을 가지고 다른 에이전트들의 행동과 관련없이 사용성을 항상 최대화 할 수 있다면 어떻까요? 다른 에이전트의 전략을 고려할 때 각 에이전트의 전략이 지역 최적인 경우 에이전트 전략 집합은 [2]내쉬 균형(게임이론의 개념으로서 각 참여자(Player)가 상대방의 전략을 주어진 것으로 보고 자신에게 최적인 전략을 선택할 때 그 결과가 균형을 이루는 최적 전략의 집합을 말한다)에 있습니다. 그렇게 되면 전략을 변경하려는 에이전트도 없을 것입니다. MAS를 어떤 플로우로 개발하는지 살펴보겠습니다.\n  특정 문제를 해결하기 위한 MAS 구성을 정의한다.\n  Coordination mechanism을 결정한다.\n  MAS를 구현하기 위한 프레임워크(Zeus 같은)를 선택한다.\n  MAS를 지원할 Collaborative mechanism을 구현한다.\n  Shared ontology를 구현한다.\n  각 태스크 에이전트를 구현한다.\n  중간자 에이전트를 커스터마이징 한다. (Facilitators, Mediators, Brokers, \u0026hellip;)\n  큰 그림부터 필요한 요소들을 하나하나 선택, 결정하고 구현하는 단계를 거칩니다.\n이러한 MAS는 다양한 곳에서 사용됩니다. 인터넷에서 정보검색을 하거나 보안 강화나 전자상거래의 쇼핑 에이전트들, 분산 신호처리 시스템 등에서 다양하게 사용됩니다.\n뭔가 주절주절 나열한 것 같습니다. 이해가 안되시는 부분이나 잘못된 부분은 언제든지 댓글로 남겨주세요. 감사합니다.\n[1] https://terms.naver.com/entry.nhn?docId=3437709\u0026amp;cid=58393\u0026amp;categoryId=58393\n[2] https://terms.naver.com/entry.nhn?docId=778808\u0026amp;cid=42085\u0026amp;categoryId=42085\n"
},
{
	"uri": "https://linuxias.github.io/machinelearning/agent/agent_sw_agent_architecture/",
	"title": "[Agent] SW Agent Architecture",
	"tags": [],
	"description": "",
	"content": "소프트웨어 에이전트 아키텍쳐에 대해 정리해보겠습니다.\n에이전트 구조는 3개로 크게 나뉠 수 있습니다.\n  Deliberative : 의도적인, 신중한\n  Reactive : 반응하는\n  Hybrid : Deliberative + Reactive\n  용어 의미만 보아서 파악할 수 있는건 Hybrid밖에 없는 것 같네요. 하나씩 정리해보도록 하겠습니다.\nDeliberative Agents Deliberative Agents는 명확하게 표현되어 질 수 있는 실세계의 상징적 모델이며 Symbolic reasoning을 통해 결정을 만들어 나가는 에이전트입니다. Sense-plan-act 를 통한 문제 해결방식으로 Deliberative 구조로 BDI, GRATE*, HOMER, Shoham 등이 유명합니다. Deliberative는 Deductive reasoning agents와 Practical reasoning agents로 나뉘어져 있습니다. Deductive reasoning은 연역적 추론을 기반으로 한 Agent이며 Practical reasoning은 실용적 추론을 기반으로 한 Agent 입니다.\nDeliberative Agent는 아래와 같은 플로우를 가집니다.\nInput -\u0026gt; [ Sensors -\u0026gt; World Model -\u0026gt; Planner -\u0026gt; Plan Executor -\u0026gt; Effector ] -\u0026gt; Output\n입력에 대해 Sensing하고 Model에 대해 목표 달성을 위한 Plan을 세우게 됩니다. 이 때 Plan은 하나일 수도 있고 여러개 일 수도 있습니다. 그 중 목표달성을 위한 적절한 Plan을 선택하여 실행하게 되는 순서입니다.\nDeliberative Agents의 문제는 Performance와 Representation 문제가 있습니다. Performance는 환경이 급속하게 변화된다면 상징적 표현을 필요한 모든 정보로 변환하는데 시간 소모가 많습니다. Representation은 어떻게 세계 모델이 상직적으로 표현될 수 있는가와 그리고 결과를 유용하게 사용할 수 있도록 제 시간에 정보를 근거로 에이전트를 추론하는 방법에 대한 문제가 있습니다. 이렇게 느린 결과는 쓸모가 없을 뿐더러 실제 세계 모델은 매우 복잡하기에 심볼로만 처리하기엔 어려움이 많습니다.\nDeliberative Agents에는 Deductive Reasoning과 Practical Reasoning이 있다고 말씀드렸는데요, 이 둘에 대해 자세히 정리해 보겠습니다. 먼저 Deliberative Agent입니다. 연역적 추론을 기반으로하는 에이전트 인데요, 연역적 추론이란 전제들이 참이면 결론은 항상 참이라는 것이 보장되는 추론입니다. 에이전트가 어떻게 정리 이론을 이용해서 무엇을 하는지 결정해야하는가? 에 대한 질문을 던질 수 있는데요, 기본적인 아이디어는 주어진 상황에서 수행가능한 최고의 액션을 서술하는 이론을 인코딩하는 사용하는 것입니다. 뭔가 어렵네요. Deductive Reasoning은 매우 단순하고 논리적인 의미를 담고있습니다. 하지만 어떻게 어떻게 실세계의 정보를 심볼정보로 변경할 지, 추론에 대한 시간 복잡도는 어떨지 고민해봐야 할 것 같습니다.\n다음 Practical Reasoning입니다. Practical Reasoning은 행동에 대한 Reasoning입니다. Practical reasoning은 두 개의 activity로 구성되어 있는데요, Deliberation과 Means-ends reasoning입니다.\n  Deliberation (숙고, 신중함) : 달성하고자 하는 일의 상태가 무엇인지 결정하는 것\n  Means-ends reasoning (방법 목적 추론) : 일의 상태들을 달성하기 위해 어떻게 해야하는지 결정하는 것\n  어렵네요. Deliberation은 달성하고자 하는 일 자체의 상태이고, Means-ends reasoning은 일의 상태를 달성하기 위해 어떠한 과정을 결정해야 하는 것인가(?) 로 생각해보면 좋을 것 같습니다. Deliberation의 출력이 곧 Intentions입니다. Intentions이란 용어는 에이전트가 선택하고 실행해야할 일의 상태를 말합니다. Practical reasoning에서 중요한 역할을 하는 녀석입니다. Intentions은 means-ends reasoning을 드라이브하고 지속됩니다. 미래의 Deliberation을 제한하기도 하며 미래에 대한 Belief들과 연관된 녀석입니다. 좀 어렵나요? 천천히 정리해보시기 바랍니다. 다음 Means-end reasoning은 에이전트에게 달성하기 위한 목표, 수행할 수 있는 액션들, 환경의 표현을 주기 위함입니다. 목표를 달성하기 위한 계획을 생성하는 것입니다.\nReactive Agents Reactive agent는 내부적으로 세계의 표현을 매우 단순화하였으며 Perception과 Action이 매우 강하게 결속되어 있는 에이전트입니다. 행위 기반 에이전트의 전형적인 예로서 에이전트 내부에서 각 상태에 따른 Agent가 나뉘어져 있습니다.\n[ State 1 -\u0026gt; Action 1 ]\n[ State 2 -\u0026gt; Action 2 ]\nSensors -\u0026gt;[ State 3 -\u0026gt; Action 3 ] -\u0026gt;Effectors\n [ State 4 -\u0026gt; Action 4 ]\n [ State 5 -\u0026gt; Action 5 ]\n위 처럼 Sensor에서 받아들인 입력을 각 State로 전달하고 액션을 결정하여 Effector로 전달합니다. 각 행위는 지각의 입력에서 출력이 연속적으로 매핑되어 있는 구조입니다. Reactive Agents는 매우 단순하고 경제적이며 구현하기 매우 쉽습니다. 시스템 문제에 대한 강건성 또한 이 에이전트의 이점입니다. 하지만 문제도 다양합니다. 환경 모델 없는 에이전트들은 지역적 환경으로부터 가능한 충분한 정보를 가져야 합니다. 만약 지역 환경을 기반으로한 결정들은 비지역 환경들에 대해선 어떻게 고려할 것이냐도 문제가 됩니다. 또한 Reactive agents는 학습시키 매우 어렵습니다. 마지막으로 상호작용하는 요소들과 환경에 따라 동작이 달라지기 때문에 특정 에이전트를 엔지니어링 하기 어렵습니다.\nHybrid Agents Hybrid Agent는 Deliberative와 Reactive 행위의 결합입니다. 계획을 세우거나 상징적 추론을 사용해 결정을 만드는 시스템과 복잡한 추론 없이 이벤트에 대해 빠르게 반응하는 시스템이 Hybrid Agent의 서브시스템으로 구성되어 있습니다. Sensor와 Effector 사이에 Deliberative 요소와 Reactive 요소가 함께 포함되어 있습니다. Deliberative와 Reactive를 자세히 알아보았으니 Hybrid는 두개의 결합이라고 이해하면 가장 좋을 것 같습니다 :D\n잘못된 내용이나 궁금한 내용은 댓글 부탁드립니다. 긴 글 읽어주셔서 감사드립니다.\n"
},
{
	"uri": "https://linuxias.github.io/container/docker/container_logging/",
	"title": "[Logging] Overview",
	"tags": [],
	"description": "",
	"content": "docker logs 명령어는 이용하여 동작중인 컨테이너의 로그 정보를 볼 수 있습니다. docker logs 명령어는 터미널에서 대화식으로 명령을 실행했을 때와 같이 명령의 출력을 표시합니다. UNIX 및 Linux 명령은 일반적으로 STDIN, STDOUT 및 STDERR이라는 3 개의 I/O 스트림이 실행될 때 열립니다. STDIN은 명령의 입력 스트림으로 키보드 입력 또는 다른 명령 입력을 포함 할 수 있습니다. STDOUT은 일반적으로 명령의 일반 출력이며 STDERR은 일반적으로 오류 메시지를 출력하는 데 사용됩니다. 기본적으로 docker logs에는 명령의 STDOUT 및 STDERR이 표시됩니다.\n하지만 특별한 경우에는 로그를 볼 수 없습니다. 그 경우는 아래와 같습니다.\n logging driver 가 로그를 파일, 호스트 또느 DB 등에 저장하게 되면 docker logs 명령어로는 로그를 확인할 수 없습니다. 그 이유는 위에서 설명한 듯이 STDOUT, STDERR를 표시하기 때문입니다. Non-interactive 프로세스들 즉 STDIO 와 상호 인터렉션이 없는 프로세스들은 STDIO 대신 다른 방식으로 로그를 저장합니다.  위와 같은 경우 docker logs 명령어로 로그를 볼 수 있도록 별도의 작업이 필요합니다. 예시는 View logs for a container or service 에서 제공하고 있는 nginx와 httpd입니다. Dockerfile 에서 저장되는 로그 파일을 STDOUT, STDERR로 리다이렉션 하거나, 설정을 변경함으로서 로그를 제공합니다.\nnginx 이미지는 /var/log/nginx/access.log에서 /dev/stdout으로의 심볼릭 링크를 만들고 /var/log/nginx/error.log에서 /dev/stderr 로의 다른 심볼릭 링크를 만들어 로그를 덮어 씁니다. nginx Dockerfile\nhttpd 이미지는 httpd 응용 프로그램의 구성을 변경하여 STDOUT을 /proc/self/fd/1 (STDOUT)에 직접 기록하고 오류를 /proc/self/fd/2 (STDERR)에 기록합니다. httpd Dockerfile\nlogging driver 설정하기 도커는 다양한 로깅 메커니즘을 포함하고 있습니다. 각 메커니즘들은 우리는 logging driver 라고 부릅니다. 기본적으로 local file, json-file, syslog, fluentd와 구글 클라우드, 아마존 클라우드에서 제공하는 logging driver까지 여러 드라이버가 존재합니다. 해당 드라이버 중 몇가지에 대한 사용법은 아래에서 알아보겠습니다. 여기서는 logging driver에 대한 설정에 대해 정리하겠습니다.\nDefault logging driver 설정하기 Docker daemon에서 사용하는 기본 logging driver를 설정하기 위해서 /etc/docker/ 경로에 존재하는 daemon.json 파일을 이용합니다. 아무런 설정을 하지 않았다면 도커에서 제공하는 logging driver는 json-file입니다. 이 설정을 syslog 로 변경하는 방법은 daemon.json 파일에 아래와 같이 추가하는 것입니다.\n{ \u0026#34;log-driver\u0026#34;: \u0026#34;syslog\u0026#34; } logging-driver에 추가적인 옵션을 주고자 한다면 아래와 같이 log-opts를 이용하여 설정해줍니다. 설정 내용은 읽어보시면 쉽게 파악하실 수 있습니다. 여기서 유의할 점은 모든 value는 문자열 기반입니다.\n{ \u0026#34;log-driver\u0026#34;: \u0026#34;json-file\u0026#34;, \u0026#34;log-opts\u0026#34;: { \u0026#34;max-size\u0026#34;: \u0026#34;10m\u0026#34;, \u0026#34;max-file\u0026#34;: \u0026#34;3\u0026#34;, } } logging driver 간략히 정리하기 Configure logging drivers 에 있는 표를 이용하여 컨테이너를 위한 logging driver들을 몇 가지만 정리하였습니다. 그 외 추가적인 내용은 링크에서 확인하시면 됩니다.\n   Driver Description     none 비활성화   local 최소한의 오버헤드를 위한 사용자가 정의한 포맷으로 로그가 저장됩니다   json-file JSON 형태로 로그가 저장됩니다. 도커의 default logging driver 입니다.   syslog syslog를 이용하여 로그를 작성합니다. syslog 데몬이 호스트에서 동작하고 있어야 합니다.   journald journald를 이용하여 로그를 작성합니다. journald 데몬이 호스트에서 동작하고 있어야 합니다.   fluentd fluentd에 로그를 작성합니다. fluentd 데몬이 호스트에서 동작하고 있어야 합니다.    만약 Docker Community Engine을 사용하는 경우에는 docker logs 명령어를 사용하여 오직 local, json-file, journald에서만 사용할 수 있습니다.\n참고자료 View logs for a container or service Configure logging drivers\n"
},
{
	"uri": "https://linuxias.github.io/sw_architecture/batch_sequential_architecture/",
	"title": "Batch Sequential Architecture",
	"tags": [],
	"description": "",
	"content": "Batch Sequential Architecture에 대해 정리해보겠습니다.\nBatch Sequential Architecture는 이전 글에서 정리한 Data Flow Software Architecture 중 하나입니다. 참고가 필요하신 분은 해당 글을 확인해주세요.\n[S/W Architecture] Data Flow Software Architectures\nBatch Sequential Architecture는 1950~70년대에 많이 사용된 데이터 처리 모델입니다. 데이터는 하나의 서브시스템에서 다음 서브시스템으로 데이터로 전달됩니다. 각 데이터 전송 서브시스템 또는 모듈은 이전 서브시스템의 데이터 처리가 끝나기 전에는 스스로 시작할 수 없습니다. 정리하자만 A-B 로 연결된 Batch Sequential Architecture에서 B는 A가 모든 데이터 처리를 완료한 후 결과 데이터가 출력되기 전까지 스스로 독립적으로 시작할 수 없습니다. 데이터를 분리해 중간중간 처리가 아닌 하나의 서브시스템이 데이터를 처리한 전체 결과를 출력해야만 다음 서브시스템이 시작할 수 있습니다.\nBatch Sequential Architecture를 구성하는 컴포넌트는 Program과 Data store입니다. 각 프로그램의 연결은 단방향 파이프로써 데이터 셋을 전달합니다. 아래와 같은 형태로 Batch Sequential Architecture가 구성됩니다.\ngraph LR; INPUT[INPUT] --\u0026gt; FILTER1[FILTER1] FILTER1[FILTER1] --\u0026gt; FILTER2[FILTER2] FILTER2[FILTER2] --\u0026gt; FILTER3[FILTER3] FILTER3[FILTER3] --\u0026gt; OUTPUT[OUTPUT] Batch Sequential Architecture는 서브시스템들이 단순하게 분리되어 있고 입력 데이터와 출력 데이터에 맞춘 서브시스템의 교체도 가능합니다. 서브시스템간 연결은 오직 데이터 이므로 데이터만 맞추면 된됩니다. 하지만 외부에서 서브시스템을 제어하기 위한 구현에서는 부적합하며 인터렉션을 위한 인터페이스를 제공할 수가 없다. 오직 데이터만이 입력과 출력이기 때문입니다. 또한 동시성을 지원하지 않기 때문에 낮은 성능과 높은 Latency를 가지는게 이 아키텍처의 한계입니다.\n이상 Batch Sequential Architecture에 대해 정리해보았습니다. 감사합니다.\n"
},
{
	"uri": "https://linuxias.github.io/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://linuxias.github.io/linux/profile/cpu/",
	"title": "CPU Profiling",
	"tags": [],
	"description": "",
	"content": "CPU Analysis uptime 시스템 부하 평균을 표시한다. load average의 값은 앞에서부터 1분, 5분, 15분 동안의 평균 부하이다.\nuptime 08:41:55 up 1 day, 17:41, 4 users, load average: 0.28, 0.24, 0.27 부하평균은 CPU 자원에 대한 요구사항으로써, ** 실행 중인 스레드 개수 + 대기열의 스레드 개수 **이다. 만약 부하 평균이 호스트 머신의 CPU 개수보다 크면 스레드를 처리하기에 CPU가 부족한 상황이며, 대기열에 여러 스레드가 CPU를 할당받기 위해 대기하고 있는 상태라고 파악할 수 있다.\nvmstat vmstat은 가상 메모리 통계 정보를 제공한다.\n$vmstat procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu----- r b swpd free buff cache si so bi bo in cs us sy id wa st 0 0 1024 12837676 7238384 10378596 0 0 14 39 24 19 2 1 97 0 0 위 컬럼에서 아래 정보에서 각 컬럼에 대한 내용은 아래와 같다.\n r : 실행 중이거나 실행을 위해 기다리는 프로세스 개수 us : 사용자 시간 sy : 시스템 시간 id : 유휴 wa : I/O 대기. 스레드가 디스크 I/O를 기다리느라 블록된 것을 표시  mpstat (multiprocessor statistics tool) CPU 별 통계 확인 가능\nmpstat -P ALL 1 Linux 4.15.0-76-generic (linuxias) 2020년 02월 17일 _x86_64_\t(8 CPU) CPU %usr %nice %sys %iowait %irq %soft %steal %guest %gnice %idle all 5.02 0.00 7.78 0.00 0.00 0.00 0.00 0.00 0.00 87.20 0 0.99 0.00 1.98 0.00 0.00 0.00 0.00 0.00 0.00 97.03 1 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 100.00 2 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 100.00 3 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 100.00 4 37.62 0.00 59.41 0.00 0.00 0.00 0.00 0.00 0.00 2.97 5 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 100.00 6 0.00 0.00 1.01 0.00 0.00 0.00 0.00 0.00 0.00 98.99 7 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 100.00 mpstat을 실행한 호스트머신에는 8개의 CPU가 있으며 각 통계정보는 위와 같다. 각 정보가 표시하는 내역은 다음과 같다.\n CPU : 논리적 CPU ID %usr : 사용자 시간 %nice : nice가 지정된 프로세스의 사용자 시간 %sys : 시스템 시간 %iowatit : I/O 대기 %irq : 하드웨어 인터럽트 CPU 사용률 %soft : 소프트웨어 인터럽트 CPU 사용률 %guest : 게스트 가상 머신을 실행하는데 사용한 시간 %idle : 유휴 시간  위 정보를 기반으로 현재 호스트머신의 대부분의 CPU는 idle 상태이며, 4번 CPU에서 특정한 작업이 이뤄지고 있고 있음을 알 수 있다. usr / sys / idle 항목을 잘 확인하면 CPU 별 CPU 사용률과 사용자, 커널 사용 시간에 대한 비율을 확인할 수 있다. 사용자 어플리케이션이 코드를 실행하는 데 소모한 CPU 시간이 usr 이며, 커널 영역 코드 실행하는데 걸린 시간이 sys이다. 커널 시간에는 시스템 콜 처리 시간, 커널 스레드가 보낸 시간, 인터럽트 처리 시간 등이 포함된다.\nsar (system activity reporter) sar 는 mpstat와 유사하게 사용되며 현재 시스템 상태를 관찰하고, 과거 통계 정보를 저장하도록 할 수 있다. 각 항목은 위에서 설명한 항목과 유사하다.\nsar -P ALL 1 Linux 4.15.0-76-generic (linuxias) 2020년 02월 17일 _x86_64_\t(8 CPU) CPU %user %nice %system %iowait %steal %idle all 5.75 0.00 8.12 0.00 0.00 86.12 0 3.00 0.00 5.00 0.00 0.00 92.00 1 0.99 0.00 0.99 0.00 0.00 98.02 2 7.92 0.00 7.92 0.00 0.00 84.16 3 1.00 0.00 0.00 0.00 0.00 99.00 4 1.98 0.00 2.97 0.99 0.00 94.06 5 0.00 0.00 2.02 0.00 0.00 97.98 6 30.69 0.00 44.55 0.00 0.00 24.75 7 0.99 0.00 1.98 0.00 0.00 97.03 perf (Performance Counters for Linux) perf는 다양한 옵션이 존재하지만 그 중 몇가지 항목에 대해서만 정리해보려 한다.\n   Command Description     record 명령어를 실행하고 결과를 perf.data 파일에 저장한다.   report perf.data 파일을 읽어 결과를 출력한다.   sched 스케쥴러 트레이싱을 위한 툴   stat 통계 정보    시스템 프로파일링 perf는 CPU 호출 경로를 프로파일링 할 때 자주 사용된다. 아래는 시스템 프로파일링에 대한 예시이다. record의 -a 옵션은 모든 CPU, -g는 호출 스택, -F 997 는 샘플링 주기를 997Hz로 sleep 10은 10초 간 기록하라라는 의미이다. record로 저장한 데이터를 report로 출력하게 되면 트리 형태로 표현된다. 너무 길어 아래 부분은 생략한다.\n$sudo perf record -a -g -F 997 sleep 10 [ perf record: Woken up 1 times to write data ] [ perf record: Captured and wrote 2.024 MB perf.data (3722 samples) ] $sudo perf report --stdio # To display the perf.data header info, please use --header/--header-only options. # # # Total Lost Samples: 15 # # Samples: 3K of event \u0026#39;cycles:ppp\u0026#39; # Event count (approx.): 2728432762 # # Children Self Command Shared Object Symbol # ........ ........ ............... ........................... ............................ # 62.00% 0.00% swapper [kernel.kallsyms] [k] secondary_startup_64 | ---secondary_startup_64 | |--56.13%--start_secondary | cpu_startup_entry | | | --55.96%--do_idle | | | |--47.86%--call_cpuidle | | cpuidle_enter | | | | | --47.85%--cpuidle_enter_state | | | | | |--34.77%--intel_idle | | | | | |--7.11%--apic_timer_interrupt ....... 위 트리구조를 해석하면 전체 정보는 샘플 크기가 작아지는 순서대로 표시된다. 가장 많이 CPU를 사용한 스레드는 swapper(유휴 스레드)이다. 트리의 내부에서 표현되는 %는 CPU 전체가 아니다. 위에서 cpuidle_enter_State가 47.85%이고 자식 중 intel idle이 34.77%라면, intel idle 이 47.85% 중 34.77%를 차지한다는 의미이다.\n특정 프로세스 프로파일링 perf record -g [command] // 프로세스 실행하며 프로파일링 시작 perf record -g -p [pid] // 실행 중신 프로세스 프로파일링 시작 htop 프로세스를 실행하고 -p 옵션으로 프로파일링 하는 예시는 아래와 같다.\n$sudo perf record -g -p `pidof htop` sleep 5 $sudo perf report --stdio # To display the perf.data header info, please use --header/--header-only options. # # # Total Lost Samples: 2 # # Samples: 50 of event \u0026#39;cycles:ppp\u0026#39; # Event count (approx.): 309981392 # # Children Self Command Shared Object Symbol # ........ ........ ....... ................. .................................................... # 64.87% 64.87% htop [kernel.kallsyms] [k] syscall_return_via_sysret | |--57.82%--0x30d8f70400000006 | __close_nocancel | syscall_return_via_sysret | --6.93%--__GI___libc_read syscall_return_via_sysret 62.48% 0.00% htop libc-2.23.so [.] __close_nocancel | ---__close_nocancel | |--57.82%--syscall_return_via_sysret | --4.42%--__entry_trampoline_start 57.82% 0.00% htop [unknown] [k] 0x30d8f70400000006 | ---0x30d8f70400000006 __close_nocancel syscall_return_via_sysret 25.35% 0.00% htop libc-2.23.so [.] __GI___libc_read 26.... 스케줄러 프로파일링 sched 옵션을 이용하여 스케쥴러 정보를 확인할 수 있다. 아래에서 사용한 latency 옵션은 각 태스크 스케쥴링 레이턴시들에 대해 정보를 제공한다. 그 외에도 script, replay, map 옵션이 있다. 자세한 내용은 man perf sched 에서 확인하면 된다.\n$sudo perf sched record sleep 3 [ perf record: Woken up 2 times to write data ] [ perf record: Captured and wrote 6.090 MB perf.data (40024 samples) ] $ sudo perf sched latency ----------------------------------------------------------------------------------------------------------------- Task | Runtime ms | Switches | Average delay ms | Maximum delay ms | Maximum delay at | ----------------------------------------------------------------------------------------------------------------- perf:2215 | 5.191 ms | 1 | avg: 0.032 ms | max: 0.032 ms | max at: 89323.211980 s lttng-sessiond:17440 | 0.101 ms | 3 | avg: 0.032 ms | max: 0.033 ms | max at: 89321.697742 s avahi-daemon:1119 | 8.777 ms | 54 | avg: 0.028 ms | max: 0.041 ms | max at: 89322.788970 s kworker/1:0:28740 | 0.159 ms | 8 | avg: 0.026 ms | max: 0.033 ms | max at: 89322.372042 s kworker/3:0:28203 | 0.028 ms | 1 | avg: 0.025 ms | max: 0.025 ms | max at: 89320.908025 s kworker/0:1:30386 | 0.096 ms | 3 | avg: 0.025 ms | max: 0.033 ms | max at: 89322.872069 s kworker/7:0:31054 | 0.154 ms | 5 | avg: 0.024 ms | max: 0.031 ms | max at: 89322.780055 s NioBlockingSele:1018 | 0.086 ms | 3 | avg: 0.023 ms | max: 0.033 ms | max at: 89320.871888 s /usr/bin/x-term:7180 | 1.292 ms | 6 | avg: 0.022 ms | max: 0.037 ms | max at: 89322.676333 s Xorg:4299 | 4.470 ms | 35 | avg: 0.021 ms | max: 0.034 ms | max at: 89322.809583 s whale:8519 | 0.513 ms | 5 | avg: 0.021 ms | max: 0.033 ms | max at: 89322.219984 s ..... sleep:2220 | 0.926 ms | 2 | avg: 0.007 ms | max: 0.010 ms | max at: 89323.211755 s ksoftirqd/7:52 | 0.041 ms | 2 | avg: 0.005 ms | max: 0.006 ms | max at: 89320.924052 s systemd-journal:340 | 0.043 ms | 1 | avg: 0.005 ms | max: 0.005 ms | max at: 89320.290112 s zeitgeist-datah:6956 | 0.209 ms | 4 | avg: 0.005 ms | max: 0.009 ms | max at: 89320.836458 s kworker/u16:1:1956 | 6.535 ms | 810 | avg: 0.002 ms | max: 0.037 ms | max at: 89321.088060 s kworker/u16:2:583 | 36.616 ms | 5468 | avg: 0.002 ms | max: 0.044 ms | max at: 89322.384987 s ----------------------------------------------------------------------------------------------------------------- TOTAL: | 163.575 ms | 9981 | --------------------------------------------------- 위 결과는 트레이싱 하는 동안의 평균과 최대 스케쥴러 지연시간을 나타낸다. 가장 위에 perf가 표시되는 이유는 트레이싱 동작 자체가 CPU와 저장장치들을 많이 사용함에 따른 비용이다. perf sched latency 뒤에 다양한 옵션이 들어 갈 수 있으므로 그거 또한 직접 확인해 보면 좋다.\n통계 정보 확인하기 (stat) stat은 CPC(CPU Performance Cycle)를 기반으로 CPU 사이클에 대한 통계 정보를 제공한다. 특정 프로세스를 실행하여 분석을 할 수도 있으며 전체에 대해 추적할 수도 있다. 아래는 df -h 명령어에 대한 통계 정보를 확인한 것 이다. 결과를 확인하면 CPU 사용, 컨텍스트 스위칭 횟수, 및 시간, 페이지 폴트, CPU 마이그레이션, 명령어를 수행하는데 사용한 싸이클 등 다양한 정보가 제공된다.\n$sudo perf stat df -h ... Performance counter stats for \u0026#39;df -h\u0026#39;: 1.73 msec task-clock # 0.813 CPUs utilized 0 context-switches # 0.000 K/sec 0 cpu-migrations # 0.000 K/sec 80 page-faults # 0.046 M/sec 3,207,136 cycles # 1.849 GHz 1,874,078 stalled-cycles-frontend # 58.43% frontend cycles idle 2,999,986 instructions # 0.94 insn per cycle # 0.62 stalled cycles per insn 635,894 branches # 366.542 M/sec 19,691 branch-misses # 3.10% of all branches 0.002134928 seconds time elapsed 0.002180000 seconds user 0.000000000 seconds sys 이벤트 이용하여 통계 정보 확인하기 perf는 확인할 수 있는 다양한 이벤트 리스트를 제공한다. perf list를 통해 확인해 보자. 그 중 CPU와 관련된 많은 이벤트들도 존재한다. 몇몇 이벤트를 선별하여 테스트 해보자. 아래와 같이 본인이 원하는 이벤트에 대한 결과만 확인할 수 있따.\n$sudo perf stat -e instructions,cycles,cache-misses du -a -h Performance counter stats for \u0026#39;du -a -h\u0026#39;: 2,296,353,620 instructions # 0.65 insn per cycle # 0.00 stalled cycles per insn 3,543,461,302 cycles 2,002,191 cache-misses 1.769061539 seconds time elapsed 0.331797000 seconds user 0.659498000 seconds sys Reference  시스템 성능 분석과 최적화 | 브렌든 그레그 지음, 오현석, 서형국 옮김 | 위키북스 Perf Manual page  "
},
{
	"uri": "https://linuxias.github.io/sw_architecture/data_flow_sw_architecture/",
	"title": "Data Flow Software Architecture",
	"tags": [],
	"description": "",
	"content": "Data Flow Software Architecture에 대해 정리해보고, 해당 아키텍처에 속하는 이키텍처들을 정리해 보려합니다. 주제에서 알 수 있듯이 데이터의 흐름에 대한 소프트웨어 아키텍처입니다. Data Flow Software Architecture는 전체 소프트웨어 시스템을 연속적인 데이터 집합에 대한 일련의 변환으로 봅니다.\n소프트웨어 시스템은 데이터가 데이터 연산 처리 순서를 지시하고 제어하는 데이터 처리 요소로 분리 될 수 있습니다. 각 컴포넌트는 입력으로 데이터를 받고, 출력으로 연산된 데이터를 출력합니다. 이렇게 출력된 연산 데이터는 다음 컴포넌트의 입력이 됩니다. 이 부분이 Data Flow Software Architecture 들의 가장 큰 특징입니다.\n데이터를 처리하는 각 서브시스템 컴포넌트들 사이의 연결은 I/O 스트림, I/O 파일, 버퍼나 파이프등 다양한 방법이 있습니다. 이 아키텍처에서 트리 구조에서는 사이클이 없는 선형적인 구조를 가지고 있으며, 그래프 토폴로지와 같은 데이터 흐름에서는 사이클이 발생할 수 있습니다.\nData Flow Software Architecture에 속하는 대표적인 구조 3가지는 아래와 같습니다.\n  Batch Sequential Architecture\n  Pipe and Filter Architecture\n  Process Control Architecture\n  Data Flow Software Architecture의 장점은 변경용이성과 재사용성입니다. 서브시스템은 서로간 독립적으로 구성되어 있습니다. 각 서브시스템은 서로간의 어떠한 영향없이 새로운 서브시스템으로 교체가 가능하며 중간에 새로운 서브시스템의 추가도 쉽기 때문에 아키텍처의 변경이 용이하고 각 서브시스템은 다른 아키텍처에서 재사용이 쉽습니다. 한 가지 유의점은 서브시스템 추가 시 출력되는 데이터 형태가 다음 서브시스템의 입력과 일치해야 합니다. A에서 B 모듈로 파일을 이용해 입출력을 하는 아키텍처에서 중간에 S 서브시스템을 추가한다고 합시다. 이때 S 서브시스템의 출력은 파일이 아니라 Buffer 형태를 사용한다면 추가가 불가능 합니다. 이런 점은 유의해야 합니다.\n위와 같은 특징을 생각했을 때 전통적인 절차지향적 구조라는 생각이 듭니다. Data Flow Software Architecture는 컴파일러나 Batch 데이터 처리 등에서 많이 사용합니다.\n다음 글에서 Batch Sequential, Pipe and Filter, Process Control을 순서대로 정리해보겠습니다.\n감사합니다.\n"
},
{
	"uri": "https://linuxias.github.io/container/docker/dockerfile/",
	"title": "Dockerfile",
	"tags": [],
	"description": "",
	"content": "Docker는 Dockerfile에 정의된 인스트럭션을 이용하여 자동적으로 이미지를 빌드 할 수 있습니다. Dockerfile은 일반적은 텍스트 파일로서 사용자가 이미지를 생성할 때 필요로 하는 모든 명령어들을 포함시킬 수 있습니다. docker build 명령어를 이용하여 사용자는 도커 이미지를 생성할 수 있습니다.\nubuntu나 centOS 이미지를 DockerHub로 부터 다운받아서 본인의 어플리케이션을 동작시키려 한다면 생각은 쉽지만 쉽게 되지 않습니다. 어플리케이션이 동작하기 위한 여러 환경이 설정되어 있지 않기 때문입니다. 동일한 어플리케이션을 여러 컨테이너에서 동작 시키고자 한다면 각 컨테이너마다 새로운 환경을 구축해 줘야 할 것입니다. 도커에서는 환경을 설정하거나 명령어를 수행하는 등 다양한 과정을 손쉽게 작성할 수 있도록 docker build 명령어를 제공하고 있습니다.\nDockerfile 작성하기 먼저 설명드리기 전에 ubuntu 18.04에 net-tools이 설치된 도커 이미지를 생성해보도록 하겠습니다.\n$mkdir docker-test \u0026amp;\u0026amp; cd docker-test $vi Dockerfile # This contents in Dockerfile FROM ubuntu:18.04 LABEL maintainer = \u0026#34;Seungha Son \u0026lt;linuxias@gamil.com\u0026gt;\u0026#34; RUN apt-get update RUN apt-get install net-tools -y Dockerfile 내용은 단순합니다. 이 Dockerfile을 이용하여 빌드를 해보고 docker images 명령어로 생성된 이미지를 확인해봅시다.\n$sudo docker build -t net-tools:0.0 . ... ---\u0026gt; d2b515967717 Successfully built d2b515967717 Successfully tagged net-tools:0.0 $sudo docker images net-tools 0.0 4081bd419bb1 4 minutes ago 93.4MB 잘 생성된 것을 알 수 있습니다. 그럼 하나하나 옵션을 정리해 보겠습니다.\nFROM FROM 명령어는 Dockerfile 내부에 가장 먼저 등장해야 하는 명령어이며 최소 하나 이상 존재해야 하는 명령어 입니다. 생성할 이미지의 기반이 될 도커 이미지를 명시해주시면 됩니다. 처음 시작하실 땐 하나만 작성을 할텐데요, 나중에 둘 이상 사용하는 예시에 대해 정리하겠습니다. 아래 예제는 생성할 이미지의 베이스가 될 이미지를 ubuntu:18.04로 명시합니다.\nFROM ubuntu:18.04 LABEL 생성하려는 이미지에 메타데이터를 추가하는 것입니다. 키-밸류 형태로 이루어져있으며, 복수 개의 메타데이터를 생성할 수 있습니다.\nLABEL maintainer \u0026quot;Seungha Son \u0026lt;linuxias@gmail.com\u0026gt;\u0026quot; ADD 파일을 이미지에 추가할 때 사용합니다. Dockerfile이 위치한 경로에 파일을 포함합니다. 아래 예제는 Dockerfile이 존재하는 경로의 test.txt 파일을 생성하는 이미지의 /home/ 경로에 추가합니다. 두 번째 예제는 3개의 텍스트 파일을 /home/ 경로에 추가하는 내용입니다. 이 처럼 여러 파일을 한 번에 명시할 수도 있습니다.\nADD test.txt /home/ ADD [test.txt test1.txt test2.txt /home/] ADD는 로컬에 있는 파일 뿐만아니라 URL을 명시하면 해당 URL의 파일을 복사해줍니다. 또한 tar 파일도 지원합니다. tar도 일반적인 파일인데, 뭐가 다를까라고 생각하실 수 있지만, ADD로 tar 파일을 추가 시 tar 파일의 압축을 풀어 내부 파일들을 해당 경로에 복사해줍니다. tar 파일 그 자체를 복사하는 것이 아니죠.\nADD https://this/is/test/url.html /home/ ADD test_comp.tar /home/ COPY ADD와 유사하게 파일을 이미지에 복사해 줍니다. COPY는 로컬에 존재하는 파일만 복사를 해주는 것이 ADD와 유사합니다. 하지만 COPY는 ADD에서 지원하는 URL, tar 파일에 대한 것은 지원하지 않습니다.\nCOPY test.txt /home/ COPY [test.txt test1.txt test2.txt /home/] WORKDIR cd 명령어와 같습니다. 생성한 이미지의 /tmp 디렉터리로 이동하는 예시 입니다.\nWORKDIR /tmp RUN 이미지를 생성하기 위해 컨테이너 내부에서 실행하는 명령어 입니다. 아래 예제는 업데이트 후 net-tools를 설치하는 명령어 예제입니다. -y 옵션은 RUN 동작 시 사용자와의 인터페이스가 없기에 설치 시 [y/N] 입력이 불가능 하기 때문입니다.\nRUN apt-get update RUN apt-get install net-tools -y CMD CMD 명령어는 생성된 이미지를 이용하여 컨테이너를 동작 시킬 때 컨테이너 시작 시 실행할 명령어 입니다. 단 한번만 사용할 수 있습니다. 명시하지 않는다면 기본적으로 /bin/bash 명령어가 실행됩니다.\nENV Dockefile 내부에서 사용할 환경변수를 설정합니다. ENV 는 도커 이미지에 포함되므로 해당 이미지로 생성하는 컨테이너에서 이 환경변수를 사용할 수 있습니다. 사용 방법은 ${variable_name} 또는 $variable_name 형태로 사용할 수 있습니다.\n# This contents in Dockerfile FROM ubuntu:18.04 LABEL maintainer = \u0026#34;Seungha Son \u0026lt;linuxias@gamil.com\u0026gt;\u0026#34; ENV install_tool net-tools RUN apt-get update RUN apt-get install $install_tool -y CMD [\u0026#34;/bin/bash\u0026#34;] $install_tool 환경 변수는 컨테이너 에서도 사용이 가능하며 docker run 시 -e 옵션을 이용하여 환경변수를 덮어쓸 수 있습니다.\nVOLUME VOLUME은 컨테이너를 생성 했을 시 호스트 머신과 공유할 컨테이너 내부의 디렉터리를 설정합니다. 여러 개의 디렉터리 경로를 설정할 수 도 있습니다.\nARG docker build 명령어 실행 시 추가로 입력을 받아 Dockefile 내부에서 사용될 변수의 값을 설정할 수 있습니다.\nUSER 기본적으로 컨테이너 실행 시 root 권한으로 실행 됩니다. 만약 다른 사용자 계정으로 실행하고자 할 때 USER를 사용합니다. root 가 아닌 linuxias 란 계정으로 시작하길 원하신다면 아래와 같이 작성해 줍니다.\nUSER linuxias 이때 유의할 점은현재 존재하는 사용자여야 합니다. adduser 명령어를 이용하여 미리 사용자를 만들어놓던지, volume을 이용하여 /etc/passwd 파일을 사용하도록 하고 호스트머신에 존재하는 사용자를 USER명령어로 사용합니다.\nENTRYPOINT 매우 매우 중요한 !!! 내용!. 앞서 컨테이너가 시작할 때 실행할 명령어를 설정해주는 방법으로 CMD를 설명하였습니다. CMD 와 유사한 기능을 하는 것이 ENTRYPOINT 입니다.\n참고자료  https://docs.docker.com/engine/reference/builder/ 시작하세요! 도커/쿠버네티스. 용찬호 지음. 위키북스  "
},
{
	"uri": "https://linuxias.github.io/sw_architecture/hierarchical_software_architecture/",
	"title": "Hierarchical Software Architecture",
	"tags": [],
	"description": "",
	"content": "Hierarchical Software Architecture, 한국어로 계층적 소프트웨어 아키텍처라 불리는 아키텍처에 대해 정리하겠습니다.\nHierarchical Architecture는 전체 시스템을 계층 구조적으로 나뉘어져 있으며 계층적으로 서로 다른 레벨의 서브시스템으로 구성되어 있습니다. Hierarchical Architecture는 매우 다양한 곳에서 사용되고 있습니다. 운영체제, 네트워크 프로토콜 계층들, 인터프리터, 그 외 다양한 곳에서 사용되고 있는데요, 이 아키텍처의 가장 대표적인 구조로서 여러분들이 가장 많이 접해본 아키텍처의 한 예가 안드로이드 일 것 같습니다. 위 안드로이드 아키텍처를 보시면 Applications, Application Framework, Libraries, Linux Kernel 까지 여러 개의 서브시스템이 계층적으로 구성되어 하나의 시스템을 이루고 있습니다. 각 서브시스템은 상위 시스템이 하위 시스템을 호출하는 구조, 즉 Call-and-Return 연결 구조를 가집니다. 서로 다른 계층 레벨들은 Method Invocation에 의해 연결되어 있으며 하위 레벨의 서브시스템이 상위 레벨 서브시스템에게 필요한 서비스를 제공하는 방식으로 구성됩니다. 이런 Hierarchical Software Architecture 스타일을 가지는 여러 아키텍처들이 존재합니다. 각 아키텍처에 대해서는 다른 글로 다룰 예정이니 참고 바랍니다.\n  Master - Slave Architecture\n  Layered Architecture\n  Virtual Machine Architecture\n  Plug-in Architecture\n  Micro-kernel Architecture\n  Hierarchical Software Architecture를 적용하기 위해서 몇 가지 주의해야 할 부분들이 있습니다. 먼저, 계층을 나누는 기준이 명확해야 합니다. 각 계층은 하위 계층만을 의존해야 하며, 각 계층을 명확하고 특정적인 태스크를 처리하도록 분리해야 합니다. 다음으로 계층을 몇 개로 나눌지도 고민해야 합니다. 무조건 많거나 무조건 적다고 좋은 것이 아닌 본인이 설계하는 시스템에 가장 접합한 계층의 수를 정의해야합니다. 세 번째로 각 계층에 대한 인터페이스를 정의해야 합니다. 인터페이스를 잘 정의해야 계층의 수정사항이 발생하여도 다른 계층에 영향을 주지 않도록 정의되어야 합니다. 마지막으로 각 계층에서 발생한 에러를 어떻게 처리할 지 고민해야 합니다. 해당 에러를 그 계층에서 처리할 지 아니면, 상위 계층에게 전달할지에 대한 부분도 잘 정의해야합니다.\nHierarchical Software Architecture에 대해 간단히 정리해 보았습니다. 감사합니다.\n"
},
{
	"uri": "https://linuxias.github.io/linux/profile/lttng/",
	"title": "LTTng:userspace",
	"tags": [],
	"description": "",
	"content": "LTTng는 리눅스 프로파일링을 위한 오픈소스 트레이싱 프레임워크입니다. LTTng는 Linux Trace Toolkit: next generation 의 약자로 리눅스 커널, 사용자 어플리케이션(C/C++, java, python), 라이브러리를 트레이싱할 수 있습니다.\nLTTng 구성 LTTng 는 크게 3가지 모듈로 구성되어 있습니다.\n lttng-ust : 사용자 어플리케이션을 추적하기 위한 라이브러리 lttng-tools : tracing session을 관리, 제어하기 위한 라이브러리들과 명령어 인터페이스 lttng-modules : 커널 추적을 위한 리눅스 커널 모듈  각 모듈에 대한 설명과 설치 방법 등은 아래 URL을 참조하면 됩니다. https://github.com/lttng https://lttng.org\n여기서는 간략하게 설명하고 넘어가겠습니다.\n(src : https://lttng.org/docs/v2.11/images/plumbing.png)\n사용자 어플리케이션 트레이싱하기 여기의 내용은 LTTng docs 의 User space instrumentation for C and C++ applications 기반으로 작성되었습니다.\n사용자 어플리케이션 분석을 위해서는 lttng-ust 와 lttng-tools 컴포넌트가 필요합니다. 각 설치에 대해서는 여기서 다루진 않겠습니다.\nC/C++ 기반 사용자 어플리케이션 트레이싱 절차는 아래와 같습니다.\n tracepoint provider 패키지의 소스 파일 생성하기 어플리케이션 소스 코드에 tracepoint 추가하기 tracepointer provider 패키지와 사용자 어플리케이션을 빌드 및 링크하기  각 순서대로 정리하겠습니다.\n1. tracepoint provider 패키지의 소스 파일 생성하기 tracepoint provider는 LTTng-UST에 의해 제공되는 tracepoints로 사용자 어플리케이션에 포함된 함수들의 집합입니다. 각 함수들은 사용자가 정의한 필드가 있는 이벤트를 전송하고 해당 이벤트는 LTTng-UST channel 버퍼에 저장됩니다. 저장된 데이터는 consumer daemon으로 전송됩니다.\n이 기능을 사용하기 위한 tracepoint provider package는 object 파일이나 shared library 형태입니다. 하나 이상의 tracepoin provider header (.h)와 tracepoint provider pacage source (.c)를 포함하고 있습니다.\nHeader 파일 템플릿 생성하기. #undef TRACEPOINT_PROVIDER#define TRACEPOINT_PROVIDER provider_name #undef TRACEPOINT_INCLUDE#define TRACEPOINT_INCLUDE \u0026#34;./tp.h\u0026#34; #if !defined(_TP_H) || defined(TRACEPOINT_HEADER_MULTI_READ)#define _TP_H #include \u0026lt;lttng/tracepoint.h\u0026gt; /* * Use TRACEPOINT_EVENT(), TRACEPOINT_EVENT_CLASS(), * TRACEPOINT_EVENT_INSTANCE(), and TRACEPOINT_LOGLEVEL() here. */ #endif /* _TP_H */ #include \u0026lt;lttng/tracepoint-event.h\u0026gt;위 템플릿에서 2번 째 라인의 provider_name은 자신의 tracepoint provider 이름으로 변경해주면 됩니다. 주의할 점은 tracepoint provider 이름은 항상 유니크해야 합니다. 만약 여러 사용자가 개발한 어플리케이션이 하나의 머신에서 동작하는 경우 각각의 네임이 충돌할 수 있으니 본인만을 위한 prefix를 넣어주는 것을 추천합니다. 그 다음 위 Header 파일의 이름을 *tp.h 로 변경해주세요.\n이제 주석으로 처리된 부분에 TRACEPOINT를 정의해 줍니다. TRACEPOINT() 매크로는 아래와 같이 구성됩니다.\nTRACEPOINT_EVENT( /* Tracepoint provider name */ provider_name, /* Tracepoint name */ tracepoint_name, /* Input arguments */ TP_ARGS( arguments ), /* Output event fields */ TP_FIELDS( fields ) ) 각 항목은 정의는 다음과 같습니다.\n provider_name : tracepoint provider 이름 tracepoint_name : tracepoint 이름 arguments : 입력 아큐먼트, 개수의 제한은 없습니다. 정의 방식은 타입, 네임이며 , 를 이용하여 구분합니다. fields : 출력 이벤트 필드 정의  #undef TRACEPOINT_PROVIDER#define TRACEPOINT_PROVIDER linuxias_hello_world #undef TRACEPOINT_INCLUDE#define TRACEPOINT_INCLUDE \u0026#34;./test_tp.h\u0026#34; #if !defined(_TEST_TP_H) || defined(TRACEPOINT_HEADER_MULTI_READ)#define _TEST_TP_H #include \u0026lt;lttng/tracepoint.h\u0026gt; TRACEPOINT_EVENT ( linuxias_hello_world, my_first_tracepoint, TP_ARGS( int, my_integer_arg, char*, my_string_arg ), TP_FIELDS( ctf_string(my_string_field, my_string_arg) ctf_integer(int, my_integer_field, my_integer_arg) ) ) #endif /* _TEST_TP_H */ #include \u0026lt;lttng/tracepoint-event.h\u0026gt;Source 파일 생성하기. Source 파일은 매우 단순합니다. 아래 내용 외에 추가할 부분은 없습니다.\n#define TRACEPOINT_CREATE_PROBES#define TRACEPOINT_DEFINE#include \u0026#34;test_tp.h\u0026#34;2. 어플리케이션 소스 코드에 tracepoint 추가하기 #include \u0026#34;stdio.h\u0026#34;#include \u0026#34;test_tp.h\u0026#34; int main(int argc, char *argv[]) { puts(\u0026#34;Press Enter to continue\u0026#34;); getchar(); tracepoint(linuxias_hello_world, my_first_tracepoint, argc, argv[0]); puts(\u0026#34;The end\u0026#34;); return 0; } 3. tracepointer provider 패키지와 사용자 어플리케이션을 빌드 및 링크하기 Makefile을 이용하여 빌드하였습니다.\nTARGET = lttng_ust_test CC = gcc CFLAGS = -c -g LDFLAGS = -llttng-ust -ldl INC+=-I. OBJS+=tp.o OBJS+=main.o all : $(TARGET) $(TARGET) : $(OBJS) $(CC) -o $@ $^ $(LDFLAGS) .c.o: $(CC) $(INC) $(CFLAGS) $\u0026lt; clean: rm -f $(OBJS) $(TARGET) 결과 확인하기. 트레이스 이벤트가 등록되었는지 확인합니다.\n$lttng-sessiond --daemonize $./lttng_ust_test 실행하면 정지해 있습니다. 다른 창에서 확인해 보면.. 아래와 같이 이벤트가 등록되어 있음을 확인할 수 있습니다. 만약 프로세스가 종료되면 이벤트들은 모두 사라지면 확인할 수 없습니다.\n$ lttng list --userspace UST events: ------------- PID: 17109 - Name: ./lttng_ust_test lttng_ust_tracelog:TRACE_DEBUG (loglevel: TRACE_DEBUG (14)) (type: tracepoint) lttng_ust_tracelog:TRACE_DEBUG_LINE (loglevel: TRACE_DEBUG_LINE (13)) (type: tracepoint) lttng_ust_tracelog:TRACE_DEBUG_FUNCTION (loglevel: TRACE_DEBUG_FUNCTION (12)) (type: tracepoint) lttng_ust_tracelog:TRACE_DEBUG_UNIT (loglevel: TRACE_DEBUG_UNIT (11)) (type: tracepoint) lttng_ust_tracelog:TRACE_DEBUG_MODULE (loglevel: TRACE_DEBUG_MODULE (10)) (type: tracepoint) lttng_ust_tracelog:TRACE_DEBUG_PROCESS (loglevel: TRACE_DEBUG_PROCESS (9)) (type: tracepoint) lttng_ust_tracelog:TRACE_DEBUG_PROGRAM (loglevel: TRACE_DEBUG_PROGRAM (8)) (type: tracepoint) lttng_ust_tracelog:TRACE_DEBUG_SYSTEM (loglevel: TRACE_DEBUG_SYSTEM (7)) (type: tracepoint) lttng_ust_tracelog:TRACE_INFO (loglevel: TRACE_INFO (6)) (type: tracepoint) lttng_ust_tracelog:TRACE_NOTICE (loglevel: TRACE_NOTICE (5)) (type: tracepoint) lttng_ust_tracelog:TRACE_WARNING (loglevel: TRACE_WARNING (4)) (type: tracepoint) lttng_ust_tracelog:TRACE_ERR (loglevel: TRACE_ERR (3)) (type: tracepoint) lttng_ust_tracelog:TRACE_CRIT (loglevel: TRACE_CRIT (2)) (type: tracepoint) lttng_ust_tracelog:TRACE_ALERT (loglevel: TRACE_ALERT (1)) (type: tracepoint) lttng_ust_tracelog:TRACE_EMERG (loglevel: TRACE_EMERG (0)) (type: tracepoint) lttng_ust_tracef:event (loglevel: TRACE_DEBUG (14)) (type: tracepoint) lttng_ust_lib:unload (loglevel: TRACE_DEBUG_LINE (13)) (type: tracepoint) lttng_ust_lib:debug_link (loglevel: TRACE_DEBUG_LINE (13)) (type: tracepoint) lttng_ust_lib:build_id (loglevel: TRACE_DEBUG_LINE (13)) (type: tracepoint) lttng_ust_lib:load (loglevel: TRACE_DEBUG_LINE (13)) (type: tracepoint) lttng_ust_statedump:end (loglevel: TRACE_DEBUG_LINE (13)) (type: tracepoint) lttng_ust_statedump:debug_link (loglevel: TRACE_DEBUG_LINE (13)) (type: tracepoint) lttng_ust_statedump:build_id (loglevel: TRACE_DEBUG_LINE (13)) (type: tracepoint) lttng_ust_statedump:bin_info (loglevel: TRACE_DEBUG_LINE (13)) (type: tracepoint) lttng_ust_statedump:start (loglevel: TRACE_DEBUG_LINE (13)) (type: tracepoint) linuxias_hello_world:my_first_tracepoint (loglevel: TRACE_DEBUG_LINE (13)) (type: tracepoint) 위에 등록된 이벤트를 이용하여 babeltrace로 확인해 봅시다.\n$lttng create Session auto-20200215-175158 created. Traces will be output to /home/linuxias/lttng-traces/auto-20200215-175158 $lttng enable-event --userspace linuxias_hello_world:my_first_tracepoint UST event linuxias_hello_world:my_first_tracepoint created in channel channel0 $lttng start Tracing started for session auto-20200215-175158 // 예제 프로세스를 종료합니다. $lttng destroy Waiting for destruction of session \u0026#34;auto-20200215-175158\u0026#34;... Session \u0026#34;auto-20200215-175158\u0026#34; destroyed $babeltrace ~/lttng-traces [17:52:13.462294498] (+412.893280571) desktop linuxias_hello_world:my_first_tracepoint: { cpu_id = 0 }, { my_string_field = \u0026#34;./lttng_ust_test\u0026#34;, my_integer_field = 4 } babeltrace를 이용하면 문자열 기반의 로그 형태로 확인을 할 수 있습니다.\nUserspace 친구들 LTTng는 사용자 영역 트레이싱을 위해 추가적인 라이브러리들을 지원합니다.\n   Library Description     liblttng-ust-libc-warrper.so C 표준 라이브러리 추적 도구   liblttng-ust-pthread-wrapper.so POSIX pthread 함수 추적 도구   liblttng-ust-cyg-profile.so 함수의 진입과 종료에 대한 추적 도구. 주의할 점은 이 라이브러리를 사용하기 위해서 어플리케이션이 -finstrument-function 컴파일 플래그가 적용되어 컴파일 되어야 한다.   liblttng-ust-dl.so dlopen(3), dlclose(3) 함수 호출 추적 도구    위 라이브러리들은 어플리케이션 빌드 시 링크될 필요가 없이 LD_PRELOAD를 이용하여 어플리케이션에 적용할 수 있다.\n$ LD_PRELOAD=liblttng-ust-dl.so my-app $ LD_PRELOAD=liblttng-ust-cyg-profile.so:liblttng-ust-libc-warrper.so my-app 위 처럼 어플리케이션 실행 시 각 라이브러리의 이벤트가 등록되고 lttng list --userspace 명령어를 통해 확인할 수 있다. 아래는 LTTng userspace helper test를 빌드하여 순차적으로 적용한 결과 중 일부분을 가져온 내용이다. 예제 코드 내에 재귀함수가 있기에 func_entry 이벤트가 순차적으로 발생한 후 func_exit 이벤트가 발생하는 것을 확인할 수 있다.\n$babeltrace ~/lttng-traces/auto-20200218-204301 ... [20:38:11.907388453] (+3.229928201) desktop lttng_ust_libc:malloc: { cpu_id = 1 }, { size = 4, ptr = 0x151BD40 } [20:38:11.907391355] (+0.000002902) desktop lttng_ust_libc:free: { cpu_id = 1 }, { ptr = 0x151BD40 } [20:38:11.907395975] (+0.000004620) desktop lttng_ust_cyg_profile:func_entry: { cpu_id = 1 }, { addr = 0x4006B6, call_site = 0x40076D } [20:38:11.907396463] (+0.000000488) desktop lttng_ust_cyg_profile:func_entry: { cpu_id = 1 }, { addr = 0x4006B6, call_site = 0x4006ED } [20:38:11.907396778] (+0.000000315) desktop lttng_ust_cyg_profile:func_entry: { cpu_id = 1 }, { addr = 0x4006B6, call_site = 0x4006ED } [20:38:11.907397090] (+0.000000312) desktop lttng_ust_cyg_profile:func_entry: { cpu_id = 1 }, { addr = 0x4006B6, call_site = 0x4006ED } [20:38:11.907397410] (+0.000000320) desktop lttng_ust_cyg_profile:func_entry: { cpu_id = 1 }, { addr = 0x4006B6, call_site = 0x4006ED } [20:38:11.907397722] (+0.000000312) desktop lttng_ust_cyg_profile:func_entry: { cpu_id = 1 }, { addr = 0x4006B6, call_site = 0x4006ED } [20:38:11.907398032] (+0.000000310) desktop lttng_ust_cyg_profile:func_entry: { cpu_id = 1 }, { addr = 0x4006B6, call_site = 0x4006ED } ... [20:38:11.907402689] (+0.000000283) desktop lttng_ust_cyg_profile:func_exit: { cpu_id = 1 }, { addr = 0x4006B6, call_site = 0x4006ED } [20:38:11.907402972] (+0.000000283) desktop lttng_ust_cyg_profile:func_exit: { cpu_id = 1 }, { addr = 0x4006B6, call_site = 0x4006ED } [20:38:11.907403263] (+0.000000291) desktop lttng_ust_cyg_profile:func_exit: { cpu_id = 1 }, { addr = 0x4006B6, call_site = 0x4006ED } [20:38:11.907403543] (+0.000000280) desktop lttng_ust_cyg_profile:func_exit: { cpu_id = 1 }, { addr = 0x4006B6, call_site = 0x4006ED } [20:38:11.907403818] (+0.000000275) desktop lttng_ust_cyg_profile:func_exit: { cpu_id = 1 }, { addr = 0x4006B6, call_site = 0x4006ED } [20:38:11.907404092] (+0.000000274) desktop lttng_ust_cyg_profile:func_exit: { cpu_id = 1 }, { addr = 0x4006B6, call_site = 0x4006ED } [20:38:11.907404378] (+0.000000286) desktop lttng_ust_cyg_profile:func_exit: { cpu_id = 1 }, { addr = 0x4006B6, call_site = 0x40076D } [20:38:11.907404684] (+0.000000306) desktop lttng_ust_cyg_profile:func_exit: { cpu_id = 1 }, { addr = 0x40070D, call_site = 0x7FD716ED8830 } ... 추적 결과 확인하기 결과를 확인하는 방법은 여러 가지가 있습니다. LTTng docs에서 제시하는 방법은 3가지 입니다.\n babeltrace tracecompass lttng-analyses  이 중 tracecompass를 사용하려 합니다. tracecompass는 eclipse 기반의 분석 툴로서 lttng-analyses를 External analyses로 플러그인 처럼 추가하여 사용할 수 있습니다. lttng-analyses는 babeltrace를 필요로 하기에 3가지 툴 모두 설치를 하여야 합니다.\ntracecompass를 이용하여 lttng-traces 결과를 import 하게되면 아래와 같이 표시됩니다. tracecomposs는 userspace보다는 kernel 트레이싱 시에 더 큰 효율을 발휘 합니다.\n사용자 영역에 대해서는 로그 수준의 화면만 확인할 수 있습니다.\n감사합니다.\n"
},
{
	"uri": "https://linuxias.github.io/sw_architecture/pipe_and_filter_architecture/",
	"title": "Pipe and Filter Architecture",
	"tags": [],
	"description": "",
	"content": "Data flow Architecture에는 Batch Sequential, Pipe and Filter, Process Control Architecture 로 3가지로 분류할 수 있습니다. 그 중 Pipe and Filter Architecture에 대해 정리해보려 합니다.\nPipe and Filter Architecture는 데이트 스트림을 처리하는 시스템을 위한 구조를 제공합니다. 데이터를 처리하는 각 프로세싱 단계는 Filter 컴포넌트 내부에 포함되어 있습니다. 데이터는 Filter 사이를 Pipe를 통해 전달되게 됩니다. 이러한 구조로 인해 Pipe and Filter Architecture는 Batch Sequential Architecture와 많이 비교됩니다.\nPipe and Filter 구조의 구성요소는 크게 3가지 입니다. 데이터 스트림, 필터, 그리고 파이프입니다. 데이터 스트림은 XML이나 Json 파이트 스트림 등 first-in / first-out 버퍼를 가지고 있습니다. 특정 시스템에서는 마샬링, 언마샬링도 사용합니다. 다음 구성요소인 필터는 Pipe and Filter Architecture에서 독립적으로 데이터 스트림을 처리하는 구성요소입니다. 입력 데이터 스트림으로부터 데이터를 읽고, 읽어들인 데이터를 처리한 후 다음 필터로 전달하도록 파이프로 데이터를 전달합니다. Pipe and Filter는 데이터가 연결된 파이프를 통해 전달되면 그 즉시 처리를 하고 다음 필터로 전달합니다. 필터를 독립적으로 동작하므로 시스템에서 자유롭게 교체 및 추가가 가능합니다. 여기서 필터는 2가지 타입으로 다시 분류할 수 있습니다. Active(능동형) 필터와 Passive(수동형) 필터입니다. 먼저 능동형 필터는 데이터를 가져오고 전달하는 것을 필터에서 처리하고 수동형 필터는 파이프가 필터로부터 데이터를 가져오고, 다음 필터로 전달합니다. 즉 능동형 필터는 수동형 파이프와 함께 동작하고 수동형 필터는 능동형 파이프와 함께 동작합니다. 마지막 구성요소인 파이프는 필터 사이에 데이터 스트림을 이동하는 경로입니다.\nPipe and Filter 구조는 리눅스 사용자라면 많이 사용하는 파이프를 생각하시면 편합니다. $cat example.txt | grep \u0026#39;test\u0026#39; 위와 같은 예제는 쉽게 이해할 수 있습니다. cat을 이용해 example.txt 내부 문자열들을 파이프를 통해 grep에게 전달되고 처리하게 됩니다.\n이러한 Pipe and Filter Architecture의 장점은 Concurrency(동시성), Reusability(재사용), Modifiability(변경용이성), Simplicity(단순성), Flexibility(유연함)입니다. Concurrency는 과도한 데이터 처리에 대해 각 필터가 독립적으로 동작하여 높은 처리량을 얻을 수 있습니다. Reusability는 각 필터가 독립적으로 동작되며 다른 필터와의 종속성이 없으므로 각 필터를 다른 시스템에 재사용이 가능합니다. Modifiability는 필터 간 종속성이 낮기에 새로운 필터를 추가하거나 수정, 제거했을때에도 시스템에 다른 수정을 최소화 할 수 있습니다. 두 필터 사이의 파이프가 존재한다는 매우 단순한 구조를 가지고 있으며 각 해당 시스템의 데이터를 Sequential하게 Parallel하게 수행이 가능함으로 유연한 구조를 만들 수 있습니다.\n하지만, Pipe and Filter 구조에도 여러 단점이 있습니다. 데이터 스트림 형태가 고정된 형태의 구조이기에 동적으로 데이터 포맷을 변경하는 구조에는 알맞지 않습니다. 만약 A 필터로 이미지가 입력되었는데 출력으로는 XML 포맷으로 출력하고 B 필터는 XML을 입력받아 Character Stream으로 출력한다면, 이 구조의 장점인 변경용이성, 유연함을 잃어버리게 됩니다. 이런 장점과 단점을 이해하고 정확하게 필요한 곳에 구조를 적용하는 연습이 필요할 것 같습니다. 마지막으로 많이 비교되는 Batch Sequential Architecture와의 차이점을 정리하고 글을 끝맺겠습니다.\nBatch Sequential 과의 차이점 위에서 정리한 내용을 보면 Batch Sequential Architecture와 매우 유사해 보이지만 큰 차이점을 가지고 있습니다. Batch Sequential은 데이터가 처리되고 다음 데이터 처리 단계로 넘어가기 위해선 이전 데이터 처리가 모두 완료되어야 합니다. 즉 A에서 B 처리 단계로 데이터가 전달되기 위해선 모든 데이터가 A처리가 완료된 이후 B로 입력됩니다. 하지만 Pipe And Filter Architecture는 데이터 스트림 처리를 위한 구조로서 A 단계에서 모든 데이터가 처리되고 B의 입력이 되는 것이 아닌 A 단계에서 먼저 처리된 데이터는 바로 B의 입력으로 Pipe를 통해 전달됩니다. 100개의 데이터가 있을 때 Batch Sequential은 100개가 모두 처리된 이후 다음 스텝으로 입력되지만, Pipe And Filter는 100개 중 처리된 데이터는 B로 전달됩니다. 이 점이 가장 큰 차이입니다.\n감사합니다.\n"
},
{
	"uri": "https://linuxias.github.io/sw_architecture/process_control_architecture/",
	"title": "Process Control Architecture",
	"tags": [],
	"description": "",
	"content": "Process Control Architecture는 Data Flow Architecture 분류에 속하는 아키텍처입니다. 해당 분류에 속하는 아키텍처는 이전에 다뤘던 Batch Sequential, Pipe and Filter Architecture가 있습니다. 자세한 내용은 아래 링크 참고 부탁드립니다.\n  Data Flow Software Architectures\n  Batch Sequential Architecture\n  Pipe and Filter Architecture\n  Process Control Architecture에 대해 간략히 정리해보겠습니다.\nProcess Control의 가장 큰 특징은 데이터의 흐름이 프로세스의 실행을 제어하는 변수 집합이라는 것입니다. 한 번에 이해하기 어려운 말인 것 같습니다. 좀 더 살펴보겠습니다.\nProcess Control Architecture는 임베디드 시스템에서 많이 사용됩니다. 시스템이 프로세스를 제어할 수 있는 변수에 의해 조작되는 시스템에 알맞는 아키텍처입니다. 많은 임베디드 시스템은 연속적으로 동작해야 합니다. 안정된 상태에 대한 출력 데이터를 유지하는게 가장 중요한 시스템입니다. 예를 들어 크루즈나 화장실 변기를 많이 예시로 듭니다. 화장실 변기 물을 내리면 다시 물이 차오릅니다. 그 때 차오르는 물의 높이는 항상 일정합니다. 이러한 시스템은 물의 높이 즉 출력 데이터를 안정화 시키기 위해 프로세스가 데이터를 제어하게 되는 구조가 됩니다.\n해당 시스템을 구성하는 몇 개의 서브시스템이 있는데 각 서브시스템은 아래와 같습니다.\n  Controlled Variable : 기본 시스템에 대한 값을 제공하며 센서에 의해 측정되어지는 값\n  Input Variable : 프로세스에 대한 입력 값\n  Manipulated Variable : 컨트롤러에 의해 조정되거나 변경되는 값\n  Process : 변수를 조작하기 위한 메커니즘\n  Sensor : 시스템 제어와 같련된 변수의 값을 구하며 조작된 변수를 재계산 하기 위한 피드백으로 사용\n  Set Point : 이 값은 제어된 변수에 대한 원하는 값\n  Control Algorithm : 프로세스 변수 조작 방법을 결정하는데 사용함\n  출처 : https://www.cs.cmu.edu/afs/cs/project/tinker-arch/www/html/Tutorial_Slides/Soft_Arch/base.097.html\n위 그림은 Cruise Control 시스템의 예제입니다. 자동차의 크루즈 모드는 일정한 속도를 유지하기 위한 시스템입니다. 원하는 속도가 입력 값이 되고 컨트롤러에 의해 Throttle이 설정됩니다. 해당 Throttle은 엔진을 동작하게 만들고 엔진은 바퀴를 회전시킵니다. 바퀴의 회전은 센서에 의해 측정되고 Controller에게 전달됩니다. Controller는 Desired Speed에 도달할 때 까지 지속적으로 Throttle을 설정하게됩니다.\n위 와 같은 시스템이 Process Control Architecture를 적용한 시스템이라고 보시면 됩니다. 바퀴의 회전을 센서로 다시 컨트롤러에게 전달되는데요, 이처럼 출력 데이터가 다시 컨트롤러의 입력으로 전달되어 Close-loop 형태를 뛰는 구조들이 있습니다. 만약 출력 데이터가 다시 피드백 되지 않는다면 Open-loop 형태라고 합니다. Close-loop Feedback은 Open-loop보다 출력 데이터를 제어하는데 훨씬 좋은 구조라는 것을 알 수 있을겁니다.\n이상으로 Process Control Architecture에 대해 정리해보았습니다.\n감사합니다.\n"
},
{
	"uri": "https://linuxias.github.io/container/docker/container_resource/",
	"title": "Resource Limitation",
	"tags": [],
	"description": "",
	"content": "컨테이너는 기본적으로 리소스 제한이 없으며 호스트의 커널 스케쥴러에 의해 허용되는 주어진 리소스를 사용할 수 있습니다. 가끔 각 컨테이너 별로 리소스(CPU, Memory 등)를 제한할 필요가 생길 수 있습니다. 이런 경우를 위해 도커에서는 컨테이너 별로 어느 정도의 CPU, Memory 자원을 사용할 지에 대해 제한할 수 있는 방법을 제공하고 있습니다.\n이 기능들이 지원되기 위해서는 커널이 필요로 합니다. 커널에서 지원하는 여부는 docker run 명령어를 사용하여 확인할 수 있습니다.\nsudo docker info [sudo] password for linuxias: Client: Debug Mode: false Server: Containers: 1 Running: 1 Paused: 0 Stopped: 0 Images: 5 Server Version: 19.03.5 Storage Driver: overlay2 Backing Filesystem: extfs Supports d_type: true Native Overlay Diff: true Logging Driver: json-file Cgroup Driver: cgroupfs Plugins: Volume: local Network: bridge host ipvlan macvlan null overlay Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog Swarm: inactive Runtimes: runc Default Runtime: runc Init Binary: docker-init containerd version: b34a5c8af56e510852c35414db4c1f4fa6172339 runc version: 3e425f80a8c931f88e6d94a8c831b9d5aa481657 init version: fec3683 Security Options: apparmor seccomp Profile: default Kernel Version: 4.15.0-76-generic Operating System: Ubuntu 16.04.6 LTS OSType: linux Architecture: x86_64 CPUs: 8 Total Memory: 31.36GiB Name: desktop ID: EKUS:7MJY:7SHK:ORWK:BHEB:3LDT:DLYP:BC66:T6B5:Z7PN:H22E:LNBL Docker Root Dir: /var/lib/docker Debug Mode: false Registry: https://index.docker.io/v1/ Labels: Experimental: false Insecure Registries: 127.0.0.0/8 Live Restore Enabled: false WARNING: No swap limit support docker run 명령어로 확인 시 가장 아래 WARNING: No swap limit support 라는 경고가 발생한다면 아래와 같이 조치하시면 됩니다. 아래 내용은 Docker docs에서 참조한 내용입니다.\nWARNING: Your kernel does not support swap limit capabilities. Limitation discarded.This warning does not occur on RPM-based systems, which enable these capabilities by default.If you don’t need these capabilities, you can ignore the warning. You can enable these capabilities on Ubuntu or Debian by following these instructions. Memory and swap accounting incur an overhead of about 1% of the total available memory and a 10% overall performance degradation, even if Docker is not running.Log into the Ubuntu or Debian host as a user with sudo privileges.Edit the /etc/default/grub file. Add or edit the GRUB_CMDLINE_LINUX line to add the following two key-value pairs:GRUB_CMDLINE_LINUX=\u0026#34;cgroup_enable=memory swapaccount=1\u0026#34;Save and close the file.Update GRUB.$ sudo update-grubIf your GRUB configuration file has incorrect syntax, an error occurs. In this case, repeat steps 2 and 3.The changes take effect when the system is rebooted.Memory Memory 부족의 위험성..? 실행 중인 컨테이너가 호스트 시스템의 메모리를 과다하게 사용하는 것은 매우 위험합니다. 컨테이너를 동작 시에 호스트 시스템의 메모리를 너무 많이 사용하지 않도록 주의해 주세요. 리눅스 호스트에서 커널이 시스템을 원활하게 동작시키기 위한 과정 중에 메모리가 부족하다는 것을 감지하면 OOME 또는 OOM 이 발생하고 프로세스를 강제 종료하게 됩니다. 즉 도커 컨테이너를 위함 여러 중요 프로세스들이 종료될 수 있습니다. 원치 않는 종료는 시스템에 큰 영향을 주고 시스템이 종료될 수 도 있으니 주의해주세요!\n과다한 메모리 사용으로 인해 도커 데몬이 커널에 의해 종료되는 위험을 최소화 하기 위한 방안이 있습니다. OOM 우선순위를 변경하여 커널에 의해 종료되는 여러 프로세스 들 중 우선순위를 낮추는 것 입니다. 위 방법은 cgroup memory 자원에 대한 제어 방법에서도 말씀 드린 적이 있습니다. cgroup으로 memory 자원 추상화된 프로세스에 대해 OOM을 disable 함으로서 종료됨을 막고 메모리가 필요하다면 사용할 수 있는 메모리가 생길 때 까지 대기하고 있는 것입니다. 도커 데몬도 위와 같은 방법을 제한합니다. 도커 데몬이 아닌 컨테이너는 OOM 우선 순위가 조정되지 않습니다. 따라서 Docker 데몬이나 다른 시스템 프로세스가 종료되는 것보다 개별 컨테이너가 종료 될 가능성이 높아집니다. 데몬이나 컨테이너에서 수동으로 --oom-score-adj를 최소값으로 설정하거나 컨테이너에서 --oom-kill-disable을 설정하여 이러한 안전 장치를 피하려고 시도해서는 안됩니다. 이 점 꼭 유의해주세요.\nnginx 이미지를 이용하여 메모리가 1GB로 제한되어 있는 컨테이너를 생성해봅시다. 제한된 메모리 설정은 docker inspect 명령어를 사용하여 확인하였습니다.\n$sudo docker run -d --memory=\u0026#34;1g\u0026#34; nginx WARNING: Your kernel does not support swap limit capabilities or the cgroup is not mounted. Memory limited without swap. 5019001493a593b4c7082362a1af05ee754d7c142705ecbb9a5e1306b82abb2d $sudo docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 5019001493a5 nginx \u0026#34;nginx -g \u0026#39;daemon of…\u0026#34; 23 seconds ago Up 22 seconds 80/tcp beautiful_chandrasekhar $sudo docker inspect beautiful_chandrasekhar | grep Memory \u0026#34;Memory\u0026#34;: 1073741824, \u0026#34;KernelMemory\u0026#34;: 0, \u0026#34;KernelMemoryTCP\u0026#34;: 0, \u0026#34;MemoryReservation\u0026#34;: 0, \u0026#34;MemorySwap\u0026#34;: -1, \u0026#34;MemorySwappiness\u0026#34;: null, 만약 nginx 컨테이너 내부에서 할당된 1GB의 메모리 이상의 메모리를 사용하게 되면 컨테이너는 자동으로 종료됩니다. 그렇기에 각 컨테이너에 메모리를 할당할 시 적절량을 잘 생각하셔서 할당해 주셔야 합니다. --memory 옵션 외의 메모리와 관련된 옵션을 정리하면 아래와 같습니다.\n   option description     --memory-swap 디스크로 swap 할 수 있는 메모리 설정   --memory-swappiness 기본적으로 호스트 커널은 컨테이너가 사용하는 익명 페이지의 비율을 바꿀 수 있습니다. \u0026ndash;memory-swappiness를 0에서 100 사이의 값으로 설정하여이 백분율을 조정할 수 있습니다.   --memory-reservation 호스트 시스템에서 경합 또는 메모리 부족을 감지하면 활성화되는 \u0026ndash;memory보다 작은 소프트 제한을 지정할 수 있습니다. \u0026ndash;memory-reservation을 사용하는 경우 우선 순위를 지정하려면 \u0026ndash;memory보다 낮게 설정해야합니다. 소프트 한계이므로 컨테이너가 한계를 초과하지는 않습니다.   --kernel-memory 컨테이너가 사용할 수 있는 커널 메모리의 최대 값입니다. 허용되는 최소값은 4m입니다. 커널 메모리는 스왑아웃 할 수 없기 때문에 커널 메모리가 부족한 컨테이너는 호스트 시스템 리소스를 차단될 수 있으며 이로 인해 호스트 시스템과 다른 컨테이너에 부작용이 발생할 수 있습니다.   \u0026ndash;oom-kill-disable 기본적으로 메모리 부족 (OOM) 오류가 발생하면 커널은 컨테이너의 프로세스를 종료합니다. 이 동작을 변경하려면 \u0026ndash;oom-kill-disable 옵션을 사용하십시오. -m /-memory 옵션도 설정 한 컨테이너에서는 OOM 킬러 만 비활성화하십시오. -m 플래그를 설정하지 않으면 호스트에 메모리가 부족할 수 있으며 커널은 메모리를 비우기 위해 호스트 시스템의 프로세스를 종료해야합니다.    CPU 기본적으로 호스트 시스템에서 동작하는 컨테이너들은 시스템의 CPU 사용량에 제한이 없습니다. 하지만 CPU 사용량을 제한할 수 있는 방법이 있습니다. 대부분 사용자들은 CFS 스케쥴러를 기본으로 사용하고 있지만, Docker 1.13 버전 이상에서는 real-time 스케쥴러로 변경이 가능합니다. 먼저 CFS 스케쥴러에 대한 설정부터 정리해보겠습니다.\nCPU - CFS 스케쥴러 CFS 스케쥴러는 리눅스 커널 CPU 스케쥴러입니다. 스케쥴러에 대한 자세한 설명은 생략하겠습니다.\n1. \u0026ndash;cpus 컨테이너가 사용할 수 있는 CPU 리소스 양을 지정합니다. 만약 cpu가 8개 있는 호스트 시스템에서 --cpus=0.5로 지정하면 컨테이너는 최대 4개의 CPU 사용을 보장 받을 수 있습니다.\n2. \u0026ndash;cpu-shares cpu-shares 옵션은 컨테이너에 가중치를 설정해 해당 컨테이너가 CPU를 상대적으로 얼마나 사용할 수 있는지를 설정할 수 있습니다. 기본적으로 1024가 기본 값으로 설정되어 있습니다. 컨테이너 간 사용하는 상대 비율로써 하나의 컨테이너가 1024, 다른 하나가 2048로 설정되어 있다면, 1:2 의 비율로 CPU 자원을 공유하게 됩니다.\n$ sudo docker run -it --cpu-shares 1024 ubuntu:18.04 3. \u0026ndash;cpuset-cpu 여러 CPU를 자유롭게 컨테이너들이 사용할 수 있도록 할수도 있지만, 특정 CPU만 사용할 수도 있게 할 수 있습니다. 아래 이미지는 문영일님의 블로그인 문c블로그에서 참조하였습니다. 이미지를 보시면 쉽게 이해가 되실 겁니다. 위 방식의 이점은 명확합니다. 코어 스위칭 비용을 없앰으로서 cache miss로 인한 성능저하 등의 문제를 최소화 할 수 있습니다.\nsudo docker run -it --cpuset-cpus=2 --name cpu_test ubuntu:18.04 root@452468a96c63:/# apt update \u0026amp;\u0026amp; apt install stress root@452468a96c63:/# stress --cpu 1 .... 다음 호스트 시스템에서 `htop`을 이용하여 확인해봅시다. ```bash $htop 1 [| 0.7%] 5 [ 0.0%] 2 [ 0.0%] 6 [ 0.0%] 3 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%] 7 [ 0.0%] 4 [ 0.0%] 8 [|| 1.3%] Mem[|||||||||||||||||||||||||| 4.33G/31.4G] Tasks: 182, 656 thr; 2 running Swp[| 2.00M/31.9G] Load average: 1.04 0.88 0.47 Uptime: 1 day, 19:25:52 CPU 3번(1번부터 0입니다.) 이 100%로 표기되어 있습니다. \u0026ndash;cpuset-cpus=2 옵션으로 인해 ubuntu:18.04 이미지로 생성한 컨테이너에서 stress 테스트 진행하였으니, 3번이 100%임을 확인할 수 있습니다.\n4. \u0026ndash;cpu-period, \u0026ndash;cpu-quota 컨테이너의 CFS 주기는 기본적으로 100ms로 설정되어 있습니다. 위 명령어를 사용하여 변경이 가능합니다.\n$sudo docker run -it --cpu-period=100000 --cpu-quota=25000 --name cpu_test ubuntu:18.04 #apt update \u0026amp;\u0026amp; apt install stress #stress --cpu 1 위와 같이 컨테이너를 생성하고 stress를 설치하여 실행해 봅시다. \u0026ndash;cpu-period 100000은 100ms를 의미합니다. \u0026ndash;cpu-quota는 \u0026ndash;cpu-period로 설정된 시간 중 CPU 스케쥴링에 얼마나 할당할 지 설정해 줍니다. 컨테이너가 스로틀링되기 전에 제한되는 마이크로 초 수입니다. 이 기능은 효과적인 한도 역할을합니다. Docker 1.13 이상을 사용하는 경우 앞서 설명한 \u0026ndash;cpus를 대신 사용하는 것을 추천드립니다.\nDocker docs에는 아래와 같이 추천하고 있습니다.\nDocker 1.13 and higher: docker run -it --cpus=\u0026#34;.5\u0026#34; ubuntu /bin/bash Docker 1.12 and lower: $ docker run -it --cpu-period=100000 --cpu-quota=50000 ubuntu /bin/bash "
},
{
	"uri": "https://linuxias.github.io/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://linuxias.github.io/linux/tips/",
	"title": "Tips",
	"tags": [],
	"description": "",
	"content": "시스템에서 kernel config 확인하기 본인의 리눅스 시스템 마다 3개 중 하나를 사용할 수 있습니다.\ncat /proc/config.gz cat /boot/config cat /boot/config-$(uname -r) "
},
{
	"uri": "https://linuxias.github.io/sw_architecture/sw_design_corruption/",
	"title": "소프트웨어 설계의 부패",
	"tags": [],
	"description": "",
	"content": "소프트웨어 설계는 무엇일까? 소스코드 작성하기 전 UML 다이어그램을 작성하는 것? 가끔 몇몇 개발자 분들과 이야기를 할 때 설계는 UML 다이어그램을 작성하는 것이라는 말을 듣는다. 그럼 설계는 UML 다이어그램과 동일 시 할 수 있는가에 대해서 고민해보면 그렇지 않다 라는 결론이 나올 수 있다. 다이어그램은 설계에서 부수적인 부분일 뿐 설계 그 자체가 될 수는 없다고 생각한다. 소프트웨어 설계는 매우 추상적인 것이라 생각한다. 사용자 요구사항부터 시나리오, 모듈, 클래스, 메소드와 더불어 어떠한 형태와 구조를 가질 것인지 프로그램 전체의 형태, 구조와도 관련이 있다고 생각한다. 과거에 프로젝트를 개발할 때 가끔 머리속으로 설계에 대한 그림이 그려질 때가 있었다. 내 머리속에 들어있는 설계의 청사진이 옳다고 생각하고 한 번에 첫 번째 릴리즈까지 가능한 적이 있었다. 그 순간은 매우 뿌듯하고 기분 좋았으나 점차 내 머리속의 설계는 잘못되었고 설계가 부패한 고기처럼 썩어있다는 걸 확인할 수 있다. 외형적으로 매우 신선해보였으나 내부를 살펴보니 곪고 또 곪아서 어떻게 할 수가 없는 상태가 된 적도 있다. 재설계를 하려해도, 시간적 소모와 지속적인 사용자의 요구사항으로 인해 새로운 설계안도 설계와 동시에 썩어들어가는 기분을 느꼈다. 그럼 언제 내 설계가 잘못되었는지 알 수 있는 방법이 없을까? 부패하고 있다는 느낌이 들때가 언제인지 고민을 하다 로버트 마틴의 \u0026lsquo;클린 소프트웨어\u0026rsquo; 란 도서에서 잘 정리해둔 것 같아 인용하여 정리해보려 한다.\n지금부터 아래와 같은 조짐이 보이면 그 설계는 부패하고 있다고 판단할 수 있다. 누구나 알고있으면서 쉽게 놓치고 있는 부분이라 생각한다.\n경직성 시스템의 변경이 어려운 상태이다. 변경을 하려보니 다른 여러 부분들까지 변경되어야 하는 구조이다. 단순한 방법으로 변경하려 해보지만 쉽게 변경이 어렵다. 즉 의존성으로 인해 하나의 변경이 의존된 다른 부분까지 모두 영향을 미치게 되는 형태이다. 이런 파급효과는 개발자를 지치게하고 이런 모듈의 변경사항이 많아질수록 설계는 유연성을 점점 잃어간다. 새로운 요구사항이 발생하였을 때 해당 요구사항에 대해 분석하고 추가, 수정의 범위 등 작업량을 산출해 내지만 실제 해당 작업을 하다보면 예상하지 못한 부분에서 변경이 필요한 부분이 있다는 것을 깨닫게 된다. 산출한 작업량보다 잘못하면 2, 3배 이상의 작업량 추정이 발생할 수 있다. 취약성\n특정 부분 설계 변경 시 전혀 상관관계가 없는 부분까지 영향을 미쳐 망가지게 된다. 위의 경직성과 다르게 의존성이 없는데도 그러한 경우가 있다. 변경사항의 영향이 전혀 관계없는 부분까지 망가뜨리게 된다면 개발자는 어떠한 생각이 들까? 문제가 문제를 만들어내는 최악의 경우가 될 것이다. 어떤 모듈의 취약성이 점점 심해질수록 전혀 관계없는 부분에서 계속 문제가 발생하게 되고 그럴 가능성은 점점 커진다. 이슈는 계속 생기게되고 개발자는 지쳐간다. 개발자 중 이슈가 발생했을 때 \u0026lsquo;이 부분은 내가 수정한 부분이 아니에요, 내 이슈가 아니에요\u0026rsquo; 라고 말하는 개발자들이 가끔 있다. 안타까운 일이다.\n부동성\n움직이지 않는 컴포넌트, 음 움직일 수 없는 컴포넌트가 더 어울리는 말인 것 같다. 설계한 시스템에서 다른 시스템에서 재사용할 수 있는 부분은 재사용하는게 가장 좋다. 하지만 가끔 설계한 소프트웨어에서 재사용가능한 부분을 전혀 찾지 못하는 경우가 있다. 다른 시스템에서도 충분히 유용하게 사용할 수 있지만 분리해낼 수 없는 부분들, 분리해내려하면 많은 시간적 소모와 분리 시 발생할 수 있는 위험으로 인해 쉽게 설계를 변경할 수 없는 경우를 말한다.\n점착성\n점착성이란 간단히 말하면 다른 물질에 끈끈히 달라붙는 성질을 말한다. 설계에서 점착성은 소프트웨어의 점착성과 환경의 점착성이라는 2가지 형태로 나뉠 수 있다. 변경사항이 발생했을 때 개발자들은 변경할 수 있는 여러가지 방법들을 찾게된다. 설계가 유지된 형태일 수도 아닐수도 있다. 설계를 유지한 상태에서 변경하는 방법이 설계를 변경하는 방법보다 어렵다면 그 설계는 점착성이 매우 높은 것이다. 서로 간 너무 끈끈히 붙어있어 수정이 매우 어려운 상황이다. 환경의 점착성은 개발환경에서 비롯된다. 주변 개발환경으로 인해 좀 더 빠른 개발을 위해 설계를 변경하고 싶을 때가 있다. 하지만 설계의 유지는 생각치 않고 잘못된 유혹에 빠져 설계가 변경된다면 옳은 동작을 하지 못할 수도 있다. 개발환경의 개선이 설계까지 영향을 미치는 잘못된 상황을 말이다.\n불필요한 복잡성\n설계를 하다보면 직접적으로 전혀 쓸모없는 구조가 설계에 포함되어 있을 수 있다. 설계하는 과정에선 전혀 발견하지 못하다가 리뷰를 하다보면 그러한 경우를 발견을 할 때도 있고 소스코드 작성 중에 발견할 수 도 있다. 나중에 필요하겠지 란 생각으로 일단 설계해놓고 전혀 불필요한 구조가 들어가 있는 것은 복잡성만 증가시키는 행위이다. 개발자 스스로 미래의 발생할 수 있는 요구사항에 대해 너무 고민한 나머지, 오버엔지니어링을 발생시킬 수 있다. 처음엔 뿌듯할 수 있다. 나중에 이런 요구사항이 왔을 때 내 설계는 유지된 상태에서 제공을 할 수 있다고, 분명 그 말도 맞는 말이다. 미래의 변경을 대비할 수 있다면 설계의 유연성과 추후 변경사항을 방지해줄 수 있다. 하지만 예상했던 효과와 다른 악영향을 미칠 수도 있음을 항상 고려해야 한다. 지금 전혀 사용하지 않는 구조로 인해 설계는 점점 복잡해져가고 미래의 원하는 성과도 얻지 못할 수 있다. 그런 경우를 더 많이 보았다. 불필요한 반복\n단일 추상 개념으로 통합할 수 있는 반복적인 구조가 설계에 포함되어 있다. 주변 개발자분들 중 가끔 필요한 내용을 Ctrl C + V를 이용해 붙여넣는 경우가 있다. 빠른 기능구현을 위해 \u0026lsquo;복붙'은 매우 유용하지만 사용할 때와 사용하지 말아야할 때를 인식해야 한다. 그렇게 붙여넣은 기능이 여러 곳일 때 문제 발생 시 모든 곳을 찾아다니며 수정하는 경우를 보았다. 리뷰 패치가 올라왔을 때 동일한 수정이 수 십 곳이 되는 경우도 있었다. 중복된 기능은 왜 계속 중복되는 꼭 복붙으로만 해결할 수 있는지에 대한 고민을 한번이라도 했으면 그런 구조는 나오지 않았을거란 아쉬움이 남는다. 적절한 추상화를 통해 반복되는 부분을 없애는 일은 최우선 순위는 아니지만 그 일을 행함으로서 매우 유지보수가 쉬운 소프트웨어 구조를 가질 수 있다. 불투명성\n전혀 이해하기 어려운 애매모호한 구조가 있다. 어떠한 의도인지 이해가 쉽지 않은 구조들은 명료하지 않기에 더욱 변경사항에 유연하게 대처하기 어려운 경향이 있다. 처음엔 명료한 구조였지만 추후 점차 불투명성이 커지는 구조가 될 수도 있다. 지속적으로 명료한 구조를 가지기 위해 노력해야 한다. 시간이 지남에 따라 불투명성으로 인해 구조가 점점 부패해지는 경우를 막아야 할 것이다.\n위와 같이 소프트웨어 설계가 썩어들어가는 여러 신호들이 있다. 이를 무시하지 말고 융통성있게 대처할 수 있어야 할 것 같다.\n참고 : 로버트 C 마틴 | 클린 소프트웨어\n"
}]