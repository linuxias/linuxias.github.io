<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Developer&#39;s Delight</title>
    <link>https://linuxias.github.io/</link>
    <description>Recent content on Developer&#39;s Delight</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-US</language>
    <lastBuildDate>Mon, 30 Dec 2019 23:06:22 +0900</lastBuildDate>
    
	<atom:link href="https://linuxias.github.io/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Docker Basic</title>
      <link>https://linuxias.github.io/container/docker/1_docker_cmd_basic/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://linuxias.github.io/container/docker/1_docker_cmd_basic/</guid>
      <description>Docker Hub에서 Image Download Docker pull command를 사용하여 DockerHub 에 등록된 이미지 다운로드 합니다. 다운로드할 이미지는 ubuntu 18.04 이미지입니다.
$sudo docker pull ubuntu:18.04 18.04: Pulling from library/ubuntu 5c939e3a4d10:Pull complete c63719cdbe7a: Pull complete 19a861ea6baf: Pull complete 651c9d2d6c4f: Pull complete Digest: sha256:8d31dad0c58f552e890d68bbfb735588b6b820a46e459672d96e585871acc110 설치된 Docker Image 확인하기 docker images 명령어를 이용하여 설치된 도커 이미지들의 리스트를 확인할 수 있습니다. 제 PC 설치된 이미지 리스트는 아래와 같습니다. 총 5가지 이미지가 설치되어 있네요.
$sudo docker images REPOSITORY TAG IMAGE ID CREATED SIZE ubuntu 18.</description>
    </item>
    
    <item>
      <title>Storage</title>
      <link>https://linuxias.github.io/container/docker/2_docker_storage/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://linuxias.github.io/container/docker/2_docker_storage/</guid>
      <description>컨테이너를 사용하여 다양한 작업을 할 수 있습니다. 여러 작업 중 컨테이너 내부에서 생성한 정보, 파일들이 컨테이너가 종료되고 난 이후에 손실되어 찾을 수 없습니다. 기본적으로 컨테이너 내부에 생성되는 모든 파일들은 쓰기가능한 컨테이너 레이에어 저장되어 집니다. 이렇게 저장된다는 것은 아래와 같은 의미를 포함합니다.
 컨테이너가 종료된 후에는 데이터가 지속 가능하지 않습니다. 다른 프로세스가 컨테이너 내부의 데이터를 참조하기 어렵습니다. 컨테이너의 쓰기가능한 레이어는 컨테이너가 동작 중인 상태에서 호스트와 매우 타이트하게 연결되어 있습니다. 이 말은, 여러분이 데이터를 다른 곳으로 옮기는게 쉽지 않음을 의미합니다.</description>
    </item>
    
    <item>
      <title>1. Cgroup</title>
      <link>https://linuxias.github.io/container/cgroup/1.cgroup/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://linuxias.github.io/container/cgroup/1.cgroup/</guid>
      <description>cgroup은 단일 또는 태스크 단위의 프로세스 그룹에 대한 자원 할당을 제어하는 커널 모듈입니다. Cgroup은 Control group으로 처음 개발되었을 때는 group이 아니라 container란 용어를 사용했었습니다. 하지만 container란 용어는 너무 많은 의미를 내포하고 있다고 판단하여 지금의 group이만 이름으로 변경되었습니다.
현재 cgroup은 v1와 v2 두가지 버전이 공존하고 있습니다. v1의 서비스시템 중 일부는 v2에 포함되어 있는 상태입니다. 언젠가 v2만 남겠죠..?
cgroup 서브시스템 cgroup은 여러 개의 서브시스템으로 이루어져 있습니다. (아래 표에 모든 서브시스템을 작성한 것은 아니니, 유의해주세요.</description>
    </item>
    
    <item>
      <title>1. Namespace</title>
      <link>https://linuxias.github.io/container/namespace/1.what_is_namespace/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://linuxias.github.io/container/namespace/1.what_is_namespace/</guid>
      <description>Namespace 기술은 cgourp(Control Group)과 함께 컨테이너(Container) 솔루션을 구성하는 기술 중 하나입니다. 이번 글에서는 namespcae에 대해 정리한 후linux에서 제공하는 namespcae의 종류에 대해 정리하고자 합니다.
namespace는 전역 시스템 리소스를 추상화하여 전역 리소스의 자체 격리 인스턴스가있는 namespace 내의 프로세스에 표시 되도록합니다. 전역 리소스에 대한 변경은 namespace의 멤버, 즉 동일한 namespace를 가진 다른 프로세스에서 볼 수 있지만 다른 namespace를 가진 프로세스에서는 보이지 않습니다. namespcae를 사용하는 것은 컨테이너를 구현하는 것입니다.
   Namespace Constant Isolates     IPC CLONE_NEWIPC System V IPC, POSIX message queues   Network CLONE_NEWNET Network devices, stacks, ports, etc.</description>
    </item>
    
    <item>
      <title>2. blkio</title>
      <link>https://linuxias.github.io/container/cgroup/2.cgroup_blkio/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://linuxias.github.io/container/cgroup/2.cgroup_blkio/</guid>
      <description>Subsystem - blkio blkio 서브시스템은 특정 block device에 대한 접근을 제한하거나 제어하기 위한 서브시스템 입니다. 자세한 내용은 ** Documentation/cgroup-v1/blkio-controller.txt ** 를 참고해주세요.
예제 살펴보기 I/O 작업이 많은 몇몇의 어플리케이션이 하나의 서버에서 동작한다고 가정합니다. 각 어플리케이션에 대해 다른 우선순위와 I/O 대역폭을 설정해주는 예제를 살펴보겠습니다.
 cgroup 가상파일시스템을 마운트합니다.  $ sudo mkdir -p /cgroup/blkio $ sudo mount -t cgroup -o blkio blkio /cgroup/blkio 마운트 여부 확인하기. 아래 명령어를 수행해서 결과가 표시된다면 정상적으로 마운트되었습니다.</description>
    </item>
    
    <item>
      <title>2. PID Namespace</title>
      <link>https://linuxias.github.io/container/namespace/2_pid_namespace/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://linuxias.github.io/container/namespace/2_pid_namespace/</guid>
      <description>PID namespace는 프로세스 ID 공간을 격리 시킵니다. 이 말인 즉, 다른 PID namespace의 프로세스들은 같은 PID를 가질 수도 있음을 의미합니다. PID namespace들은 프로세스 집합의 종료, 재시작과 같은 기능을 제공하기 위한 컨테이너를 허용합니다. 또한 컨테이너를 새로운 호스트로 마이그레이션하는 등의 기능을 컨테이너가 제공할 수 있도록 해줍니다.
PID namepsace의 특이한 점은 새로운 PID namespace의 PID는 1 부터 시작한다는 것입니다. standalone 시스템과 동일하게 각 namespace의 시작 프로세스는 pid를 1번을 가지게됩니다. PID namespace를 사용하기 위해선 CONFIG_PID_NS 커널 옵션을 설정해야 합니다.</description>
    </item>
    
    <item>
      <title>3. memory</title>
      <link>https://linuxias.github.io/container/cgroup/3.cgroup_memory/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://linuxias.github.io/container/cgroup/3.cgroup_memory/</guid>
      <description>Subsystem - memory memory 서브시스템은 cgroup에서 사용하는 메모리 자원에 대해 프로세스가 사용하는 메모리 양과 사용 가능한 메모리 자원을 컨트롤 할 수 있습니다. 또한 사용되는 메모리 자원에 대한 레포트도 자동으로 생성해주는 서브시스템입니다.
memory 서브시스템은 시스템으로부터 태스크의 그룹의 메모리 접근등의 동작을 격리화시킵니다. 이 서브시스템이 사용되는 경우는 다음과 같습니다.
 메모리 소모가 많은 어플리케이션을 격리하고 더 작은 어플리케이션으로 제한할 수 있습니다. mem=XXXX 설정을 통해 부팅하는 경우의 좋은 대안이 될 수 있습니다. 가상화 솔루션에서 원하는 메모리 양을 제어할 수 있습니다.</description>
    </item>
    
    <item>
      <title>3. User Namespace</title>
      <link>https://linuxias.github.io/container/namespace/3.user_namespace/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://linuxias.github.io/container/namespace/3.user_namespace/</guid>
      <description>먼저 이 글에서 사용한 코드는 linux kernel 4.16 임을 알려드립니다.
 User namespace는 시큐리티와 관련된 식별자 및 속성을 분리하며, 특히 User ID와 Group ID, 루트 디렉토리, Key, Capability를 분리합니다. 프로세스의 User, Group ID는 user namespace 내,외부적으로 다를수 있습니다. 특히 프로세스는 User namespace 외부에 권한이 없는 정상적인 User ID를 가질 수 있으며, 동시에 namepsace 내부에 User ID 0을 가질 수 있습니다. 즉, 프로세스에는 user namespace 내의 작업에 대한 전체 권한이 있지만 namespace 외부 작업에 대한 권한이 없습니다.</description>
    </item>
    
    <item>
      <title>4. cpu, cpuset, cpuaccet</title>
      <link>https://linuxias.github.io/container/cgroup/4.cpu.cpuset.cpuacct/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://linuxias.github.io/container/cgroup/4.cpu.cpuset.cpuacct/</guid>
      <description>Subsystem - cpu, cpuset 이번 글에서는 Cgroup의 서브시스템 중 cpu와 cpuset 에 대해 정리해보겠습니다.
cpu subsystem cpu 서브시스템은 cgroup 계층 및 해당 작업에 대한 CPU 시간을 스케쥴링 할 수 있습니다. 시스템이 busy 상태일 때 CPU 공유를 최소화 즉 사용량을 제한 할 수 있습니다. 이 서브시스템은 CPU에 cgroup 작업 액세스를 제공하기 위한 스케줄러(Documentation/scheduler/sched-design-CFS.txt)를 제공합니다.
cpuset subsystem cpuset 서브시스템은 개별 CPU 및 메모리 노드를 cgroup에 바인딩 하기 위한 서브시스템입니다. 리눅스의 testset 명령과 유사하게 CPU 코어를 할당 할 수 있는 서브시스템 입니다.</description>
    </item>
    
    <item>
      <title>4. QEMU</title>
      <link>https://linuxias.github.io/container/namespace/4.qemu/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://linuxias.github.io/container/namespace/4.qemu/</guid>
      <description>Introduction QEMU는 고속의 동적 바이너리 변환 기법을 사용하는 프로세서 에뮬레이터이자 가상화 하이퍼바이저 입니다. 여기서 바이너리 변환이란 하나의 Instruction Set을 다른 Instruction Set으로 번환해주는 처리과정을 말하며, 정적 또는 동적인 방법이 존재합니다.
QEMU에 대해 정리하기 앞서 QEMU와 KVM을 굉장히 많은 분들이 헷갈려 하시는데, 아래 블로그에 매우 자세하게 정리가 되어 있으니 참고하시면 좋을 것 같습니다.
[Qemu와 KVM의 개념 - 에뮬레이션과 가상화][http://blog.naver.com/PostView.nhn?blogId=alice_k106&amp;amp;logNo=221179347223&amp;amp;parentCategoryNo=7&amp;amp;categoryNo=&amp;amp;viewDate=&amp;amp;isShowPopularPosts=true&amp;amp;from=search]
이 글에서는 KVM이 아닌 QEMU에 관한 내용만 정리하려 합니다.
처음 QEMU 개발 시에는 x86 기반이 아닌 아키텍처에서 x86 기반의 리눅스 응용 프로그램을 실행하는 목적으로 개발되었다고 합니다.</description>
    </item>
    
    <item>
      <title>5. Mount namespace</title>
      <link>https://linuxias.github.io/container/namespace/5_mount_namespace/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://linuxias.github.io/container/namespace/5_mount_namespace/</guid>
      <description>mount namespace는 프로세스와 그 자식 프로세스에게 다른 파일시스템 마운트 포인트를 제공합니다. 각 namespace instance의 프로세스들에게 보여지는 마운트 포인트를 격리시키는 기능을 제공합니다. 이렇게 격리된 마운트 포인트는 각 프로세스에게 단일 디렉토리 구조로 보여지게됩니다.
처음 설치된 시스템에서는 기본적으로 모든 프로세스가 하나의 mount namespaece(기본 namespace)에 속하기 때문에 파일시스템를 마운트하거나 해제하는 등의 모든 사항을 확인 및 인지할 수 있습니다.
clone(2) 또는 unshare() 시스템 콜과 CLONE_NEWNS 플래그를 사용하여 새로운 프로세스를 생성하면 생성하는 프로세스의 mount namespace를 그대로 복사하여 생성하게 됩니다.</description>
    </item>
    
    <item>
      <title>6. UTS namespace</title>
      <link>https://linuxias.github.io/container/namespace/6.uts_namespace/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://linuxias.github.io/container/namespace/6.uts_namespace/</guid>
      <description>UTS(Unix TimeSharing) namespace는 리눅스 컨테이너가 ** hostname -f ** 명령어에 의해 반환된 결과 값인 자신의 식별자를 유지관리하기 위해 hostname과 domainname을 namespace 별로 격리해줍니다. 격리하는 대상은 구별을 위한 식별자가 됩니다. 단순하게 생각하면 hostname은 호스트 각각의 이름이고 domainname은 그룹의 이름입니다. 기본적으로 domainname을 설정하지 않았다면 none으로 표시됩니다.
UTS namespace로 분리할 수 있는 식별자는 sethostname(2)과 setdomainname(2)을 사용하여 설정되며 unname(2), gethostname(2) 및 getdomainname(2)을 사용하여 검색할 수 있습니다.
unshare 또는 clone 시스템 콜 호출 시에 CLONE_NEWUTS 플래그를 사용하면 새로운 UTS namespace를 생성하고 해당 namespace를 가진 프로세스를 생성할 수 있습니다.</description>
    </item>
    
    <item>
      <title>7. Cgroup Namespace</title>
      <link>https://linuxias.github.io/container/namespace/7.cgroup_namespace/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://linuxias.github.io/container/namespace/7.cgroup_namespace/</guid>
      <description>cgroup namespace는 프로세스의 cgroups의 뷰를 가상화 합니다.
각 cgroup 네임스페이스에는 고유한 cgroup 루트 디렉토리 세트가 있습니다. 그 루트 디렉토리는 /proc/[pid]/cgroup 파일의 해당 레코드에 표시되는 상대 위치의 베이스 위치입니다. clone(2) 또는 unshare(2) 시스템 콜에 CLONE_NEWCGROUP 플래그를 전달하여 새로운 cgroup namespace를 생성할 수 있습니다. 이렇게 생성된 namespace는 현재 cgroups 디렉토리가 새 namespace의 cgroup 루트 디렉토리가되는 새 cgroup namespace로 들어갑니다. 이 정책은 cgroup 버전 1,2 모두에 적용됩니다.
/proc/[pid]/cgroup 을 확인하면 표시되는 각 레코드의 세 번째 필드에 표시된 경로 이름은 해당 cgroup 계층에 대한 읽기 프로세스의 루트 디렉토리에 상대적인 위치입니다.</description>
    </item>
    
    <item>
      <title>프로그래밍 면접 이렇게 준비한다</title>
      <link>https://linuxias.github.io/books/readyforprogramminginterview/</link>
      <pubDate>Sun, 05 Jan 2020 03:43:45 +0900</pubDate>
      
      <guid>https://linuxias.github.io/books/readyforprogramminginterview/</guid>
      <description>목차
CHAPTER 1 구직을 시작하기 전에
너 자신을 알라
시장을 알라
팔릴 만한 능력을 계발하라
일 제대로 해내기
온라인 프로파일을 정돈하라
요약
CHAPTER 2 입사 지원 절차
회사 선택 및 접촉
면접 절차
리크루터의 역할
근무 조건 협상
요약
CHAPTER 3 전화 예비 면접
전화 예비 면접의 이해
전화 예비 면접 방법
전화 예비 면접 문제
요약
CHAPTER 4 프로그래밍 문제 접근법
절차
문제 해결
풀이 분석
요약
CHAPTER 5 연결 리스트</description>
    </item>
    
    <item>
      <title>[gcc] Warning 옵션</title>
      <link>https://linuxias.github.io/linux/compilelink/gcc_warning_option/</link>
      <pubDate>Mon, 30 Dec 2019 23:24:46 +0900</pubDate>
      
      <guid>https://linuxias.github.io/linux/compilelink/gcc_warning_option/</guid>
      <description>gcc 컴파일 옵션으로 많이 사용하는 -Wall , -Wextra 이외에 다양한 옵션들은 정리하고자 합니다.
   옵션 설명 특이사항     -fstack-usage 컴파일러가 프로그램에 대한 스택 사용 정보를 함수 단위로 출력하도록 합니다. 함수의 이름, 바이트 수 등이 표기됩니다. x   -Wframe-larger-than={len} 함수 프레임의 크기가 len을 넘어가면 Warning이 출력합니다. x   -Wstrict-overflow=n -fstrict-overflow가 활성화 되어있는 경우에만 활성화됩니다. 컴파일러가 signed 오버플로우가 일어나지 않을거라 가정하고 최적화를 진행하는 경우에 Warning이 발생합니다.</description>
    </item>
    
    <item>
      <title>[gcc] 최적화 속성 사용하기</title>
      <link>https://linuxias.github.io/linux/compilelink/gcc_specific_attribute/</link>
      <pubDate>Mon, 30 Dec 2019 23:23:27 +0900</pubDate>
      
      <guid>https://linuxias.github.io/linux/compilelink/gcc_specific_attribute/</guid>
      <description>gcc를 컴파일러로 사용할 시 -O2, -O3와 같은 최적화 옵션을 자주 사용하게 됩니다.
특별한 경우에 특정 함수나 코드 구간에 대해서 특정 최적화 단계를 적용하고 싶다면 아래와 같이 진행합니다.
함수에 최적화 적용하기 void __attribute__((optimize(&amp;#34;O0&amp;#34;))) func(void) { } 코드 범위에 최적화 적용하기 #pragma GCC push_options #pragma GCC optimize (&amp;#34;O0&amp;#34;)  //Write your code  #pragma GCC pop_options 감사합니다.</description>
    </item>
    
    <item>
      <title>정적 라이브러리 링크</title>
      <link>https://linuxias.github.io/linux/compilelink/staticliblink/</link>
      <pubDate>Mon, 30 Dec 2019 23:21:11 +0900</pubDate>
      
      <guid>https://linuxias.github.io/linux/compilelink/staticliblink/</guid>
      <description>라이브러리 중 정적 라이브러리(static library)를 사용하는 경우가 종종 있다. 그리고 해당 라이브러리가 또 다른 (static library)를 Link 하는 경우가 있다. 이럴 때 문제가 발생하게 된다. 아래와 같이 어플리케이션이 필요로 하는 정적 라이브러리를 링크하게 되는데, 해당 라이브러리가 다른 정적 라이브러리를 링크하게 되는 경우이다.
Application &amp;ndash;&amp;gt; Static library 1 &amp;ndash;&amp;gt; Static library 2
Application은 Static library 1의 존재만 알 뿐 2의 존재는 알지도, 알 필요도 없다.
pkg-config를 이용한 경우 pc 파일에 정의 아래 libpalosalodp 의 pc.</description>
    </item>
    
    <item>
      <title>1. Model Representation / Cost function</title>
      <link>https://linuxias.github.io/machinelearning/basic/model_representation_and_cost_function/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://linuxias.github.io/machinelearning/basic/model_representation_and_cost_function/</guid>
      <description>아래 내용은 Andrew Ng 교수님의 강의와 자료를 기반으로 학습한 내용을 정리하여 작성하였습니다.
개인의 학습 내용이기에 잘못 해석 및 이해하고 있는 부분도 있을 수 있으니, 다양한 자료를 기반으로 참고하시는 걸 추천드립니다.
Machine Learning 중 Supervised Learning에서 가장 기초적으로 다뤄지는 내용이 Linear regression(선형 회귀)입니다.
제 개인적인 의견으로 Linear Regression은 모델표현이 쉽고 단순하며 Cost function과 Gradient descent 등을 이해하기 쉽기 때문이라고 생각됩니다.
 위의 그래프를 보시면, Portland의 집 크기에 따른 가격 Data를 표현한 그래프입니다.</description>
    </item>
    
    <item>
      <title>[Agent] BDI Architecture</title>
      <link>https://linuxias.github.io/machinelearning/agent/bdi_architect/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://linuxias.github.io/machinelearning/agent/bdi_architect/</guid>
      <description>Belief - Desire - Intention Architect BDI Architect는 Software Agent 분야에서 자주 사용되었던 구조입니다. 이 구조의 이름인 BDI 는 3가지 단어 입니다. Belief, Desire, Intention 이 3가지 단어의 앞자리를 따서 만들어졌습니다. 그럼 3가지 단어가 이 구조의 큰 요소일 텐데 구조를 정리해 가며 설명드리겠습니다.
BDI Architect는 목표를 이루기 위해 순간, 순간 행해야할 행위를 결정하는 의사결정 프로세스입니다. BDI 구조는 reactive behavior와 goal-directed behavior가 조화를 이루는 구조로써 Agent는 그 목표를 달성하기 위해 최선을 다하면서도 그 목표가 여전히 유효한지, 달성할 수 있는지에 대해 계속해서 확인하는 과정을 거치게 됩니다.</description>
    </item>
    
    <item>
      <title>[Agent] Communication</title>
      <link>https://linuxias.github.io/machinelearning/agent/agent_communication/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://linuxias.github.io/machinelearning/agent/agent_communication/</guid>
      <description>Communication between agents 에이전트들간의 커뮤니케이션을 알아보기 이전에 한 가지 용어를 정리하고자 합니다. &amp;lsquo;Speech Act&amp;rsquo; 란 용어입니다. Speech Act는 한국어로 언어행위론 이라고 정의할 수 있습니다. 언어행위론이란 언어를 통해 이루어지는 행위를 말합니다. 나는 너를 용서한다 라고는 말하면 말로써 행위가 표현되듯 언어가 어떤 영향을 주는지에 대해 초점이 맞춰져 있습니다. 명령, 요구 등으로 나뉠 수 있죠.
그럼 이제 커뮤니케이션에 대해 정리해보겠습니다. 예를 들어서 살펴보죠. 개인비서 에이전트가 있다고 합니다. 이 에이전트에게 여러분은 김군과 저녁 약속을 잡아달라고 요청합니다.</description>
    </item>
    
    <item>
      <title>[Agent] MAS (Multi-agent System)</title>
      <link>https://linuxias.github.io/machinelearning/agent/agent_mas/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://linuxias.github.io/machinelearning/agent/agent_mas/</guid>
      <description>MAS (Multi-agent System) 현재 세계의 시스템에선 하나의 agent로는 모든 처리를 할 수 없습니다. 그래서 각각의 기능을 담당하는 여러 agent를 생성하여 협업 또는 조율하게 만드는 방법을 사용하기도 합니다. 이번 글에서는 MAS, Multi-agent System에 대해 정리해보겠습니다.
Multi-agent System (지금부턴 MAS라 칭하겠습니다.)은 시스템 내에 여러 agent를 가지고 있는 시스템입니다. 각 agent 간 커뮤니케이션을 통해 상호작용을 하게 되고, 서로 다른 작업을 하기에 &amp;ldquo;spheres of influence&amp;rdquo; 라고 불리는 자신만의 환경 영역을 가지고 있습니다. 즉 변화하는 환경에 대해 영향을 받거나 주는 환경이 다를 수 있습니다.</description>
    </item>
    
    <item>
      <title>[Agent] SW Agent Architecture</title>
      <link>https://linuxias.github.io/machinelearning/agent/agent_sw_agent_architecture/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://linuxias.github.io/machinelearning/agent/agent_sw_agent_architecture/</guid>
      <description>소프트웨어 에이전트 아키텍쳐에 대해 정리해보겠습니다.
에이전트 구조는 3개로 크게 나뉠 수 있습니다.
  Deliberative : 의도적인, 신중한
  Reactive : 반응하는
  Hybrid : Deliberative + Reactive
  용어 의미만 보아서 파악할 수 있는건 Hybrid밖에 없는 것 같네요. 하나씩 정리해보도록 하겠습니다.
Deliberative Agents Deliberative Agents는 명확하게 표현되어 질 수 있는 실세계의 상징적 모델이며 Symbolic reasoning을 통해 결정을 만들어 나가는 에이전트입니다. Sense-plan-act 를 통한 문제 해결방식으로 Deliberative 구조로 BDI, GRATE*, HOMER, Shoham 등이 유명합니다.</description>
    </item>
    
    <item>
      <title>[multitail] 여러 로그 동시에 확인하기</title>
      <link>https://linuxias.github.io/linux/tip/multitail/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://linuxias.github.io/linux/tip/multitail/</guid>
      <description>로그를 확인할 때 tail 명령어를 많이 사용합니다. tail 명령어를 이용 시 여러 개의 로그를 동시에 보고싶지만 하나의 터미널 창에서는 하나의 로그만 확인 가능합니다. 이러한 점을 보완한 툴이 multitail입니다.
아래 명령어로 먼저 설치해 줍니다.
$ sudo apt install multitail 사용방법은 간단합니다.
$ multitail {log file} {log file} 예를 들어,
$ multitail /var/log/apache2/access.log /var/log/apach2/error.log 위 명령어를 수행 시 아래와 같은 결과를 얻을 수 있습니다.
감사합니다.</description>
    </item>
    
    <item>
      <title>Batch Sequential Architecture</title>
      <link>https://linuxias.github.io/sw_architecture/batch_sequential_architecture/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://linuxias.github.io/sw_architecture/batch_sequential_architecture/</guid>
      <description>Batch Sequential Architecture에 대해 정리해보겠습니다.
Batch Sequential Architecture는 이전 글에서 정리한 Data Flow Software Architecture 중 하나입니다. 참고가 필요하신 분은 해당 글을 확인해주세요.
[S/W Architecture] Data Flow Software Architectures
Batch Sequential Architecture는 1950~70년대에 많이 사용된 데이터 처리 모델입니다. 데이터는 하나의 서브시스템에서 다음 서브시스템으로 데이터로 전달됩니다. 각 데이터 전송 서브시스템 또는 모듈은 이전 서브시스템의 데이터 처리가 끝나기 전에는 스스로 시작할 수 없습니다. 정리하자만 A-B 로 연결된 Batch Sequential Architecture에서 B는 A가 모든 데이터 처리를 완료한 후 결과 데이터가 출력되기 전까지 스스로 독립적으로 시작할 수 없습니다.</description>
    </item>
    
    <item>
      <title>CPU Profiling</title>
      <link>https://linuxias.github.io/linux/profile/cpu/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://linuxias.github.io/linux/profile/cpu/</guid>
      <description>CPU Analysis uptime 시스템 부하 평균을 표시한다. load average의 값은 앞에서부터 1분, 5분, 15분 동안의 평균 부하이다.
uptime 08:41:55 up 1 day, 17:41, 4 users, load average: 0.28, 0.24, 0.27 부하평균은 CPU 자원에 대한 요구사항으로써, ** 실행 중인 스레드 개수 + 대기열의 스레드 개수 **이다. 만약 부하 평균이 호스트 머신의 CPU 개수보다 크면 스레드를 처리하기에 CPU가 부족한 상황이며, 대기열에 여러 스레드가 CPU를 할당받기 위해 대기하고 있는 상태라고 파악할 수 있다.</description>
    </item>
    
    <item>
      <title>Data Flow Software Architecture</title>
      <link>https://linuxias.github.io/sw_architecture/data_flow_sw_architecture/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://linuxias.github.io/sw_architecture/data_flow_sw_architecture/</guid>
      <description>Data Flow Software Architecture에 대해 정리해보고, 해당 아키텍처에 속하는 이키텍처들을 정리해 보려합니다. 주제에서 알 수 있듯이 데이터의 흐름에 대한 소프트웨어 아키텍처입니다. Data Flow Software Architecture는 전체 소프트웨어 시스템을 연속적인 데이터 집합에 대한 일련의 변환으로 봅니다.
소프트웨어 시스템은 데이터가 데이터 연산 처리 순서를 지시하고 제어하는 데이터 처리 요소로 분리 될 수 있습니다. 각 컴포넌트는 입력으로 데이터를 받고, 출력으로 연산된 데이터를 출력합니다. 이렇게 출력된 연산 데이터는 다음 컴포넌트의 입력이 됩니다. 이 부분이 Data Flow Software Architecture 들의 가장 큰 특징입니다.</description>
    </item>
    
    <item>
      <title>Docker Compose</title>
      <link>https://linuxias.github.io/container/docker/compose/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://linuxias.github.io/container/docker/compose/</guid>
      <description>Overview Docker Compose 는 멀티 컨테이너 도커 어플리케이션을 정의하거나 동작시키기 위한 툴이다. 이 툴로 인해 개발용이성이 매우 향상되었다. docker 명령어를 사용하나 단일 도커 이미지를 생성하거나 단일 컨테이너를 동작시켰다. 둘 이상의 상호 연관된 컨테이너가 필요한 경우 docker run 명령어를 여러번 실행하여 환경을 구성하였다. 한번 구성한 환경을 지속적으로 사용한다면 모르겠지만, 설정이 변경되거나 다른 호스트 머신에서 구축이 필요한 경우 또 다시 docker run 명령(docker network 등 포함)을 다시 실행해야 한다. 얼마나 불편한가?
개발자는 게을러야 한다.</description>
    </item>
    
    <item>
      <title>Dockerfile</title>
      <link>https://linuxias.github.io/container/docker/dockerfile/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://linuxias.github.io/container/docker/dockerfile/</guid>
      <description>Docker는 Dockerfile에 정의된 인스트럭션을 이용하여 자동적으로 이미지를 빌드 할 수 있습니다. Dockerfile은 일반적은 텍스트 파일로서 사용자가 이미지를 생성할 때 필요로 하는 모든 명령어들을 포함시킬 수 있습니다. docker build 명령어를 이용하여 사용자는 도커 이미지를 생성할 수 있습니다.
ubuntu나 centOS 이미지를 DockerHub로 부터 다운받아서 본인의 어플리케이션을 동작시키려 한다면 생각은 쉽지만 쉽게 되지 않습니다. 어플리케이션이 동작하기 위한 여러 환경이 설정되어 있지 않기 때문입니다. 동일한 어플리케이션을 여러 컨테이너에서 동작 시키고자 한다면 각 컨테이너마다 새로운 환경을 구축해 줘야 할 것입니다.</description>
    </item>
    
    <item>
      <title>FileSystem</title>
      <link>https://linuxias.github.io/linux/profile/filesystem/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://linuxias.github.io/linux/profile/filesystem/</guid>
      <description>free(1) $free -th total used free shared buff/cache available Mem: 31G 1.5G 28G 79M 1.1G 29G Swap: 2.0G 0B 2.0G Total: 33G 1.5G 30G top(1) ... KiB Mem : 32832784 total, 30064040 free, 1579628 used, 1189116 buff/cache KiB Swap: 2097148 total, 2097148 free, 0 used. 30760488 avail Mem ... 1189116 buff/cache
vmstat buff and cache column -&amp;gt; buffer cache
vmstat -w 1 procs -----------------------memory---------------------- ---swap-- -----io---- -system-- --------cpu-------- r b swpd free buff cache si so bi bo in cs us sy id wa st 0 0 0 30052984 119008 1078384 0 0 66 6 792 1632 2 1 98 0 0 0 0 0 30052724 119008 1078388 0 0 0 0 7244 14004 0 1 99 0 0 0 0 0 30055024 119008 1076084 0 0 0 0 10043 19655 0 1 99 0 0 slabtop slabtop displays detailed kernel slab cache information in real time.</description>
    </item>
    
    <item>
      <title>ftrace</title>
      <link>https://linuxias.github.io/linux/debugging/ftrace/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://linuxias.github.io/linux/debugging/ftrace/</guid>
      <description>name information note     current_tracer 현재 설정된 tracer 값    available_tracers tracer 가능한 정보들, 커널 컴파일 시 설정될 수 있다. 활성화 위해선 current_tracer에 이벤트를 작성한다.    tracing_on ftrace ring buffer을 활성화할지 여부를 결정한다. 트레이서를 비활성화하려면 이 파일에 0을, 활성화하려면 1을 각각 설정하십시오. 참고, 이는 링 버퍼에 대한 쓰기를 비활성화할 뿐이며, 추적 오버헤드는 여전히 발생할 수 있다.    trace 사람 친화적인, 즉 텍스트 형태로 트레이스 결과가 출력된다.</description>
    </item>
    
    <item>
      <title>Head First GO</title>
      <link>https://linuxias.github.io/books/headfirst_go/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://linuxias.github.io/books/headfirst_go/</guid>
      <description>올해 3월 Head First Go 가 출간되었다. Head First는 O&amp;rsquo;REILLY 사에서 출간한 컴퓨터 관련 시리즈 도서로 다양한 책들이 있으며 많은 소프트웨어 개발자들이 한번은 봤을법한 표지로도 유명하다.
도서의 목차는 아래와 같다.
목차 서문 1장. 시작해 봅시다: 문법 기초 2장. 다음엔 어떤 코드가 실행될까요?: 조건문과 반복문 3장. 호출해 주세요: 함수 4장. 코드 묶음: 패키지 5장. 목록에서: 배열 6장. 확장 문제: 슬라이스 7장. 데이터 라벨링: 맵 8장. 저장소 만들기: 구조체 9장. 나만의 타입: 사용자 정의 타입 10장.</description>
    </item>
    
    <item>
      <title>Hierarchical Software Architecture</title>
      <link>https://linuxias.github.io/sw_architecture/hierarchical_software_architecture/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://linuxias.github.io/sw_architecture/hierarchical_software_architecture/</guid>
      <description>Hierarchical Software Architecture, 한국어로 계층적 소프트웨어 아키텍처라 불리는 아키텍처에 대해 정리하겠습니다.
Hierarchical Architecture는 전체 시스템을 계층 구조적으로 나뉘어져 있으며 계층적으로 서로 다른 레벨의 서브시스템으로 구성되어 있습니다. Hierarchical Architecture는 매우 다양한 곳에서 사용되고 있습니다. 운영체제, 네트워크 프로토콜 계층들, 인터프리터, 그 외 다양한 곳에서 사용되고 있는데요, 이 아키텍처의 가장 대표적인 구조로서 여러분들이 가장 많이 접해본 아키텍처의 한 예가 안드로이드 일 것 같습니다. 위 안드로이드 아키텍처를 보시면 Applications, Application Framework, Libraries, Linux Kernel 까지 여러 개의 서브시스템이 계층적으로 구성되어 하나의 시스템을 이루고 있습니다.</description>
    </item>
    
    <item>
      <title>kubeadm으로 kubernetes 설치 및 초기화</title>
      <link>https://linuxias.github.io/cloud/kubernetes_install/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://linuxias.github.io/cloud/kubernetes_install/</guid>
      <description>kubernetes를 설치하는 방법은 다양하지만 그 중 kubeadm을 이용한 방법을 간단히 정리한다.
먼저 설치하기 전 제약사항으로 kubernetes는 cpu core 2개 이상, Memory 2G 이상에서만 설치가 가능하다. 만약 AWS Freetier로 EC2를 사용하는 분들은 EC2 Instance를 무엇을 사용하는지 확인 할 필요가 있다. (Freetier로 제공되는 t2.micro는 kubernetes 설치가 불가능하며 t3.small 이상의 Instance가 필요하다.)
kubeadm을 설치하기 전 저장소를 추가한다. 아래 명령어는 관리자 명령으로 수행한다.
#curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add - #cat &amp;lt;&amp;lt;EOF &amp;gt; /etc/apt/sources.list.d/kubernetes.list deb http://apt.</description>
    </item>
    
    <item>
      <title>Logging</title>
      <link>https://linuxias.github.io/container/docker/container_logging/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://linuxias.github.io/container/docker/container_logging/</guid>
      <description>docker logs 명령어는 이용하여 동작중인 컨테이너의 로그 정보를 볼 수 있습니다. docker logs 명령어는 터미널에서 대화식으로 명령을 실행했을 때와 같이 명령의 출력을 표시합니다. UNIX 및 Linux 명령은 일반적으로 STDIN, STDOUT 및 STDERR이라는 3 개의 I/O 스트림이 실행될 때 열립니다. STDIN은 명령의 입력 스트림으로 키보드 입력 또는 다른 명령 입력을 포함 할 수 있습니다. STDOUT은 일반적으로 명령의 일반 출력이며 STDERR은 일반적으로 오류 메시지를 출력하는 데 사용됩니다. 기본적으로 docker logs에는 명령의 STDOUT 및 STDERR이 표시됩니다.</description>
    </item>
    
    <item>
      <title>LTTng:userspace</title>
      <link>https://linuxias.github.io/linux/profile/lttng/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://linuxias.github.io/linux/profile/lttng/</guid>
      <description>LTTng는 리눅스 프로파일링을 위한 오픈소스 트레이싱 프레임워크입니다. LTTng는 Linux Trace Toolkit: next generation 의 약자로 리눅스 커널, 사용자 어플리케이션(C/C++, java, python), 라이브러리를 트레이싱할 수 있습니다.
LTTng 구성 LTTng 는 크게 3가지 모듈로 구성되어 있습니다.
 lttng-ust : 사용자 어플리케이션을 추적하기 위한 라이브러리 lttng-tools : tracing session을 관리, 제어하기 위한 라이브러리들과 명령어 인터페이스 lttng-modules : 커널 추적을 위한 리눅스 커널 모듈  각 모듈에 대한 설명과 설치 방법 등은 아래 URL을 참조하면 됩니다.</description>
    </item>
    
    <item>
      <title>Memory profiling</title>
      <link>https://linuxias.github.io/linux/profile/memory/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://linuxias.github.io/linux/profile/memory/</guid>
      <description>메모리 분석에 사용할 수 있는 다양한 도구에 대해 정리한다.
vmstat 가상 메모리 통계 정보를 제공한다. 현재 가용 메모리와 페이지 통계 등 메모리에 대한 다양한 정보를 제공한다. manual 페이지에는 아래와 같이 필드 설명이 되어 있다.
Memory swpd: the amount of virtual memory used. free: the amount of idle memory. buff: the amount of memory used as buffers. cache: the amount of memory used as cache. inact: the amount of inactive memory. (-a option) active: the amount of active memory.</description>
    </item>
    
    <item>
      <title>Performance Methodologies </title>
      <link>https://linuxias.github.io/linux/profile/performance_methodologies/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://linuxias.github.io/linux/profile/performance_methodologies/</guid>
      <description>성능 분석의 목표는 End-user 성능을 향상시키고 소프트웨어 동작에 대한 비용을 줄이기 위한 활동이며, 이러한 활동은 측정 가능한 측면, 요소등을 통해 달성하고자 하는 기대 성능 목표를 설명하는데 도움이 된다.
성능을 얘기할 때 측정 가능한 요소들은 빠질 수 없습니다. 먼저 측정가능한 요소에 대한 용어를 먼저 정의하려 한다.
  latency : 어떤 요청에 대한 응답을 받기까지 기다려야 하는 시간 Rate : 초 당 동작 또는 요청 수 Throughput : 단위 시간 당 처리량 Utilization : 요청을 서비스하는 자원의 경우 그 자원이 얼마나 바쁜 상태인가를 측정하는 기준 Cost : 가격 대비 성능 비율  위에 나열한 정의 외에도 매우 다양한 요소들이 있다.</description>
    </item>
    
    <item>
      <title>Pipe and Filter Architecture</title>
      <link>https://linuxias.github.io/sw_architecture/pipe_and_filter_architecture/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://linuxias.github.io/sw_architecture/pipe_and_filter_architecture/</guid>
      <description>Data flow Architecture에는 Batch Sequential, Pipe and Filter, Process Control Architecture 로 3가지로 분류할 수 있습니다. 그 중 Pipe and Filter Architecture에 대해 정리해보려 합니다.
Pipe and Filter Architecture는 데이트 스트림을 처리하는 시스템을 위한 구조를 제공합니다. 데이터를 처리하는 각 프로세싱 단계는 Filter 컴포넌트 내부에 포함되어 있습니다. 데이터는 Filter 사이를 Pipe를 통해 전달되게 됩니다. 이러한 구조로 인해 Pipe and Filter Architecture는 Batch Sequential Architecture와 많이 비교됩니다.
Pipe and Filter 구조의 구성요소는 크게 3가지 입니다.</description>
    </item>
    
    <item>
      <title>Proc Filesystem</title>
      <link>https://linuxias.github.io/linux/debugging/proc_filesystem/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://linuxias.github.io/linux/debugging/proc_filesystem/</guid>
      <description>The /Proc Filesystem for Debugging proc - process infomation pseudo-filesystem (Process infomation)
리눅스가 인기 있는 이유 중 하나는 UNIX 시스템의 좋은 feature들을 결합시킨 점입니다. Proc 파일시스템은 그 좋은 feature들 중 한가지 인데요, 대다수의 Linux 배포판에 포함되어 있습니다. proc 파일시스템은 /proc에 마운트되어 있으며 kernel data 구조에 대해 인터페이스를 제공하고 있습니다. proc 파일시스템의 파일들은 READ-ONLY 이지만, 몇몇의 파일들은 커널 변수를 변경하여 적용할 수 있도록 WRITE가 가능합니다. proc 파일시스템의 구성요소를 하나하나 살펴보도록 하시죠.</description>
    </item>
    
    <item>
      <title>Process</title>
      <link>https://linuxias.github.io/linux/profile/process/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://linuxias.github.io/linux/profile/process/</guid>
      <description>1. Perf + FlameGraph Process의 FlameGraph를 뽑아내는 과정을 설명한다. Perf의 --call-graph 기능을 이용한다.
1) Kernel setting echo -1 &amp;gt; /proc/sys/kernel/perf_event_paranoid echo 0 &amp;gt; /proc/sys/kernel/kptr_restrict chmod a+r /proc/kallsyms echo 100 &amp;gt; /proc/sys/kernel/perf_cpu_time_max_percent echo performance &amp;gt; /sys/devices/system/cpu/cpu0/cpufreq/scaling_governor 2) Get perf data perf record -o perf.data --call-graph=dwarf -F 15000 -p {PID} perf script --max-stack 1024 --input=perf.data -f &amp;gt; out.perf 3) Create FlameGraph c++filt &amp;lt; out.perf &amp;gt; out.perf_filted stackcollapse-perf.pl --all out.perf_filted &amp;gt; out.perf_collapsed flamegraph.pl -color=java --hash out.</description>
    </item>
    
    <item>
      <title>Process Control Architecture</title>
      <link>https://linuxias.github.io/sw_architecture/process_control_architecture/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://linuxias.github.io/sw_architecture/process_control_architecture/</guid>
      <description>Process Control Architecture는 Data Flow Architecture 분류에 속하는 아키텍처입니다. 해당 분류에 속하는 아키텍처는 이전에 다뤘던 Batch Sequential, Pipe and Filter Architecture가 있습니다. 자세한 내용은 아래 링크 참고 부탁드립니다.
  Data Flow Software Architectures
  Batch Sequential Architecture
  Pipe and Filter Architecture
  Process Control Architecture에 대해 간략히 정리해보겠습니다.
Process Control의 가장 큰 특징은 데이터의 흐름이 프로세스의 실행을 제어하는 변수 집합이라는 것입니다. 한 번에 이해하기 어려운 말인 것 같습니다.</description>
    </item>
    
    <item>
      <title>Resource Limitation</title>
      <link>https://linuxias.github.io/container/docker/container_resource/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://linuxias.github.io/container/docker/container_resource/</guid>
      <description>컨테이너는 기본적으로 리소스 제한이 없으며 호스트의 커널 스케쥴러에 의해 허용되는 주어진 리소스를 사용할 수 있습니다. 가끔 각 컨테이너 별로 리소스(CPU, Memory 등)를 제한할 필요가 생길 수 있습니다. 이런 경우를 위해 도커에서는 컨테이너 별로 어느 정도의 CPU, Memory 자원을 사용할 지에 대해 제한할 수 있는 방법을 제공하고 있습니다.
이 기능들이 지원되기 위해서는 커널이 필요로 합니다. 커널에서 지원하는 여부는 docker run 명령어를 사용하여 확인할 수 있습니다.
sudo docker info [sudo] password for linuxias: Client: Debug Mode: false Server: Containers: 1 Running: 1 Paused: 0 Stopped: 0 Images: 5 Server Version: 19.</description>
    </item>
    
    <item>
      <title>소프트웨어 설계의 부패</title>
      <link>https://linuxias.github.io/sw_architecture/sw_design_corruption/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://linuxias.github.io/sw_architecture/sw_design_corruption/</guid>
      <description>소프트웨어 설계는 무엇일까? 소스코드 작성하기 전 UML 다이어그램을 작성하는 것? 가끔 몇몇 개발자 분들과 이야기를 할 때 설계는 UML 다이어그램을 작성하는 것이라는 말을 듣는다. 그럼 설계는 UML 다이어그램과 동일 시 할 수 있는가에 대해서 고민해보면 그렇지 않다 라는 결론이 나올 수 있다. 다이어그램은 설계에서 부수적인 부분일 뿐 설계 그 자체가 될 수는 없다고 생각한다. 소프트웨어 설계는 매우 추상적인 것이라 생각한다. 사용자 요구사항부터 시나리오, 모듈, 클래스, 메소드와 더불어 어떠한 형태와 구조를 가질 것인지 프로그램 전체의 형태, 구조와도 관련이 있다고 생각한다.</description>
    </item>
    
    <item>
      <title>시스템에서 kernel config 확인하기</title>
      <link>https://linuxias.github.io/linux/tip/kernel-config/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://linuxias.github.io/linux/tip/kernel-config/</guid>
      <description>본인의 리눅스 시스템 마다 3개 중 하나를 사용할 수 있습니다.
cat /proc/config.gz cat /boot/config cat /boot/config-$(uname -r) </description>
    </item>
    
    <item>
      <title>쿠버네티스를 활용한 클라우드 네이티브 데브옵스</title>
      <link>https://linuxias.github.io/books/cloudnativedevopswithkubernetes/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://linuxias.github.io/books/cloudnativedevopswithkubernetes/</guid>
      <description>CHAPTER 1 소프트웨어 세상의 세 가지 혁명 __1.1 클라우드 혁명 __1.2 데브옵스 탄생 __1.3 컨테이너 등장 __1.4 컨테이너 오케스트레이션 __1.5 쿠버네티스 __1.6 클라우드 네이티브 __1.7 운영의 미래 __1.8 마치며
CHAPTER 2 쿠버네티스 첫걸음 __2.1 첫 번째 컨테이너 실행하기 __2.2 데모 애플리케이션 __2.3 컨테이너 빌드하기 __2.4 컨테이너 레지스트리 __2.5 헬로, 쿠버네티스 __2.6 Minikube __2.7 마치며
CHAPTER 3 쿠버네티스 구축하기 __3.1 클러스터 아키텍처 __3.2 자체 호스팅 쿠버네티스 비용 __3.3 관리형 쿠버네티스 서비스 __3.</description>
    </item>
    
  </channel>
</rss>